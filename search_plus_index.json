{"./":{"url":"./","title":"关于framework-learning","keywords":"","body":"framework-learning 以下是我在学习计算机相关知识的过程中整理的一些资料，部分参考了网上一些大神的文章。 我把它们都做成Markdown的文章供各位同学参考，其中一部分我也以XMind的方式来进行梳理总结。 为了良好的阅读体验，可以移步到本项目的GitBook网页阅读版: framework-learning 。 网页阅读版的内容与本项目是同步的，请放心阅读。 目前有很多知识我个人也在学习和复习之中，后面的push可能会比较慢，所以请各位同学理解。 架构设计 系统架构设计 站内消息系统的设计 编程语言 Jdk&Jvm&Juc(待JVM第三版修改) Java常见基础知识点 IO Java集合 Java多线程 Java并发 AQS 对象在内存中的布局 JVM 简单了解类文件结构 类的生命周期(类加载) JVM常量池 GC JVM调优相关内容 Jdk新特性 JDK&JVM&JUC - XMind 计算机基础 Linux Linux简介 操作系统的内核 进程间通信 Linux文件系统 Linux用户权限 Linux运行级别 Shell Linux命令大全 完全使用GNU/Linux学习 非关系型数据库 - XMind 数据结构与算法 数据结构与算法简介 数据结构 算法 推荐一个数据结构与算法的可视化网站，希望能够帮到正在学习数据结构与算法的同学: 数据结构与算法可视化 计算机网络 OSI七层模型 TCP/IP HTTP 计算机网络 - XMind 设计模式 设计模式简介 七大原则 二十三种设计模式 数据库 关系型数据库 RDBMS简介 RDBMS常见知识点 索引 Mysql Mysql存储引擎 事务 数据库锁 Mysql日志 Mysql优化 Mysql数据类型 关系型数据库 - XMind 非关系型数据库 NoSQL简介 Redis简介 Redis常见知识点 Redis数据结构 Redis事务 Redis缓存淘汰策略 Redis持久化策略 Elasticsearch ElasticSearch 中间件 AMQP AMQP简介 RabbitMQ常见知识点 AMQP - XMind Zookeeper Zookeeper Tomcat Tomcat 开发框架与库 Netty(更新中) Netty简介 Netty组件 Transport传输 ByteBuf容器 ChannelHandler和ChannelPipeline Netty线程模型和EventLoop事件循环 Bootstrap引导 Spring框架 SpringFramework常见知识点 SpringMVC常见知识点 SpringMVC源码分析 SpringBoot常见知识点 Spring - XMind ORM ORM简介 Mybatis简介 Mybatis常见知识点 Mybatis源码分析 ORM - XMind 待办 [ ] Git (突然想到Git的官方文档已经是最好的资料了，这里不再画蛇添足了) [ ] JVM部分后续会根据《深入理解Java虚拟机 - 第三版》更新 [ ] Netty (更新中) [ ] PostgreSQL 如何对本文档做出补充? 这里主要讲讲本项目的目录结构吧。 img: 此目录存放所有的图片，这个各位同学稍微看看就知道了。Github有一个特点(好处)就是: 当我们在markdown文件之中指定了本地图片时， 它能自动把我们的图片上传到它的服务器上，这也是为什么你会在我的markdown之中看到这样的内容: ![图片名](../img/图片名.png) xmind_file: 此目录存放了每个模块对应的xmind文件。 本项目的每一个模块就是一块知识内容,每个模块下都有对应知识点的md文件，如: jdk-jvm-juc ... Java常见基础知识点.md ... 各位同学如果要做补充和纠错，可以选择在对应知识点的模块下新建md或者修改原来的md。 其他的目录的文件,如:gitbook_doc/* 由我亲自修改，并不需要改动。 各位同学有好的建议可以提issue或pr，感谢各位同学的支持！ LICENSE GNU "},"gitbook_doc/GitBook_Introduction.html":{"url":"gitbook_doc/GitBook_Introduction.html","title":"关于本电子书","keywords":"","body":"关于本电子书 本电子书是为笔者的开源学习资料: framework-learning 提供一个更为良好的阅读环境。 本电子书的内容会与 framework-learning 的内容进行同步，所以请放心阅读。 如果您觉得本项目对您有帮助，请不吝star。如果您有对于本项目或本电子书的建议，欢迎issue/pr，非常感谢您的对本项目的支持! 项目地址: framework-learning 笔者联系方式: Github: guang19 邮箱: 2196927727@qq.com QQ/Tim: 2196927727 "},"gitbook_doc/system_architecture_design/系统架构设计简介.html":{"url":"gitbook_doc/system_architecture_design/系统架构设计简介.html","title":"系统架构设计","keywords":"","body":"系统架构设计 我认为系统架构设计能力是我们设计系统的基本能力，当然俺肯定还没有达到那个境界， 所以这个专栏会写我对于某些系统的设计方式，如消息系统，搜索系统等等。 "},"gitbook_doc/system_architecture_design/站内消息系统的设计.html":{"url":"gitbook_doc/system_architecture_design/站内消息系统的设计.html","title":"站内消息系统的设计","keywords":"","body":"站内消息系统的设计 各位使用过简书，知乎或b站的同学应该都有这样的使用体验：当有其他用户关注我们或者私信我们的行为时， 我们会收到相关的消息。 虽然这些功能看上去简单，但其背后的设计是非常复杂的，几乎是一个完成的系统， 可以称之为站内消息系统。 我以b站举例（个人认为b站的消息系统是我见过的非常完美的，UI也最为人性化的）： 可以看到b站把消息大致分为了三类： 系统推送的通知(System Notice)； 回复，at，点赞等用户行为产生的提醒(Remind)； 用户之间的私信(Chat)。 这样设计不仅分类明确，且处于同一个主体的事件提醒还会做一个聚合，极大的提高了用户体验， 不让用户收到太多分散的消息。 举个例子：比如你在某个视频或某篇文章下发表了评论，有100个人给你的评论点了赞， 那么你希望消息页面呈现的是一个一个用户给你点赞的提醒，还是像以下聚合之后的提醒： 我相信你大概率会选择后者。 我认为对于很多应用来说，这样的设计都是非常合理的，接下来我写写我对于消息系统的设计。 系统通知(System Notice) 系统通知一般是由后台管理员发出，然后指定某一类（全体，个人等）用户接收。 基于此设想，可以把系统通知大致分为两张表： 一张记录管理员发出的通知，称之为 t_manager_system_notice 管理员系统通知表； 一张存储用户接受的通知，称之为 t_user_system_notice 用户系统通知表。 t_manager_system_notice结构如下： 字段名 类型 描述 system_notice_id LONG 系统通知ID title VARCHAR 标题 content TEXT 内容 type VARCHAR 发给哪些用户：单用户single；全体用户all，vip用户，具体类型各位同学可以根据自己的需求选择 state BOOLEAN 是否已被拉取过，如果已经拉取过，就无需再次拉取 recipient_id LONG 接受通知的用户的ID，如果type为单用户，那么recipient为该用户的ID;否则recipient为0 manager_id LONG 发布通知的管理员ID publish_time TIMESTAMP 发布时间 t_user_system_notice结构如下： 字段名 类型 描述 user_notice_id LONG 主键ID state BOOLEAN 是否已读 system_notice_id LONG 系统通知的ID recipient_id LONG 接受通知的用户的ID pull_time TIMESTAMP 拉取通知的时间 当管理员发布一条通知后，将通知插入t_manager_system_notice表中，然后系统定时的从 t_manager_system_notice表中拉取通知，然后根据通知的type将通知插入t_user_system_notice表中。 如果通知的type是single的，那就只需要插入一条记录到t_user_system_notice中。如果是全体用户， 那么就需要将一个通知批量根据不同的用户ID插入到t_user_system_notice中， 这个数据量就需要根据平台的用户量来计算。 举个例子： 管理员A发布了一个活动的通知，他需要将这个通知发布给全体用户， 当拉取时间到来时，系统会将这一条通知取出。 随后系统到用户表中查询选取所有用户的ID， 然后将这一条通知的信息根据所有用户的ID，批量插入t_user_system_notice中。 用户需要查看系统通知时，从t_user_system_notice表中查询就行了。 注意： 因为一次拉取的数据量可能很大，所以两次拉取的时间间隔可以设置的长一些。 拉取t_manager_system_notice表中的通知时，需要判断state，如果已经拉取过，就不需要重复拉取， 否则会造成重复消费。 当一条通知需要发布给全体用户时，我们应该考虑到用户的活跃度。因为如果有些用户长期不活跃， 我们还将通知推送给他（她），这显然会造成空间的浪费。 所以在选取用户ID时，我们可以将用户上次 登录的时间与推送时间做一个比较，如果用户一年未登陆或几个月未登录，我们就不选取其ID，进而避免 无谓的推送。 有的同学可能有疑问： 某条通知已经被拉取过的话，在其后注册的用户是不是不能再接收到这条通知？ 是的。但如果你想将已拉取过的通知推送给那些后注册的用户，也不是特别大的问题。 只需要再写一个定时任务，这个定时任务可以将通知的push_time与用户的注册时间比较一下，重新推送即可。 以上就是系统通知的设计了，接下来再看看较难的提醒类型的消息。 事件提醒(EventRemind) 之所以称提醒类型的消息为事件提醒，是因为此类消息均是通过用户的行为产生的，如下： xxx 在某个评论中@了你； xxx 点赞了你的文章； xxx 点赞了你的评论； xxx 回复了你的文章； xxx 回复了你的评论。 诸如此类事件，我们以单词action形容不同的事件（点赞，回复，at）。 可以看到除了事件之外，我们还需要了解用户是在哪个地方产生的事件，以便当我们收到提醒时， 点击这条消息就可以去到事件现场，从而增强用户体验，我以事件源 source 来形容事件发生的地方。 当action为点赞，source为文章时，我就知道：有用户点赞了我的某篇文章； 当action为点赞，source为评论时，我就知道：有用户点赞了我的某条评论； 当action为at， source为评论时，我就知道：有用户在某条评论里at了我； 当action为回复，source为文章时，我就知道：有用户回复了我的某篇文章； 当action为回复，source为评论时，我就知道：有用户回复了我的某条评论； 由此可以设计出事件提醒表 t_event_remind，其结构如下： 字段名 类型 描述 event_remind_id LONG 消息ID action VARCHAR 动作类型，如点赞、at、回复等 source_id LONG 事件源ID，如评论ID、文章ID等 source_type VARCHAR 事件源类型：\"Comment\"、\"Post\"等 source_content VARCHAR 事件源的内容，比如回复的内容，回复的评论等等 url VARCHAR 事件所发生的地点链接url state BOOLEAN 是否已读 sender_id LONG 操作者的ID，即谁关注了你，at了你 recipient_id LONG 接受通知的用户的ID remind_time TIMESTAMP 提醒的时间 消息聚合 消息聚合只适用于事件提醒，以聚合之后的点赞消息来说： 100人 {点赞} 了你的 {文章 ID = 1} ：《A》； 100人 {点赞} 了你的 {文章 ID = 2} ：《B》； 100人 {点赞} 了你的 {评论 ID = 3} ：《C》； 聚合之后的消息明显有两个特征，即： action和source type，这是系统消息和私信都不具备的， 所以我个人认为事件提醒的设计要稍微比系统消息和私信复杂。 如何聚合？ 稍稍观察下聚合的消息就可以发现：某一类的聚合消息之间是按照source type 和 source id来分组的， 因此我们可以得出以下伪SQL： SELECT * FROM t_event_remind WHERE recipient_id = 用户ID AND action = 点赞 AND state = FALSE GROUP BY source_id , source_type; 当然，SQL层面的结果集处理还是很麻烦的，所以我的想法先把用户所有的点赞消息先查出来， 然后在程序里面进行分组，这样会简单不少。 拓展 其实还有一种设计提醒表的做法，即按业务分类，不同的提醒存入不同的表，这样可以分为 点赞提醒表，回复提醒表，at提醒表。 我认为这种设计比第一种的更松耦合，不必所有类型的 提醒都挤在一张表里，但是这也会带来表数量的膨胀。 所以各位同学可以自行选择方案。 私信 站内私信一般都是点到点的，且要求是实时的，服务端可以采用Netty等高性能网络通信框架完成请求。 我们还是以b站为例，看看它是怎么设计的： b站的私信部分可以分为两部分： 一部分是左边的与不同用户的聊天室；另一部分是与当前正在对话的用户的对话框， 显示了当前用户与目标用户的所有消息。 按照这个设计，我们可以先设计出聊天室表 t_private_chat，因为是一对一，所以聊天室表会包含对话的两个用户的信息： 字段名 类型 描述 private_chat_id LONG 聊天室ID user1_id LONG 用户1的ID user2_id LONG 用户2的ID last_message VARCHAR 最后一条消息的内容 这里user1_id和user2_id代表两个用户的ID，并无特定的先后顺序。 接下来是私信表 t_private_message 了，私信自然和所属的聊天室有联系， 且考虑到私信可以在记录中删除（删除了只是不显示记录，但是对方会有记录，撤回才是真正的删除）， 就还需要记录私信的状态，以下是我的设计： 字段名 类型 描述 private_message_id LONG 私信ID content TEXT 私信内容 state BOOLEAN 是否已读 sender_remove BOOLEAN 发送消息的人是否把这条消息从聊天记录中删除了 recipient_remove BOOLEAN 接受人是否把这条消息从聊天记录删除了 sender_id LONG 发送者ID recipient_id LONG 接受者ID send_time TIMESTAMP 发送时间 消息设置 消息设置一般都是针对提醒类型的消息的，且肯定是由用户自己设置的。所以我想到一般有以下设置选项： 是否开启点赞提醒； 是否开启回复提醒； 是否开启@提醒； 下面是b站的消息设置： 可以看到b站还添加了陌生人选项，也就是说如果给你发送私信的用户不是你关注的用户，那么视之为陌生人私信， 就不接受。 以下是我对于消息设置的设计： 字段名 类型 描述 user_id LONG 用户ID like_message BOOLEAN 是否接收点赞消息 reply_message BOOLEAN 是否接收回复消息 at_message BOOLEAN 是否接收at消息 stranger_message BOOLEAN 是否接收陌生人的私信 总结 以上就是我对于整个站内消息系统的大概设计了，我参考了很多文章的内容以及很多网站的设计，但实际项目 的需求肯定与我所介绍的有很多出入，所以各位同学可以酌情参考。 "},"gitbook_doc/jdk-jvm-juc/Java常见基础知识点.html":{"url":"gitbook_doc/jdk-jvm-juc/Java常见基础知识点.html","title":"Java常见基础知识点","keywords":"","body":" java基础知识(部分图源:JavaGuide) 面向对象和面向过程的区别 OracleJdk与OpenJdk的区别 Java与C 的异同 JVM,JDK和JRE的区别 Java语言的特点 面向对象的特征 重载和重写的区别 接口与抽象类的区别 Object类的方法有哪些? 静态属性方法和成员属性方法区别 子类属性与父类属性初始化顺序 自动拆箱和装箱 String为什么不可变? final关键字的作用 StringBuilder和StringBuffer区别 equals知识点 深拷贝与浅拷贝 IO流分类 使用字节流还是字符流? BigDecimal Java异常体系结构 Comparable和Comparator 什么是泛型,什么是类型擦除? 泛型通配符 上界通配符 下界通配符 ?和T的区别 为什么要慎用 Arrays.asList()? Java中引用的类型 java基础知识(部分图源:JavaGuide) PS:以下部分内容希望各位同学下载openjdk的源码,亲自实践。 openjdk8u: hotspot:hotspot openjdk:jdk 面向对象和面向过程的区别 首先面向过程和面向对象的语言没有具体的性能高下之分,要依据每种语言的设计来做参考. 个人认为面向过程与面向对象的最大区别在于: 面向过程的语言是结构化的,面向对象的语言是模块化的。 模块化的代码比结构化的代码更易于维护,复用与扩展。 OracleJdk与OpenJdk的区别 OpenJdk是基于Sum捐赠的HotSpot的源代码开发的,是开源的。 OracleJdk是Oracle对Jdk的商业化版本,由Oracle公司发布并维护. 因此OracleJdk比OpenJdk更可靠(不过随着版本的迭代，二者之间的差异正在减少)。 ' Java与C++的异同 Java和C++都是基于面向对象思想的语言。 Java不提供指针来访问内存。C++允许指针访问内存。 垃圾回收机制。Java无需开发者手动释放内存,因为Java有垃圾回收机制自动回收内存; C++则需要开发者手动释放内存。因此Java在内存管理上相对C++更安全。 Java不支持多继承，而C++支持。 JVM,JDK和JRE的区别 JVM: JVM(java virtual machine)是java虚拟机 JRE: JRE(java runtime environment)是java运行时环境 JDK: JDK(java development kit)是java开发工具包,不仅包含了jre和jvm,还提供了javac编译器和javadoc等其他开发所需的工具 Java语言的特点 面向对象 平台无关性,也就是跨平台(依靠JVM) 垃圾回收机制(GC) 支持多线程 支持网络编程 编译与解释(JIT) 面向对象的特征 面向对象三大特征:封装,继承,多态。 封装: 封装是隐藏类的属性和实现细节,只对外提供可访问或修改的接口的技术。 封装的目的是为了简化编程和增加程序的安全性。 继承: 继承是 在已存在的类上定义新的类的技术。 在Java中,已存在的类被称为基类(父类),新的类叫做派生类(子类)。子类拥有父类的所有属性,方法。 但是子类对于父类中私有的方法或属性只是拥有,并不能访问和使用。 继承的目的主要是为了代码的复用. 多态: 多态指的是相同类型的对象,调用其相同的方法,参数也相同,但是它的表现形式也就是结果不同。 多态的目的是为了程序的可扩展性和维护性。 在Java中可以使用继承与接口2大特性实现多态。 重载和重写的区别 重载和重写完全没有可比性,不知道为啥老有人喜欢拿它们做比较。 重载: 重载是描述一个类中有多个方法名相同的方法，但是这些方法的参数,类型,返回值,参数的顺序可能不同，表现形式也就不同。 重写: 重写是描述子类对父类的某个方法的逻辑进行了重新编写,但重写的只是方法的内容, 方法名,参数,类型,顺序,返回值都是不变的。 接口与抽象类的区别 接口需要被实现，而抽象类需要被继承。 接口里的方法默认是公共抽象的(在jdk8中,接口被允许定义default方法,jdk9中还允许定义private私有方法)， 而抽象类既允许抽象也允许非抽象的方法。 一个类允许实现多个接口,但只允许继承一个抽象父类。 接口是对类的规范,规范的是行为能力（你能做什么）。而抽象类是对类的抽象,抽象的是逻辑。（你是什么） Object类的方法有哪些? getClass equals hashCode toString wait wait(long): 让当前Thread进入TIMED_WATING状态 wait(long,int):让当前Thread进入TIMED_WATING状态 notify notifyAll clone finalize 静态属性方法和成员属性方法区别 静态属性和方法属于类Class,而成员属性和方法属于实例化的对象。 静态方法只能使用静态方法和静态属性,不能使用成员属性和方法, 因为静态属性和方法在对象还没被实例化的时候就存在了。 简单理解就是不允许一个已存在的事物使用一个不存在的事物。 子类属性与父类属性初始化顺序 无论如何，静态数据首先加载，且如果一个类有父类，那么先加载其父类，所以先初始化父类静态变量并执行父类静态初始化块(静态变量和静态初始化块按源码编写的顺序执行， 普通初始化块和普通成员变量也是如此)，再初始化子类静态变量并执行子类静态初始化块。 普通初始化块和普通成员变量优先于构造方法,所以接下来初始化父类的普通成员变量并执行父类的普通初始化块,再调用父类构造方法。 最后初始化子类普通成员变量，执行子类普通代码块并调用构造方法。 自动拆箱和装箱 自动拆箱和装箱实际上是Java编译器的一个语法糖。 自动装箱是指: 将基本数据类型转为对应的包装类对象的过程。 自动拆箱是指: 将包装类转为对应的基本数据类型。 自动装箱实际上是调用了包装类对象的valueof方法,如: Integer.valueof(1) 自动拆箱实际上是调用了包装类的xxxValue方法,如: Integer.intValue() 在自动装箱的时候,如果包装类允许缓存并且值在缓存的范围内,那么装箱生成的对象会被缓存到常量池中。 Integer,Byte,Short,Long,Character包装类型具有缓存池, 而其他三种:Float,Double,Boolean不具有缓存池。 包装类的缓存池缓存的范围基本都为: -128 - 127之间， 除了Character的缓存范围为 0 - 127。 String为什么不可变? 先说下我的看法:String是Java中最常使用的类没有之一,如果String是可变的,那么会发生非常多数不清的问题。 如ip地址,人名,邮箱非常多的敏感数据。 如果String是可变的,就会发生安全问题, 且字符串常量池也就无从谈起了。 String是不可变的,那么它本质上也是线程安全的。 不可变类的缺点就是每个不同的值都需要创建对象 String 是用final修饰的，保证了String类不能被扩展。 String内部的字段是用final修饰的(我的jdk版本是11,String由byte[]实现)， 并且没有对外提供修改字段的方法。这也是为什么String不可变的原理。 final关键字的作用 被final修饰的类，不能被继承，并且这个类所有的成员方法都为final，不能被重写。 被final修饰的属性变量，不能被修改。如果该变量是基本数据类型的，那么其值在初始化后不能被修改。 如果该变量是引用类型的，那么该引用不能再指向其他对象。 被final修饰的方法不能被子类重写。 StringBuilder和StringBuffer区别 StringBuilder和StringBuffer都是可变的字符串,但是StringBuilder是线程不安全的。 StringBuffer是安全的,因此单线程情况下考虑使用StringBuilder,多线程情况下考虑使用StringBuffer。 他们之间的关系就好比HashMap和HashTable的关系。 equals知识点 == 和 equals区别: ==比较的是对象的内存地址,equals比较的是对象的值。 因此在Java中比较2个对象的值是否相等使用equals,判断2个对象是否是一个对象,使用==。 hashCode方法返回的真是对象内存地址吗? 这个已在对象内存布局部分有讲解，此处就不重复写了。 equals方法重写要求 自反性: x.equals(x) == true 永远成立 非空性: x.equals(null) == false 永远成立 对称性: 如果 x.equals(y) == true , 那 y.equals(x)== true 传递性: 如果 x.equals(y) == true,并且 y.equals(z) == true,那么一定满足x.equals(z) == true 一致性: 如果x.equals(y) == true , 那么只要x和y的值不变,那么x.equals(y) == true 永远成立 为什么重写equals方法一定要重写hashcode方法? 在普通环境下(不涉及hash表),equals方法和hashcode方法一毛钱关系没有的, 此时重写equals但不重写hashcode是没有关系的。 但当使用map,set这些散列表时, 它们会根据对象的hashcode来计算对象在散列表中的位置的。 试想下,如果2个对象的值相等,但是由于它们是2个对象,hashcode却不相等。 那么即使放入map,set(map)仍会存在重复数据。 深拷贝与浅拷贝 深拷贝: 拷贝所有的内容,除了基本数据类型的变量复制一份,连引用类型的变量也复制一份。 浅拷贝: 复制基本数据类型的变量,对于引用类型的变量,直接返回这个引用本身。 IO流分类 按照流的流向,分为:输入流和输入流。 按照操作单元,分为:字节流和字符流。 使用字节流还是字符流? 考虑通用性,应该使用字节流。 如果只是文本文件的操作,可以使用字符流。 BigDecimal BigDecimal是Java中表示大浮点数的类型。 在Java中,如果遇到浮点数的判断,可以使用BigDecimal来做计算, 因为如果使用普通数据类型很可能会发生精度丢失的情况,这个时候的结果可能会出乎意料之外. Java异常体系结构 在Java中,异常分为 Exception和Error,这2个类都继承自Throwable。 Exception: Exception异常是程序本身可以处理的。Exception 分为运行时异常(RuntimeException)和 非运行时异常(CheckedException)。 RuntimeException: RuntimeException(运行时异常)是在程序运行时可能会发生的异常,如NullPointException, 这类异常往往是不可预料的,编译器也不会要求你手动try catch或throws。 CheckedException: CheckedException(非运行时异常)是RuntimeException以外的异常,如IOException， 这类异常要求必须显示的try catch或throws ， 如果不处理,那么编译就不会通过。 Error: Error错误是程序无法处理的,表示程序或JVM出现了很严重的，无法解决的问题。 Comparable和Comparator Comparable: 自然排序接口。实现了它的类意味着就支持排序。 Comparator: 外部比较器。无需让需要排序的对象实现排序逻辑，而是根据Comparator定义的逻辑来排序。 Comparator相较于Comparable更加的灵活。 什么是泛型,什么是类型擦除? Java泛型(Generics) 是JDK5中引入的一个新特性，泛型提供了编译时类型安全检测机制， 该机制允许程序员在编译时检测到非法的类型。 泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。 Java中的泛型是伪泛型，在Java编译期间，所有的泛型信息都会被擦除，这就是通常所说的类型擦除。 List list = new ArrayList<>(); list.add(12); //这里直接添加会报错 list.add(\"a\"); Class clazz = list.getClass(); Method add = clazz.getDeclaredMethod(\"add\", Object.class); //但是通过反射添加，是可以的 add.invoke(list, \"kl\"); System.out.println(list) 泛型通配符 常用的 T，E，K，V，? ？ 表示不确定的 java 类型 T (type) 表示具体的一个java类型 K V (key value) 分别代表java键值中的Key Value E (element) 代表Element ? 无界通配符 一个抽象父类Animal和子类Dog，现在需要一个动物列表，我的第一反应是这样的： List listAnimals 但是老板的想法却是这样的： List listAnimals 通配符在声明局部变量时是没有什么意义的，但是当你为一个方法设置声明一个参数时，它是非常重要的。 public static void main(String[] args) { List dogs = new ArrayList<>(); countLegs(dogs); /*编译器会会报错，显示类型不匹配*/ //countLegs1(dogs); } static int countLegs(List animals){ int reVal = 0; for (Animal animal : animals) { reVal+= animal.getLegs(); } return reVal; } static int countLegs1(List animals){ int reVal = 0; for (Animal animal : animals) { reVal+= animal.getLegs(); } return reVal; } 所以，对于不确定或者不关心实际要操作的类型，可以使用无限制通配符()，表示可以持有任意类型。像countLegs方法，规定了参数传入的上界，但是并不关心具体类型是什么，对于传入的参数，只要是Animal的子类，就都可以支持，而countLegs1方法不行。 为什么countLegs1方法就不行呢？Dog不是Animal的子类吗，根据多态的角度来讲，理论上应该是可以的，但是，在泛型的继承体系中，Dog并不算Animal的一个子类。 假设我们有以下代码： animals的泛型是父类对象Animal，dogs的泛型是子类对象Dog，那么dogs转换animals能成功吗，我们知道子类对象转父类对象是可以的，但是子类泛型转父类泛型能成功吗，我们假设能成功。 public static void main(String[] args) { List animals = new ArrayList(); List dogs = new ArrayList(); animals = dogs; } 那么animals就指向了一个泛型为Dog的集合容器，但是现在这个集合容器的泛型是Animal，看上去可以将另一个子类Cat加进容器中。 public static void main(String[] args) { List animals = new ArrayList(); List dogs = new ArrayList(); animals = dogs; animals.add(new Cat(4)); } 这段代码看上去好像没什么问题，但是仔细想想。现在animals指向的是一个泛型为Dog的容器，现在又将Cat放进容器中，就相当于在一堆狗里面放进一只猫，这就矛盾了。所以子类泛型不能转换为父类泛型，反过来也是一样。 上界通配符 上界：用extend关键字指定，表示所指定的类型只能是某个类的子类或者这个类本身 static K test(K arg1,E arg2){ K result = arg1; arg1.compareTo(arg2); return result; } // 表示既要实现Comparable接口，又要实现Serializable接口 static K test(K arg1) { K result = arg1; return result; } 有多个限定类型用\"&\"隔开，有多个类型变量用\",\"逗号隔开 下界通配符 下界：用super关键字指定，表示所指定的类型只能是这个类本身或者某个类的父类，直至Object static void test1(List dst ,List list){ for (T t : list) { dst.add(t); } } ?和T的区别 ? 表示不确定的类型，常用于泛型方法的调用代码和形参，不能用于定义泛型类、泛型方法和泛型变量。 T 表示一个具体的类型，常用于泛型类和泛型方法的定义，可以定义泛型变量。 利用T来保证泛型的一致性。 // 通过 T 来 确保 泛型参数的一致性,通常我们需要先声明 public void test(List dest, List src) //通配符是 不确定的，所以这个方法不能保证两个 List 具有相同的元素类型，不需要提前声明 public void test(List dest, List src) 类型参数可以多重限定而通配符不行。 static T test(T arg1) { T result = arg1; return result; } ?可以使用下界限定，但T不行。 类型参数 T 只具有 一种 类型限定方式： T extends A 但是通配符 ? 可以进行 两种限定： ? extends A ? super A 为什么要慎用 Arrays.asList()? 因为Arrays.asList这个方法返回的根本就不是我们期盼的ArrayList, 而是Arrays类内部实现的ArrayList,这个内部类只支持访问和set操作, 并不支持remove,add,clear等修改操作。 Java中引用的类型 Java中引用类型总共有四种: 强引用，软引用，弱引用，虚引用。 强引用(Strong Reference): Java程序中绝大部分都是强引用，一般使用new关键字创建的对象就是强引用。 只要强引用存在，强引用的对象就不会被回收，除非不可达(参考jvm部分) 软引用(Soft Reference): 软引用一般不会被回收，但是当堆内存不够的时候， 比如几乎快要发生OOM的时候，就会回收掉软引用对象。 弱引用(Weak Reference): 只要垃圾回收开始，就会回收掉弱引用的对象。 虚引用(Phantom Reference,又称幽灵引用): 和其他几种引用不同，虚引用不决定对象的生命周期， 它在任何时候都可能被回收掉。 "},"gitbook_doc/jdk-jvm-juc/IO.html":{"url":"gitbook_doc/jdk-jvm-juc/IO.html","title":"IO","keywords":"","body":" Java IO 操作系统的内核 操作系统的用户态与内核态 为什么要有用户态与内核态? 用户态切换到内核态的几种方式 阻塞和非阻塞 同步与异步 Linux IO模型 阻塞IO 非阻塞IO(网络IO模型) IO多路复用(网络IO模型) 信号驱动IO(网络IO模型) 异步IO Java IO 图源: 简书 (如有侵权,请联系俺,俺会立刻删除) 操作系统的内核 操作系统的内核是操作系统的核心部分。 它负责系统的内存，硬件设备，文件系统以及应用程序的管理。 操作系统的用户态与内核态 unix与linux的体系架构：分为用户态与内核态。 用户态与内核态与内核态是操作系统对执行权限进行分级后的不同的运行模式。 为什么要有用户态与内核态? 在cpu的所有指令中，有些指令是非常危险的，如果使用不当，将会造成系统崩溃等后果。 为了避免这种情况发生，cpu将指令划分为特权级(内核态)指令和非特权级(用户态)指令。 对于那些危险的指令只允许内核及其相关模块调用，对于那些不会造成危险的指令，就允许用户应用程序调用。 内核态(核心态,特权态): 内核态是操作系统内核运行的模式。 内核态控制计算机的硬件资源，如硬件设备，文件系统等等，并为上层应用程序提供执行环境。 用户态: 用户态是用户应用程序运行的状态。 应用程序必须依托于内核态运行,因此用户态的态的操作权限比内核态是要低的， 如磁盘，文件等，访问操作都是受限的。 系统调用: 系统调用是操作系统为应用程序提供能够访问到内核态的资源的接口。 用户态切换到内核态的几种方式 系统调用: 系统调用是用户态主动要求切换到内核态的一种方式， 用户应用程序通过操作系统调用内核为上层应用程序开放的接口来执行程序。 异常: 当cpu在执行用户态的应用程序时，发生了某些不可知的异常。 于是当前用户态的应用进程切换到处理此异常的内核的程序中去。 硬件设备的中断: 当硬件设备完成用户请求后，会向cpu发出相应的中断信号， 这时cpu会暂停执行下一条即将要执行的指令，转而去执行与中断信号对应的应用程序， 如果先前执行的指令是用户态下程序的指令，那么这个转换过程也是用户态到内核台的转换。 阻塞和非阻塞 阻塞: 一个线程调用一个方法计算 1 - 100 的和，如果该方法没有返回结果， 那么调用方法的线程就一直等待直到该方法执行完毕。 非阻塞: 一个线程调用一个方法计算 1 - 100的和，该方法立刻返回，如果方法返回没有结果， 调用者线程也无需一直等待该方法的结果，可以执行其他任务，但是在方法返回结果之前， 线程仍然需要轮询的检查方法是否已经有结果。 结论: 阻塞与非阻塞针对调用者的立场而言。 同步与异步 同步: 一个线程调用一个方法计算 1 - 100 的和，如果方法没有计算完，就不返回。 异步: 一个线程调用一个方法计算 1 - 100 的和，该方法立刻返回，但是由于方法没有返回结果， 所以就需要被调用的这个方法来通知调用线程 1 - 100的结果， 或者线程在调用方法的时候指定一个回调函数来告诉被调用的方法执行完后就执行回调函数。 结论:同步和异步是针对被调用者的立场而言的。 Linux IO模型 Linux下共有5种IO模型: 阻塞IO 非阻塞IO IO多路复用 信号驱动IO 异步IO 阻塞IO 阻塞IO是很常见的一种IO模型。 在这种模型中，用户态的应用程序会执行一个操作系统的调用， 检查内核的数据是否准备好。如果内核的数据已经准备好， 就把数据复制到用户应用进程。如果内核没有准备好数据， 那么用户应用进程(线程)就阻塞，直到内核准备好数据并把数据从 内核复制到用户应用进程， 最后应用程序再处理数据。 阻塞IO是同步阻塞的。 阻塞IO的同步体现在: 内核只有准备好数据并把数据复制到用户应用进程才会返回。 阻塞IO的阻塞体现在:用户应用进程等待内核准备数据和把数据从用户态拷贝到内核态的这整个过程， 用户应用进程都必须一直等待。 当然,如果是本地磁盘IO,内核准备数据的时间可能会很短。 但网络IO就不一样了，因为服务端不知道客户端何时发送数据， 内核就仍需要等待socket数据，时间就可能会很长。 阻塞IO的优点是对于数据是能够保证无延时的，因为应用程序进程会一直阻塞直到IO完成。 但应用程序的阻塞就意味着应用程序进程无法执行其他任务， 这会大大降低程序性能。一个不太可行的办法是为每个客户端socket都分配一个线程， 这样就会提升server处理请求的能力。不过操作系统的线程资源是有限的， 如果请求过多，可能造成线程资源耗尽，系统卡死等后果。 非阻塞IO(网络IO模型) 在非阻塞IO模型中，用户态的应用程序也会执行一个操作系统的调用， 检查内核的数据是否准备完成。如果内核没有准备好数据, 内核会立刻返回结果,用户应用进程不用一直阻塞等待内核准备数据， 而是可以执行其他任务,但仍需要不断的向内核发起系统调用，检测数据是否准备好， 这个过程就叫轮询。 轮询直到内核准备好数据，然后内核把数据拷贝到用户应用进程， 再进行数据处理。 非阻塞IO的非阻塞体现在: 用户应用进程不用阻塞在对内核的系统调用上 非阻塞IO的优点在于用户应用进程在轮询阶段可以执行其它任务。 但这也是它的缺点，轮询就代表着用户应用进程不是时刻都会发起系统调用。 可能数据准备好了，而用户应用进程可能等待其它任务执行完毕才会发起系统调用， 这就意味着数据可能会被延时获取。 IO多路复用(网络IO模型) 在IO多路复用模型中,用户应用进程会调用操作系统的select/poll/epoll函数, 它会使内核同步的轮询指定的socket， (在NIO中,socket就是注册到Selector上的SocketChannel,可以允许多个) 直至监听的socket有数据可读或可写，select/poll/epoll函数才会返回, 用户应用进程也会阻塞的等待select/poll/epoll函数返回。 当select/poll/epoll函数返回后，即某个socket有事件发生了，用户应用进程就会 发起系统调用，处理事件，将socket数据复制到用户进程内，然后进行数据处理。 IO多路复用模型是同步阻塞的 IO多路复用模型的同步体现在: select函数只有监听到某个socket有事件才会返回。 IO多路复用模型的阻塞体现在: 用户应用进程会阻塞在对select函数上的调用上。 IO多路复用的优点在于内核可以处理多个socket， 相当于一个用户进程(线程)就可以处理多个socket连接。 这样不仅降低了系统的开销，并且对于需要高并发的应用是非常有利的。 而非阻塞IO和阻塞IO的一个用户应用进程只能处理一个socket， 要想处理多socket，只能新开进程或线程，但这样很消耗系统资源。 PS: 在IO多路复用模型中, socket一般应该为非阻塞的， 这就是Java中NIO被称为非阻塞IO的原因。 但实际上NIO属于IO多路复用，它是同步阻塞的IO。 具体原因见 知乎讨论 PS: select/poll/epoll函数是IO多路复用模型的基础，所以如果想 深入了解IO多路复用模型，就需要了解这3个函数以及它们的优缺点。 信号驱动IO(网络IO模型) 在信号驱动IO模型中，用户应用进程发起sigaction系统调用,内核收到并立即返回。 用户应用进程可以继续执行其他任务，不会阻塞。当内核准备好数据后向用户应用进程 发送SIGIO信号，应用进程收到信号后，发起系统调用， 将数据从内核拷贝到用户进程， 然后进行数据处理。 个人感觉在内核收到系统调用就立刻返回这一点很像异步IO的方式了，不过 与异步IO仍有很大差别。 异步IO 在异步IO模型中，用户进程发起aio_read系统调用，无论内核的数据是否准备好， 都会立即返回。用户应用进程不会阻塞,可以继续执行其他任务。当内核准备好数据, 会直接把数据复制到用户应用进程。最后内核会通知用户应用进程IO完成。 异步IO的异步体现在:内核不用等待数据准备好就立刻返回， 所以内核肯定需要在IO完成后通知用户应用进程。 弄清楚了阻塞与非阻塞，同步与异步和上面5种IO模型，相信再看 Java中的IO模型，也只是换汤不换药。 BIO : 阻塞IO NIO : IO多路复用 AIO : 异步IO 本来打算写Java中的IO模型的，发现上面几乎讲完了(剩API使用吧)，没啥要写的了， 所以暂时就这样吧。如果各位同学有好的建议，欢迎pr或issue。 "},"gitbook_doc/jdk-jvm-juc/Java集合.html":{"url":"gitbook_doc/jdk-jvm-juc/Java集合.html","title":"Java集合","keywords":"","body":" java集合 线程不安全的集合 HashMap的特点 HashMap的长度(容量)为什么要设计成2的幂？ HashTable的特点 TreeMap ArrayList的特点 Vector的特点 LinkedList的特点 Set ConcurrentModificationException异常 线程安全的集合 线程安全的 List CopyOnWriteArrayList 线程安全的Set 线程安全的Map ConcurrentHashMap ConcurrentSkipListMap java集合 线程不安全的集合 HashMap的特点 HashMap在Jdk8之前使用拉链法实现,jdk8之后使用拉链法+红黑树实现。 HashMap是线程不安全的,并允许null key 和 null value。 HashMap在我当前的jdk版本(11)的默认容量为0,在第一次添加元素的时候才初始化容量为 16, 之后才扩容为原来的2倍。 HashMap的扩容是根据 threshold决定的 : threshold = loadFactor * capacity。 当 size 大于 threshold 时,扩容。 当某个桶的元素数量达到指定的阈值TREEIFY_THRESHOLD(8)时，HashMap会判断当前数组的 长度是否大于MIN_TREEIFY_CAPACITY(64),如果大于，那么这个桶的链表将会转为红黑树，否则HashMap将会扩容。 当某个桶的红黑树节点的数量小于等于指定的阈值UNTREEIFY_THRESHOLD(6)时，那么在扩容的时候，这个桶的红黑树将转为链表。 HashMap的长度(容量)为什么要设计成2的幂？ 这就不得不佩服大师们的设计。 想想看，一个对象的hashcode是很大的，当HashMap的容量仅为16,32时， 如何根据hashcode来确定key在数组中的下标。 一个好的办法就是取余: hashcode % length。 这样就能确保，key的下标是永远不会超过数组的长度的。 但是想想，除了取余有没有更好的办法， 当然有: hash % length == hash & (length - 1) 为什么上面这个性能超高的等式成立，当然是有条件的， 只有当length为2的幂的时候这样的等式才成立, 这就明白了为什么使用2的幂来定义HashMap的长度。 HashTable的特点 HashTable底层使用拉链法实现。 HashTable就像Vector一样,也是jdk1就存在的很古老的一个类，它是线程安全的， 实现线程安全的手段是使用synchronized。 HashTable的默认容量为16，每次扩容为原来的2倍+1。 HashTable不允许null key 和 null value。 TreeMap TreeMap使用红黑树实现,不允许null key,允许自然排序Comparable和比较器Comparator排序。 ArrayList的特点 ArrayList底层使用Object数组实现。 ArrayList的容量默认为0,只有在第一次执行add操作时才会初始化容量为10，正常的扩容是为原来的1/2倍。 由于ArrayList采用数组实现,它的容量是固定的,所以当添加新元素的时候,如果超出了数组的容量, 那么此时add操作的时间复杂度将会是O(n-1)。 ArrayList实现了RandomAccess接口，该接口没有具体的规范，只是一个标记， 这代表ArrayList支持快速的随机访问。 ArrayList在内存空间利用率上肯定是不如LinkedList的， 因为数组是一片固定的连续的内存空间，一旦分配就无法改变， 所以难免会有空间不足或空间使用率很低的情况。 Vector的特点 ArrayList是线程不安全的，Vector是线程安全的， 但Vector实现线程安全的手段是synchronized。这就好比HashMap与HashTable的区别。 Vector默认容量为10。 Vector是当它的扩容增量大于0时，会扩容为原来的容量+扩容增量，否则扩容为原来的2倍。 LinkedList的特点 LinkedList底层使用双端链表实现。 LinkedList的add操作只需要改变尾节点的引用就行了。 但是如果需要在指定位置进行add操作的话，那么时间复杂度也是比较高的,为O(n)， 因为需要从头节点或尾节点遍历到需要操作的节点。 LinkedList的空间利用率虽然很高，但是它的每个Node可以说也是占用了较大空间的， 因为每个Node需要保存它的前继和后继节点。 PS: 双端链表与双向链表的区别: 双端链表:每个Node都保存了前后2个节点的引用，双向链表的first节点的前一个节点为null, last节点的后一个节点为null。 双向链表: 每个Node都保存了前后2个节点的引用， 双向链表的first节点的前一个节点指向last节点， last节点的最后一个节点指向first节点。 Set 为啥不单独说HashSet，我目前看到的JDK所有的Set,都是使用Map实现的, 除了CopyOnWriteArraySet(底层是CopyOnWriteArrayList)。 TreeSet --> TreeMap LinkedHashSet --> LinkedHashMap HashSet --> HashMap ConcurrentSkipListSet --> ConcurrentSkipListMap Set是如何保证元素不会重复,这个得看各自Map的实现了。 拿HashMap来讲，它先判断key的hash是否相等，然后才使用equals判断2个对象是否相等。 ConcurrentModificationException异常 ConcurrentModificationException可以从名字看出是并发修改的异常。 但我要说的是这个异并不是在修改的时候会抛出的，而是在调用迭代器遍历集合的时候才会抛出。 而集合类的大部分toString方法，都是使用迭代器遍历的。所以如果多线程修改集合后， 接着就遍历集合，那么很有可能会抛出ConcurrentModificationException。 在ArrayList，HashMap等非线程安全的集合内部都有一个modCount变量， 这个变量是在集合被修改时(删除，新增)，都会被修改。 如果是多线程对同一个集合做出修改操作，就可能会造成modCount与实际的操作次数不符， 那么最终在调用集合的迭代方法时，modCount与预期expectedModeCount比较， expectedModCount是在迭代器初始化时使用modCount赋值的， 如果发现modCount与expectedModeCount不一致，就说明在使用迭代器遍历集合期间， 有其他线程对集合进行了修改,所以就会抛出ConcurrentModificationException异常。 线程安全的集合 线程安全的 List 使用集合工具类Collections的 synchronizedList把普通的List转为线程安全的List.(不推荐) 使用Vector.(不推荐) 使用CopyOnWriteArrayList,推荐使用此种方法，因为以上2种全部都是单纯的Synchronized加锁. CopyOnWriteArrayList CopyOnWriteArrayList是线程安全的ArrayList，可以被称为写时复制的ArrayList。 CopyOnWriteArrayList底层仍然使用数组实现， 但是它的修改操作(增删改)采用synchronized关键字保证并发的安全性， 然后在进行修改的时候复制原来的数组到一个新副本，对新副本进行修改，修改完后再设置原数组。 这样就不会让写操作影响读操作了。 但是CopyOnWriteArrayList不容忽视的缺点就是修改操作比较消耗内存空间，所以它适用于读多写少的环境。 线程安全的Set 使用集合工具类的Collections的synchronizedSet把普通的set转为线程安全的set(不推荐) 使用CopyOnWriteArraySet,此set适用于读多写少的情况，它的底层采用CopyOnWriteArrayList实现. 使用ConcurrentSkipListSet，底层采用ConcurrentSkipListMap实现 线程安全的Map 使用集合工具类Collections的synchronizedMap把普通map转为线程安全的map(不推荐) HashTable(不推荐) 使用ConcurrentHashMap(常用) ConcurrentSkipListMap(跳表map,推荐) ConcurrentHashMap ConcurrentHashMap使用数组+链表/红黑树实现,其扩容机制与HashMap一样。 但是ConcurrentHashMap控制并发的方法改为了CAS+synchronized, synchronized锁的只是链表的首节点或红黑树的首节点。 PS:我只看了常用的put,get,remove等核心方法的源码. 整个ConcurrentHashMap的实现用\"复杂\"来形容一点也不为过, 你只要想到它内部有52个内部类就知道有多复杂了,但如果不考虑并发CAS这部分， ConcurrentHashMap和普通的HashMap的差别是不大的。 ConcurrentSkipListMap ConcurrentSkipListMap是基于跳表这种数据结构实现的。 跳表比较特殊，它由多个层次的链表组成，每层链表又有多个索引节点连接， 每层链表的元素也都是有序的。处于上层索引的链表都是下层链表的子集。 跳表与普通链表相比查找元素的效率更高。 "},"gitbook_doc/jdk-jvm-juc/Java多线程.html":{"url":"gitbook_doc/jdk-jvm-juc/Java多线程.html","title":"Java多线程","keywords":"","body":" 多线程 进程和线程 并发和并行 多线程的利弊 什么是上下文切换? 线程的优先级 线程的几种状态 sleep方法和wait方法的区别 stop,suspend,resume等方法为什么会被遗弃 interrupt,interrupted,isInterrupted方法区别 join方法 yield方法 多线程 进程和线程 进程与线程最主要的区别是它们是操作系统管理资源的不同方式的体现。 准确来说进程与线程属于衍生关系。 进程是操作系统执行程序的一次过程,在这个过程中可能会产生多个线程。 比如在使用QQ时，有窗口线程， 文字发送的线程，语音输入的线程，可能不是很恰当。 由于系统在线程之间的切换比在进程之间的切换更高效率，所以线程也可以被视为轻量级进程。 并发和并行 并发: 多个线程任务被一个cpu轮流执行。注意，这里并不是只允许一个cpu执行多任务，多个cpu执行也是可以的。 并发强调的是计算机应用程序有处理多个任务的能力。 并行:多个线程被多个cpu同时执行。这里也并不是只允许多个cpu处理多任务，一个cpu也是可以的， 只要cpu能在同一时刻处理多任务。并行强调的是计算机应用程序拥有同时处理多任务的能力。 多线程的利弊 利: 线程可以比作轻量级的进程，cpu在线程之间的切换比在进程之间的切换，耗费的资源要少的多。 现在是多核cpu时代，意味着多个线程可以被多个cpu同时运行(并行)，如果可以利用好多线程，那么可以编写出高并发的程序。 弊: 虽然线程带来的好处很多，但是并发编程并不容易，如果控制不好线程，那么就可能造成死锁，资源闲置，内存泄露等问题。 什么是上下文切换? cpu是采用时间片的轮转制度，在多个线程之间来回切换运行的。 当cpu切换到另一个线程的时候，它会先保存当前线程执行的状态， 以便在下次切换回来执行时，可以重新加载状态，继续运行。 从保存线程的状态再到重新加载回线程的状态的这个过程就叫做上下文切换。 线程的优先级 在Java中可以通过Thread类的setPriority方法来设置线程的优先级， 虽然可以通过这样的方式来设置线程的优先级，但是线程执行的先后顺序并不依赖与线程的优先级。 换句话说就是，线程的优先级不保证线程执行的顺序。 线程的几种状态 见:jdk Thread类源码中的state枚举类 NEW,RUNNABLE,BLOCKED,WAITING,TIMED_WAITING,TERMINATED sleep方法和wait方法的区别 sleep方法是Thread类的方法；而wait方法是Object类的方法。 sleep方法会使当前线程让出cpu的调度资源，从而让其他线程有获得被执行的机会， 但是并不会让当前线程释放锁对象（抱着锁睡觉）; 而wait方法是让当前线程释放锁并进入wait状态， 不参与获取锁的争夺，从而让其他等待资源的线程有机会获取锁， 只有当其他线程调用notify或notifyAll方法是，被wait的线程才能重新与其他线程一起争夺资源。 stop,suspend,resume等方法为什么会被遗弃 stop: stop方法被弃用很好理解，因为stop方法是强行终止线程的执行， 不管线程的run方法是否执行完，资源是否释放完，它都会终止线程的运行，并释放锁。 显然，这在设计上就不合理。 suspend和resume: suspend方法用于阻塞一个线程,但并不释放锁， 而resume方法的作用只是为了恢复被suspend的线程。 假设A，B线程都争抢同一把锁，A线程成功的获得了锁， 然后被suspend阻塞了，却并没有释放锁，它需要其他线程来唤醒， 但此时B线程需要获得这把锁才能唤醒A，所以此时就陷入了死锁。 interrupt,interrupted,isInterrupted方法区别 interrupt: 这个方法并不是中断当前线程，而是给当前线程设置一个中断状态。 isInterrupted: 当线程调用interrupt方法后，线程就有了一个中断状态， 而使用isInterrupted方法就可以检测到线程的中断状态。 interrupted: 这个方法用于清除interrupt方法设置的中断状态。 如果一个线程之前调用了interrupt方法设置了中断状态， 那么interrupted方法就可以清除这个中断状态。 join方法 join方法的作用是让指定线程加入到当前线程中执行。 假如在main方法里面创建一个线程A执行，并调用A的join方法， 那么当前线程就是main，指定的A线程就会在main之前执行， 等A执行完后，才会继续执行main。 public static void main(String[] args) throws Exception { Thread a = new Thread(()-> { try { TimeUnit.SECONDS.sleep(1); }catch (Exception e){} System.out.println(\"thread join\"); }); a.start(); //a会在main线程之前执行 a.join(); System.out.println(\"main\"); } join方法的底层是wait方法，调用A线程(子线程)的join方法实际上是让main线程wait， 等A线程执行完后，才能继续执行后面的代码。 yield方法 yield属于Thread的静态方法， 它的作用是让当前线程让出cpu调度资源。 yield方法其实就和线程的优先级一样，你虽然指定了， 但是最后的结果不由得你说了算， 即使调用了yield方法，最后仍然可能是这个线程先执行， 只不过说别的线程可能先执行的机会稍大一些。 "},"gitbook_doc/jdk-jvm-juc/Java并发.html":{"url":"gitbook_doc/jdk-jvm-juc/Java并发.html","title":"Java并发","keywords":"","body":" 并发 synchronized synchronized底层原理 synchronized 使用方法 Synchronized和ReentrantLock的区别 乐观锁 悲观锁 独占锁 共享锁 公平锁 非公平锁 可重入锁(递归锁) 偏向锁 轻量级锁 自旋锁 自适应自旋锁 锁消除 锁粗化 死锁 如何避免死锁? volatile volatile保证内存的可见性 volatile禁止指令重排序 volatile如何禁止指令重排序的? volatile不保证原子性 CAS CAS在JAVA中的底层实现(Atomic原子类实现) CAS的缺点 解决ABA问题 ThreadLocal ThreadLocal引发的内存泄露: 线程池的好处 线程池构造参数 阿里巴巴开发者手册不建议开发者使用Executors创建线程池 并发 synchronized synchronized是jdk提供的jvm层面的同步机制。 它解决的是多线程之间访问共享资源的同步问题,它保证了 在被它修饰的方法或代码块同一时间只有一个线程执行。 java6之前的synchronized属于重量锁,性能较差, 它是基于操作系统的Mutex Lock互斥量实现的。 因为java线程是映射到操作系统的线程之上的, 所以暂停或唤醒线程都需要Java程序从用户态转换为内核态,这段转换时间消耗较长。 java6之后jvm团队对synchronized做出了非常大的优化。 synchronized底层原理 先看我编写的一段测试代码: 使用 javap -c -v -l 指令反编译 class文件后的 字节码指令 如下 可以清楚的看到,在进入synchronized的时候，底层字节码编译出来的指令为 monitorenter,在执行完同步代码块后又有一个monitorexit指令. 想了解synchronized究竟是如何实现的,可以直接进入openjdk:src/share/vm/runtime 目录, 这个目录存放的是hotspot虚拟机在运行时所需的代码。 可以直接锁定其中的 objectMonitor.cpp源文件和objectMonitor.hpp头文件. 看到这2个文件，相信各位同学应该就知道，这个就是synchronized锁对象的monitor，它也是 一个对象,不过它是一个c++对象(见:objectMonitor.hpp头文件): 其实真正的锁应该是这个monitor cpp对象,synchronized锁的那个java对象起到的只是关联monitor的作用, 只不过我们身在java层面，无法感知到jvm层面monitor的作用，所以才称synchronized的java锁对象为锁。 以下是monitorenter指令执行过程(见 InterpreterRuntime.cpp): PS:本来想真正弄清楚fast_enter(偏向锁的实现),slow_enter(轻量级锁实现)和inflate(膨胀锁实现) 的,无奈暂时看不太懂cpp源码，但是有的地方是可以根据语义来推断的。 这里做一个总结吧,这个总结可能不太准确，但大致是这样的: 每次执行monitorenter指令的时候,是将当前synchronized锁对象 关联的monitor的_recursions加1, 执行monitorexit指令的时候,将当前object对象关联的monitor的_recursions减1, 当_recursions为0的时候，就说明线程不再持有锁对象。 如果熟悉AQS原理的同学就知道在AQS内部， 有一个被volatile修饰state变量， 这个state变量就是AQS的核心， state变量的作用类比到此处就是monitor计数器的作用。 synchronized 使用方法 修饰静态方法: 修饰静态方法是给类加锁,会作用于所有对象,因为静态方法属于类, 而不属于对象,不管有多少个对象,static方法都是共享的。 修饰实例方法: 修饰实例方法是给对象加锁,会作用于当前类的实例对象。 修饰代码块: 修饰代码块,根据代码块给定的对象加锁,线程想要进入代码块,只能获取指定的对象的锁。 Synchronized和ReentrantLock的区别 Synchronized是基于JVM层面的同步机制;而ReentrantLock是基于Java API层面提供的同步机制。 Synchronized和Reentrantlock都属于可重入锁。 ReentrantLock提供了比Synchronized更高级的功能: 公平锁 更方便的线程间的通信(Condition) 等待可中断(在线程等待获取锁的时候可以被中断) 乐观锁 乐观锁对共享的数据很乐观，认为不会发生线程安全的问题，从而不给数据加锁。 乐观锁适用于读多写少的环境。常见的例子就是mysql的更新使用version控制。 CAS属于乐观锁。 悲观锁 悲观锁对共享的数据很悲观，认为无论什么时候都有可能发生线程安全的问题， 所以在每次读写数据的时候都会加锁。 Synchronized属于悲观锁。 独占锁 锁一次只能被一个线程占有使用。 Synchronized和ReetrantLock都是独占锁。 共享锁 锁可以被多个线程持有。 对于ReentrantReadWriteLock而言,它的读锁是共享锁,写锁是独占锁。 公平锁 公平锁指根据线程在队列中的优先级获取锁,比如线程优先加入阻塞队列,那么线程就优先获取锁。 非公平锁 非公平锁指在获取锁的时候,每个线程都会去争抢,并且都有机会获取到锁,无关线程的优先级。 可重入锁(递归锁) 一个线程获取到锁后,如果继续遇到被相同锁修饰的资源,那么可以继续获取该锁。 Synchronized和Reentrantlock都是可重入锁。 偏向锁 在线程获取偏向锁的时候, jvm会判断锁对象MarkWord里偏向线程的ID是否为当前线程ID。 如果是,则说明当前锁对象处于偏向状态。 如果不是,则jvm尝试CAS把对象的MarkWord的偏向线程ID设置为当前线程ID, 如果设置成功,那么对象偏向当前线程，并将当对象的锁标志位改为01。 如果设置失败，则说明多线程竞争，将撤销偏向锁，升级为轻量级锁。 偏向锁适用于单线程无锁竞争环境(单线程环境)。 hotspot偏向锁实现(faster_enter): 轻量级锁 在线程获取对象锁时，jvm首先会判断对象是否为无锁状态(无锁状态标志位为01)。 如果对象是无锁状态，那么将在线程的栈帧中开辟一块空间用于存储对象的MarkWord， 然后将对象的MarkWord复制到栈帧空间去，并使用CAS更新对象的MarkWord为指向 线程栈帧的指针。 如果更新成功，那么当前线程获取锁成功，并修改对象的MarkWord标志位 为 00 。 如果更新失败，那么jvm会判断对象的MarkWord是否已经指向线程的栈帧。 如果已经指向，那么线程直接执行同步代码。否则，说明多个线程竞争，将inflate为重量级锁。 轻量级锁适用于多线程无锁竞争环境(多线程轮流执行,并不会发生冲突)。 hotspot轻量级锁实现(slow_enter): 自旋锁 在争夺锁的过程中，线程不会停止获取锁，而是通过CAS不断的判断线程是否符合获取锁的条件。 AQS获取锁的核心就是CAS。 自适应自旋锁 自旋锁意味着线程会不断的消耗cpu资源，短时间还行，长时间就意味着而资源的浪费。 所以自适应自旋锁会有一个自旋的生命周期,过了这个生命周期,线程将不再自旋。 网上有文章说这个生命周期依据前一个线程的自旋时间来决定，但是我暂且没有找到相关资料，不敢妄自揣测。 锁消除 锁消除属于Java编译器对程序的一种优化机制。 锁消除是指当JVM的JIT编译器检测出一些已经加锁的代码不可能出现共享的数据存在竞争的问题， 会消除这样的锁。锁消除的依据来源于逃逸分析算法。 如果判断到一段代码，在堆上的数据不会逃逸出去被其他线程访问到， 那么就把它们当做栈上的数据，为线程私有的，自然无需同步加锁。 //每次线程进入此方法，创建的都是不同的StringBuffer临时对象, //也就是说 StringBuffer 临时对象不会逃出方法t,作用于外部, //所以根本不存在线程之间的竞争，那么JIT在编译时就会消除append方法的锁 public String t(String s1, String s2,String s3) { return new StringBuffer().append(s1).append(s2) .append(s3).toString(); } 锁粗化 当虚拟机检测出一系列连续的操作都对同一个连续加锁， 那么它会把加锁的返回扩大至整个操作的序列的外部，保证只加锁一次。 public String t() { StringBuffer stringBuffer = new StringBuffer(); for (int i = 0 ; i 死锁 死锁是指多个进程在执行过程中,循环等待彼此占有的资源而导致程序的无限期的阻塞的情况。 产生死锁的条件: 互斥条件: 一个资源在一段时间内只能被一个进程所持有。 不可抢占条件: 进程所持有的资源只能由进程自己主动释放,其他资源的申请者不能向进程持有者抢夺资源。 占有且申请条件: 进程已经持有一个资源后,又申请其他资源,但是其他资源已被其他线程所占有。 循环等待条件: 在条件3之上,进程1有进程2需要申请的资源,进程2有进程1需要申请的资源。那么这2个线程 不停等待彼此持有的资源,又不能释放已拥有的资源,陷入循环等待。 死锁: 如何避免死锁? 只要打破死锁产生的4个条件之一就行,但是真正能够被打破的条件只有第3和第4个条件。 因为第1和第2个条件都是锁的必要条件。 所以有如下解决死锁的方案: 可以打破第3个条件: 实现资源的有序分配。 可以打破第4个条件: 设置等待超时时间。 volatile volatile是JVM提供的轻量级的线程同步机制。它可以保证内存的可见性，禁止指令重排序。 但是volatile，并不能保证数据的原子性，所以它不合适作为线程同步的工具。 volatile保证内存的可见性 可见性是指一个线程的对于共享数据的修改对其他线程是可见的。 jvm的内存模型是: 线程总是从主内存读取变量到工作内存，然后在工作内存中进行修改， 在修改完变量后，才把变量同步到主内存中。 如果多个线程同时读取了一个变量到各自的内存中，其中一个线程对变量进行了修改，并同步回了主内存， 但其它线程仍然使用的是原来的旧值，这就造成了数据的不一致。 解决这个问题的办法就是给变量加上volatile关键字修饰。 volatile使得被它修饰的变量在被线程修改后，那么线程就需要把修改后的变量重新同步到主内存， 且其他线程每次使用这个变量，都需要从主内存读取。 volatile禁止指令重排序 指令重排序是编译器和cpu为了程序的高效运行的一种优化手段, 指令重排序只能保证程序执行的结果是正确的，但是无法保证程序指令运行的顺序是否与代码的顺序一致, volatile就禁止了这种重排序。 比如: 1. int a = 1; 2. int b = 3; 3. int c = a + b; 上面的代码在编译后,指令执行的顺序可能有: 1,2,3和2,1,3 这样程序实际执行的顺序可能与代码的顺序不符,但并不会影响程序最终的结果。 volatile如何禁止指令重排序的? volatile通过提供内存屏障来防止指令重排序。 java内存模型会在每个volatile写操作前后都会插入store指令，将工作内存中的变量同步回主内存。 在每个volatile读操作前后都会插入load指令，从主内存中读取变量。 volatile不保证原子性 比如: i++ 如果是多线程环境下，一个线程读取到i的值到工作内存，然后对i做出自增操作， 然后写回主内存，其它内存才知道i的值被修改了，这个过程本身就不是原子的。 所以不能拿volatile来带替synchronized,如果是多线程环境，仍然需要使用synchronized保证线程同步。 CAS CAS: Compare And Swap 比较成功并交换。 CAS体现的是一种乐观锁的机制。 CAS涉及到3个元素: 指定的内存地址,期盼值和目标值。 将指定内存地址的值与期盼值相比较，如果比较成功就将内存地址的值修改为目标值。 CAS在JAVA中的底层实现(Atomic原子类实现) CAS在Java中的实现是 juc的atomic包下的Atomicxx原子类。 而这些Atomic原子类的核心是: Unsafe类 。 Unsafe类是个final类，它的核心方法都是native的， 因为Java无法像C/C++一样使用指针来操作内存, Unsafe类就解决了这个问题。 拿incrementAndGet方法来说， Unsafe首先调用getAndAddInt方法, 它会根据当前Atomic的value在内存中地址获取到当前对象的值, 然后再重复此操作，把之前获得的值与第二遍获得的值进行比较， 如果成功，就把内存地址的值更新为新值，否则就do while循环. 并且有个重要的细节就是,Atomic原子类内部的value值都是由volatile修饰的, 这就使得Atomic的value值是对其他线程可见的。 CAS的缺点 循环时间开销大: 我在看源码的时候，发现Atomic的CAS操作并没有进行CAS失败的退出处理， 只是单纯的循环比较并交换，这就让我很担心它的性能问题， 如果长时间不成功，那会是很可怕的一件事请，至少cpu的负荷会很大。 只能保证一个共享变量的原子操作: Atomic原子类只能保证一个变量的原子操作， 如果是多数据的话，还是考虑用互斥锁来实现数据的同步吧 ABA问题: ABA问题是指如果一个线程进行CAS操作并成功了，却不代表这个过程就是没有问题的。 假设2个线程读取了同一份数据，线程1修改了这个值并把它改回了原值，并同步到主内存中， 另一个线程准备进行CAS操作,当它发现原值和期盼的值是一样的，那么CAS仍然成功。 解决ABA问题 在juc的atomic包中提供了 AtomicStampedReference 类, 这个类较普通的原子类新增了一个stamp字段，它的作用相当于version。 每次修改这个引用的值，也都会修改stamp的值， 当发现stamp的值与期盼的stamp不一样，也会修改失败. 这就类似于以version实现乐观锁一样。 ThreadLocal ThreadLocal为每个线程都提供了一份相同的变量的副本， 每个线程都可以修改这个副本，但不用担心与其他线程发生数据冲突， 实现了线程之间的数据隔离。 ThreadLocal的原理还得从Thread线程类说起， 每个Thread类内部都有一个ThreadLocalMap，当使用ThreadLocal的get和remove操作的时候， 就是使用每个线程的ThreadLocalMap的get和remove。 ThreadLocal引发的内存泄露 在ThreadLocalMap中，key是使用弱引用的ThreadLocal存储的。 弱引用是只要垃圾回收器开始回收，无论内存是否充足，都会回收掉弱引用对象，如此一来， 当ThreadLocal被回收掉,那么ThreadLocalMap将可能出现Null Key 的 value。但是也不必太过担心， 因为设计者已经想到了这点，所以ThreadLocal会自动处理key 为 null的 value。 线程池的好处 http连接池，数据库连接池，线程池等都是利用了池化技术。 如果一个资源需要多次使用并且很昂贵，那么使用new创建的对象或资源，可能会带来较大的消耗。 池化技术的好处在于 方便资源的管理，无需显示的使用new创建。 降低了资源的消耗，在池子里的资源可以重复利用 提供了任务的响应速度，任务可以很快的被分配资源进行处理。 线程池构造参数 new ThreadPoolExecutor (int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize: 线程池的核心线程数(常驻线程数),也就是线程池的最小线程数,这部分线程不会被回收. maximumPoolSize: 线程池最大线程数,线程池中允许同时执行的最大线程数量 keepAliveTime: 当线程池中的线程数量超过corePoolSize，但此时没有任务执行， 那么空闲的线程会保持keepAliveTime才会被回收，corePoolSize的线程不会被回收。 unit: KeepAliveTime的时间单位 workQueue: 当线程池中的线程达到了corePoolSize的线程数量， 并仍然有新任务，那么新任务就会被放入workQueue。 threadFactory: 创建工作线程的工厂,也就是如何创建线程的,一般采用默认的 handler: 拒绝策略，当线程池中的工作线程达到了最大数量， 并且阻塞队列也已经满了，那么拒绝策略会决定如何处理新的任务。ThreadPoolExecutor 提供了四种策略: AbortPolicy(是线程池的默认拒绝策略): 如果使用此拒绝策略，那么将对新的任务抛出RejectedExecutionException异常，来拒绝任务。 DiscardPolicy: 如果使用此策略，那么会拒绝执行新的任务，但不会抛出异常。 DiscardOldestPolicy: 如果使用此策略，那么不会拒绝新的任务，但会抛弃阻塞队列中等待最久的那个线程。 CallerRunsPolicy: 如果使用此策略，不会拒绝新的任务，但会让调用者执行线程。 也就是说哪个线程发出的任务，哪个线程执行。 阿里巴巴开发者手册不建议开发者使用Executors创建线程池 newFixedThreadPool和newSingleThreadPoolExecutor都是创建固定线程的线程池, 尽管它们的线程数是固定的，但是它们的阻塞队列的长度却是Integer.MAX_VALUE的,所以， 队列的任务很可能过多，导致OOM。 newCacheThreadPool和newScheduledThreadPool创建出来的线程池的最大线程数量却是Integer.MAX_VALUE的， 如果任务数量过多,也很可能发生OOM。 "},"gitbook_doc/jdk-jvm-juc/AQS.html":{"url":"gitbook_doc/jdk-jvm-juc/AQS.html","title":"AQS","keywords":"","body":" AQS(AbstractQueuedSynchronizer) AQS概述 AQS的两种共享资源的访问方式 lock,tryLock和lockInterruptibly区别 CountDownLatch Semaphore CycliBarrier ReentrantReadWriteLock如何区分读写锁的? AQS(AbstractQueuedSynchronizer) AQS是Doug Lea大师为JDK编写的一套基于API层面的抽象队列同步器. AbstractQueuedSynchronizer,抽象队列同步器. Lock,CountDownLatch等等这些并发工具都是基于AQS来实现的。 由此可以看出Doug Lea大师的功力已经臻至化境 AQS概述 AQS的核心思想是如果被请求的资源空闲，那么就将当前请求资源的线程设置为有效的工作线程; 如果请求的资源被其他线程所占有， 那么就使用CLH线程阻塞队列来提供阻塞线程并唤线程分配资源的机制。 在CLH队列中，每个请求资源的线程都会被封装成队列中的一个节点。 在AQS内部有一个int类型的state表示线程同步状态， 当线程lock获取到锁后，该state计数就加1,unlock就减1， 这就是为什么解锁次数要对应加锁次数的原因。 AQS主要实现技术为:CLH队列(Craig,Landin and Hagersten)， 自旋CAS，park(阻塞线程)以及unparkSuccessor(唤醒阻塞队列中的后继线程)。 AQS的两种共享资源的访问方式 AQS定义了两种共享资源方式. 独占式(Exclusive): 同一时间只有一个线程可以访问共享资源,也就是独占锁。 如:Synchronized,ReentrantLock。 对于独占式锁的实现,在AQS中对应tryAcquire获取锁和tryRelease释放锁。 共享式(Share): 同一时间允许多个线程同时访问共享资源,也就是共享锁。 CountDownLatch,Semaphore,ReentrantReadWriteLock的ReadLock都是共享锁。 对于共享式锁的实现,在AQS中对应tryAcquireShare获取锁和tryReleaseShare释放锁。 lock,tryLock和lockInterruptibly区别 PS: AQS中的锁计数指的是 state 变量。 lock: 如果线程获取到了锁或线程已经拥有了锁就更改锁计数， 否则线程就加入阻塞队列并一直CAS自旋获取。 tryLock: 线程尝试获取锁，如果线程获取到了锁或线程已经拥有了锁就更改锁计数，否则返回false。 lockInterruptibly: 如果线程在获取锁之前被设置了中断状态，那么当线程获取锁时就会响应中断状态， 抛出InterruptedException异常。如果获取不到就加入阻塞队列并自旋获取，并且阻塞自旋期间还会响应中断， 也就是说在阻塞自旋期间可能抛出InterruptedException异常。 所以lockInterruptibly优先响应中断，而不是优先获取锁。 如果线程获取到了锁或线程已经拥有了锁才更改锁计数。 CountDownLatch CountDownLatch允许count个线程阻塞在一个地方，直至所有线程的任务都执行完毕。 CountDownLatch是共享锁的一种实现,它默认构造AQS的state为count。 当线程使用countDown方法时,其实使用了tryReleaseShared方法以CAS的操作来减少state, 直至state为0就代表所有的线程都调用了countDown方法。 假如某线程A调用await方法时，如果state不为0，就代表还有线程未执行countDown方法， 那么就把线程A放入阻塞队列Park，并自旋CAS判断state == 0。 直至最后一个线程调用了countDown，使得state == 0， 于是阻塞的线程判断成功，并被唤醒，就继续往下执行。 Semaphore Semaphore允许一次性最多(不是同时)permits个线程执行任务。 Semaphore与CountDownLatch一样，也是共享锁的一种实现。 它默认构造AQS的state为permits。 当执行任务的线程数量超出permits,那么多余的线程将会被放入阻塞队列Park,并自旋判断state是否大于0。 只有当state大于0的时候，阻塞的线程才有机会继续执行,此时先前执行任务的线程继续执行release方法， release方法使得state的变量会加1，那么自旋的线程便会判断成功。 如此，每次只有不超过permits个的线程能自旋成功，便限制了执行任务线程的数量。 所以这也是我为什么说它可能不是permits个线程同时执行， 因为只要state>0,线程就有机会执行. CycliBarrier CycliBarrier的功能与CountDownLatch相似，但是CountDownLatch的实现是基于AQS的， 而CycliBarrier是基于ReentrantLock(ReentrantLock也属于AQS同步器)和Condition的。 CountDownLatch虽然可以令多个线程阻塞在同一代码处，但只能await一次就不能使用了。 而CycliBarrier有Generation代的概念，一个代，就代表CycliBarrier的一个循环， 这也是CycliBarrier支持重复await的原因。 ReentrantReadWriteLock如何区分读写锁的? Sync既有写锁，又有读锁，因此一个state不够用， 所以使用state的高16位表示读锁，低16位表示写锁. ReentrantReadWriteLock部分源码: static final int SHARED_SHIFT = 16; static final int SHARED_UNIT = (1 >> SHARED_SHIFT; } /** Returns the number of exclusive holds represented in count. */ static int exclusiveCount(int c) { return c & EXCLUSIVE_MASK; } 剩下的就读源码吧。 其实吧，在我读了几遍源码后,才发现jdk的源码不算特别难阅读。 但是像我在读SpringBoot的源码时，我就只能分析个大概。 主要是Jdk的源码之间并没有什么耦合性，你看一个jdk的类，不像Spring的源码那样绕来绕去， 各种设计模式搞得你头晕。 所以我建议阅读源码可以从jdk的源码开始，前提是你需要一定的基础才能看得懂。 比如我这个版本(11)就发现AQS的部分源码与之前版本的源码不同。 这个版本的AQS使用了: VarHandle 这个类来设置Node类内部的属性， 而之前都是直接使用构造方法来构造Node的。 并且AQS使用的是LockSupport来阻塞线程的，LockSupport仍然使用的是Unsafe类来进行操作的, 这些都属于java与c/c++交互的类,所以你如果没有基础，会诧异,jdk还有这种东西呀 ^-^... "},"gitbook_doc/jdk-jvm-juc/对象在内存中的布局.html":{"url":"gitbook_doc/jdk-jvm-juc/对象在内存中的布局.html","title":"对象在内存中的布局","keywords":"","body":" 对象在内存中的布局(64位) 对象头 实例数据 对齐填充 markword和metadata jol工具查看对象布局 查看对象内存布局 hashcode 对象的hashcode返回的是对象的内存地址吗? 对象在内存中的布局(64位) 鉴于此章已经涉及到jvm的内容了，所以各位同学在此章需要对JVM的一些基本术语有所了解。 这里我找到了openjdk官方对于hotspot的一些基本词汇表，各位同学在学习的时候，可以参考这个词汇表: OpendJdk Hotspot Glossary 对象在内存中的布局,在32位和64位操作系统上的实现也是不同的，以我的机器为例(64位) 对象在内存中由 对象头,实例数据,对齐填充三部分组成。 其中实例数据和对齐填充是不固定的。 实例数据 实例数据存储着对象在程序中被定义的各个字段的数据,也就是对象的字段的 数据。如果一个类没有实例字段，也就不存在实例数据，所以这是它不固定的原因。 对齐填充 Java对象的小必须是8字节的倍数,像13,15这种非8的倍数的对象的大小, 不足或多余的部分就要使用对齐填充数据补齐。 如果Java对象大小正好是8的倍数,那么就无需对齐填充数据。 对象头 关于对象头，在hotspot中，opendjdk是这样描述的: 大意是说: 对象头是jvm在每次GC时管理的对象的通用结构，包含了对象的布局，类型(Class Type),GC状态， 同步状态和hashcode等信息，在数组中，还会跟随数组的长度。 在hotspot虚拟机中的对象头由2部分组成: mark 和 metadata(包括klass , compressed_klass)(*如果是数组,对象头还会保存数组长度)(见oop.hpp文件) mark/markword就是说面说过的，保存了对象的GC状态，同步状态和hashcode等信息。 下面是mark/markword的组成(见:markOop.hpp头文件): 对象处于不同的同步状态和GC状态，markword都不同(见:markOop.hpp头文件): markword和metadata Mark Word(mark)组成: 锁状态 锁标志 markword组成 无锁 01 由hashcode,分代年龄,是否偏向锁(1位),锁标志位组成 偏向锁 01 由偏向线程的ID,偏向时间戳(epoch),是否偏向锁(1位),分代年龄,锁标志位组成 轻量级锁 00 由指向栈中锁的记录和锁标志位组成 膨胀锁 10 由指向锁的指针和锁标志位组成 GC 11 无数据 Klass Pointer / Compressed Klass: Klass Pointer是指向对象类型的指针，指针指向对象的类元数据。 jvm通过klass pointer判断对象属于哪个类。 在64位的jvm实现中，Klass Pointer的长度为64bit(32位系统, 指针为32bit)，也就意味着,64位系统比32位的系统占用更多内存。 所以jvm提供了压缩指针(Compressed Klass)来节省空间，在64位系统下，压缩指针是默认开启的， 可以使用-XX:-UseCompressedOops来关闭指针压缩。 jol工具查看对象布局 org.openjdk.jol jol-core 0.10 相信各位同学可能还是对上面的概念有点模糊，那就可以使用jol工具来查看一下 对象的真实布局，在实践之前，请各位同学带着几个问题看下面的内容: hashcode真实存在吗? hashCode方法返回的真是对象内存地址吗? 查看对象内存布局 以下是我自己的一个测试demo，详解了jol的使用: 以上可以看到jol工具很直观的给我们展现了对象的内存布局， 但是在对象的markword之中，我们并没有看到hashcode的值， 难道对象不存在hashcode吗？ hashcode 上一个测试在打印对象内存布局之前，我并没有调用对象的hashcode方法， 相信各位同学也注意到了，我把那2行代码注释掉了。 打开那2行注释再运行看看: 我们发现，在调用hashcode方法后，对象的hashcode的值与打印结果完全一致， 到这里可以初步猜想(没有实际验证): hashcode的值也是不固定存在的。 在没有调用对象的hashcode方法之前，对象不存在hashcode。 当调用完对象的hashcode之后，jvm就把生成的hashcode值赋予了对象的markword之中。 对象的hashcode返回的是对象的内存地址吗? 在hotspot中，hashcode返回的不完全是地址 (见：hotspot的/src/share/vm/runtime/synchronizer.cpp): 可以看到hashcode有:随机数，自增长序列，关联地址等多种生成策略。 "},"gitbook_doc/jdk-jvm-juc/JVM.html":{"url":"gitbook_doc/jdk-jvm-juc/JVM.html","title":"JVM","keywords":"","body":" JVM JVM运行时内存分区 程序计数器 程序计数器的特点 Java虚拟机栈 栈帧 局部变量表 操作数栈 动态连接 方法出口 本地方法栈 堆 方法区 JavaVirtualMachineError StackOverflowError OutOfMemoryError JVM PS:JVM部分参考了《深入理解Java虚拟机 - 第二版》(周志明). 个人认为《深入理解Java虚拟机 - 第二版》上的部分内容已经过时 有些知识请各位同学明鉴，此外我后续会根据 《深入理解Java虚拟机 - 第三版》的内容来做更新和修改。 JVM运行时内存分区 以HotSpot为例: JDK8之前: 线程私有的部分有:程序计数器(PC寄存器),JAVA虚拟机栈,本地方法栈(native)。 线程共享部分有: GC堆,永久代(是方法区的一种实现)。 JDK8之后: 线程私有的部分不变, 线程共享部分的永久代改为了元空间(MetaSpace) (永久代和元空间都是方法区的实现),字符串常量池也移动到了heap空间 程序计数器 程序计数器是一块较小的内存空间，它的作用是作为当前线程执行的字节码的行号计数器。 当字节码解释器工作时，通过改变行号计数器的值来选取下一条要执行的字节码指令。 分支，循环，跳转，异常处理，线程恢复等功能都需要依赖程序计数器完成。 程序计数器是属于线程私有的部分。 当cpu在多个线程之间切换执行时，需要记录下当前线程执行的字节码的位置， 以便下次切换回当前线程时，能够继续执行字节码指令， 所以每个线程都需要有自己的程序计数器。 程序计数器的特点 如果当前线程执行的是java方法，那么程序计数器记录的是字节码指令的地址。 如果当前线程执行的native方法，那么程序计数器记录的值为空(undefined)。 程序计数器这部分内存区域是JVM中唯一不会出现OOM错误的区域 程序计数器的生命周期与线程相同,即程序计数器随着线程创建而创建， 随着线程的销毁而销毁。 使用 javap -c 反编译class文件后的代码如下, 红框里的就是字节码的偏移地址: Java虚拟机栈 Java虚拟机栈与程序计数器一样，都是线程私有的部分，生命周期也跟线程一样。 Java虚拟机栈描述的是Java方法运行时的内存模型，它由一个一个的栈帧组成。 栈帧 栈帧是用于支持Java方法运行时的数据结构。 栈帧包含了局部变量表，操作数栈，动态连接，方法出口等信息。 每个方法执行时，都会在java虚拟机栈中创建一个栈帧。 对方法的调用和返回，就对应着栈帧的入栈和出栈的过程。 Java虚拟机栈: 局部变量表 局部变量表用于存储方法参数和方法内定义的局部变量。 局部变量表存放了各种已知的数据类型的变量。 一个局部变量的类型可以是基本数据类型 (int,short,float,double,boolean,long,byte,char)或引用类型(reference)。 在Java代码被编译成class字节码后，方法Code属性的locals就确定了方法的局部变量表的大小。 局部变量表以slot为最小单位，一个slot代表4个字节，也就是32位长度的大小。 操作数栈 操作数栈是一个后进先出(LIFO)的数据结构。 它存储的是方法在进行数据运算时的元素。 和局部变量表一样，操作数栈的每个元素的类型也可以是基本数据类型和引用类型。 操作数栈的深度不会超过 Code属性的stack值。 使用javap -c 反编译class文件后可以得到的字节码指令如下: 动态连接 了解动态连接首先需要了解符号引用和直接引用 符号引用: 符号引用存于Class文件常量池。分为类的全限定名，方法名和描述符，字段名和描述符。 直接引用: 指向目标的指针，可以简单理解为目标的内存地址(如指向类的字段的内存地址)。 Class文件常量池如下(javap -c 反编译class文件后的字节码): 在虚拟机栈中，每个栈帧都包含了一个该栈帧所属方法的符号引用， 持有这个符号引用的目的是为了支持方法调用过程中的动态连接。 这些符号引用有的一部分会在JVM类解析阶段就会转为直接引用，这部分转换成为静态解析。 还有一部分会在运行时转为直接引用，这部分称为动态连接。 方法出口 当方法执行时，有2种方式可以退出该方法。 正常退出: 当方法执行时，执行到return指令，该方法就会正常退出。 一般来说，方法正常退出时，调用线程的程序计数器的值可以作为方法返回的地址， 栈帧中可能会保存这个计数器的值。 异常退出: 在方法执行过程中遇到了异常，并且方法内部没有处理这个异常，就会导致方法退出。 方法异常退出时，返回地址需要通过异常处理器表来确定的，栈帧中不会保存这部分值。 无论何种退出方式，在方法退出后，都需要回到方法被调用的位置，程序才能继续执行。 本地方法栈 本地方法栈与虚拟机栈的作用是相似的， 不过虚拟机栈是为执行Java方法提供服务的， 本地方法栈视为执行native方法提供服务的。 在本地方法执行的时候，也会在本地方法栈中创建栈帧， 用于存放该本地方法的局部变量表，操作数栈，动态连接和方法返回地址等信息。 堆 堆是JVM中内存占用最大的一块区域，它是所有线程共享的一块区域。 堆的作用是为对象分配内存并存储和回收它们。 堆是垃圾回收的主要区域，所以堆区也被成为GC堆。 堆区可以划分为 新生代(Young Generation),老年代(Old Generation) 和 永久代(Permanent Generation),但永久代已被元空间代替, 元空间存储的是类的元信息，几乎不可能发生GC。 新生代再细分可以分为: Eden空间，From Survivor空间和To Survivor空间。 缺省状态下新生代占堆区的 1/3,老年代占堆区的2/3， eden空间占新生代的80%,2个Survivor空间栈新生代的20%, FromSurvivor和ToSurvivor的空间占比为1:1。 (通过-XX:NewRatio参数可以调整新生代和老年代的空间占比) (通过-XX:SurvivorRatio参数可以调整eden和survivor的空间占比) 发生在新生代的GC叫做Young GC或Minor GC, 发生在老年代的GC叫做Old GC或Major GC 堆: PS: FromSurvivor和ToSurvivor这两块内存空间并不是固定的， 在进行GC的时候，这两块内存会轮流替换使用。这部分内容参考GC部分。 PS: 有的文章说 Full GC与Major GC一样是属于对老年代的GC， 也有的文章说 Full GC 是对整个堆区的GC，所以这点需要各位同学自行分辨Full GC语义。 见: 知乎讨论 方法区 方法区在JVM规范里也是各个线程共享的一部分区域， 它用于存储已被jvm加载的类的元信息，运行时常量池等数据。 HotSpot虚拟机对于方法区的实现在jdk8之前为永久代，在jdk8之后， HotSpot移除了永久代，新增了元空间。 元空间使用的是本地内存，所以元空间仅受本地物理内存的限制。 元空间存储着已被加载的类的方法描述，字段描述，运行时常量池等信息。 字符串常量池在jdk7已经从永久代转移到了堆内存之中。 无论是永久代还是元空间，都有可能发生OOM。 JavaVirtualMachineError StackOverflowError 当前线程执行或请求的栈的大小超过了Java 虚拟机栈的最大空间(比如递归嵌套调用太深),就可能出现StackOverflowError错误 OutOfMemoryError 发生OOM的情况: java heap space 当需要为对象分配内存时，堆空间占用已经达到最大值， 无法继续为对象分配内存，可能会出现OOM: java heap space错误。 Requested array size exceeds VM limit 当为数组分配内存时，数组需要的容量超过了虚拟机的限制范围， 就会抛出OOM: Requested array size exceeds VM limit。根据我的测试，Integer.MAX_VALUE - 2 是虚拟机能为数组分配的最大容量 GC overhead limit exceed 垃圾回收器花费了很长时间GC,但是GC回收的内存非常少, 就可能抛出OOM:GC overhead limit exceed 错误。 但是这点在我的机器上测试不出来,可能与jdk版本或gc收集器或Xmx分配内存的大小有关, 一直抛出的是java heap space Direct buffer memory 当程序分配了超额的本地物理内存(native memory/ direct buffer)， minor gc(young gc)并不会回收这部分内存， 只有 full gc才会回收直接内存，如果不发生full gc， 但直接内存却被使用完了，那么可能会发生 OOM: Direct buffer memory。 unable to create new native thread 操作系统的线程资源是有限的， 如果程序创建的线程资源太多(无需超过平台限制的线程资源上限)， 就可能发生 OOM: unable to create new native thread 错误。 Metaspace 当加载到元空间中的类的信息太多，就有可能导致 OOM : Metaspace。 使用cglib的库，可以动态生成class，所以可以使用cglib测试此错误。 "},"gitbook_doc/jdk-jvm-juc/简单了解类文件结构.html":{"url":"gitbook_doc/jdk-jvm-juc/简单了解类文件结构.html","title":"简单了解类文件结构","keywords":"","body":"简单了解类文件结构 Class文件结构如下: 使用vim -b filename 以二进制模式编辑class文件， 然后输入 :%!xxd 即可查看十六进制的Class文件,如下: 当然，最直观的方法是对 class 文件使用 javap -c命令进行详细查看。 "},"gitbook_doc/jdk-jvm-juc/类的生命周期.html":{"url":"gitbook_doc/jdk-jvm-juc/类的生命周期.html","title":"类的生命周期(类加载)","keywords":"","body":" 类的生命周期 类加载过程 加载 连接 初始化 使用 类的卸载 Java中类加载器有多少个 类加载器的命名空间 双亲委派机制 为什么需要双亲委派机制? 双亲委派机制的实现原理? 类的生命周期 当java源代码文件被javac编译成class文件后，并不能直接运行， 而是需要经过加载，连接和初始化这几个阶段后才能使用。 在使用完类或JVM被销毁后，JVM会将类卸载掉。 类加载过程 类加载过程需要经过3个阶段: 加载 连接 初始化 其中连接又可分为3个阶段: 验证 ， 准备 ， 解析。 加载 在加载阶段，类加载器将类的class文件的二进制数据读取到内存， 并保存到方法区，并在堆区生成该类的Class对象。 通常有多种方式可以获取类的二进制数据: 通过javac编译器编译java源文件，读取在本地磁盘上生成的class文件。 从Jar，ZIP等归档文件中读取class文件。 通过网络读取类的字节流。 通过动态生成字节码的技术(如使用动态代理，cglib)来生成class。 PS:数组由数组元素的类型的类加载器在java程序运行时加载，这是ClassLoader类的部分注释: 见: 测试 连接 1.验证 验证阶段是为了确保类的字节流符合虚拟机规范，并且不会对虚拟机造成恶意损害。 JVM会对字节流进行如下验证: 文件格式验证:会验证class文件是否符合虚拟机规范，如是否以0×CAFEBABE开头， 主次版本号是否在虚拟机规定范围类，常量池中的类型是否有JVM不支持的类型。 元数据验证: 会对类的元信息进行语义分析，确保符合Java语法规范。 字节码验证: 通过分析数据流和控制流，确保类的方法体的程序语义是合法的， 符合逻辑的。 符号引用验证: 确保常量池中的符号引用能在解析阶段正常解析。 2.准备: 准备阶段会为类的静态变量初始化零值，如(0,0L,null,false). 3.解析: 解析阶段会将常量池中的符号引用转为直接引用。 符号引用包括类的全限定名，方法名和描述符，字段名和描述符。直接引用是指向目标的指针，可以简单理解为目标的内存地址。 初始化 初始化阶段是类加载过程的最后一个阶段。 在这个阶段,只有主动使用类才会初始化类，总共有8种情况会涉及到主动使用类。 当jvm执行new指令时会初始化类没，即当程序创建一个类的实例对象。 当jvm执行getstatic指令时会初始化类，即程序访问类的静态变量(不是静态常量，常量归属于运行时常量池)。 当jvm执行putstatic指令时会初始化类，即程序给类的静态变量赋值。 当jvm执行invokestatic指令时会初始化类，即程序调用类的静态方法。 当使用反射主动访问这个类时,也会初始化类,如Class.forname(\"...\"),newInstance()等等。 当初始化一个子类的时候，会先初始化这个子类的所有父类，然后才会初始化这个子类。 当一个类是启动类时，即这个类拥有main方法，那么jvm会首先初始化这个类。 MethodHandle和VarHandle可以看作是轻量级的反射调用机制，而要想使用这2个调用， 就必须先使用findStatic/findStaticVarHandle来初始化要调用的类。 PS:见:测试 使用 在类被初始化完成后，就可以使用类了。 类的卸载 类被卸载(Class对象被GC掉)需要满足3个条件: 该类的实例对象都已被GC，也就是说堆中不存在该类的实例对象。 该类没有在其它任何地方被使用。 加载该类的类加载器实例已被GC。 在JVM的生命周期中，被JVM自带的类加载器所加载的类是不会被卸载的。 而被我们自定义的类加载器所加载的类是可能会被卸载的。 其实只要想通一点就好了，jdk自带的BootstrapClassLoader， PlatformClassLoader和AppClassLoader负责加载jdk提供的类， 它们(类加载器)的实例肯定不会被回收，其中BootstrapClassLoader在java中更是不能被获取到。 而我们自定义的类加载器的实例是可以被GC掉的， 所以被我们自定义类加载器加载的类是可以被GC掉的。 PS:使用-XX:+TraceClassUnloading 或 -Xlog:class+unload=info可以打印类卸载的信息。 Java中类加载器有多少个 BootstrapClassLoader(用于加载Java基础核心类库。由c/c++编写，Java获取不到)。 PlatformClassLoader (jdk9之后才有此类加载器，jdk8之前是扩展加载器ExtensionClassLoader 。PlatformClassLoader加载平台相关的模块，ExtensionClassLoader加载jdk扩展的模块)。 AppClassLoader。(应用程序类加载器，负责加载我们程序的classpath下的jar和类)。 自定义类加载器。 类加载器的命名空间 每个类加载器实例都有自己的命名空间，命名空间由该加载器及其所有父加载器加载的所有的类组成。 在同一个命名空间中(一个类加载器实例)，不会出现全限定名(包括包名)相同的2个类(不会加载2个相同名称的类)。 在不同的命名空间中(多个类加载器实例)，可能会出现全限定名(包括包名)相同的2个类(可能加载2个相同名称的类)。 PS:见:测试 双亲委派机制 为什么需要双亲委派机制? 双亲委派机制是为了防止类被重复加载，避免核心API遭到恶意破坏。 如Object类，它由BootstrapClassLoader在JVM启动时加载。 如果没有双亲委派机制，那么Object类就可以被重写，其带来的后果将无法想象。 双亲委派机制的实现原理? 每个类都有其对应的类加载器。 双亲委派机制是指在加载一个类的时候，JVM会判断这个类是否已经被其类加载器加载过了。 如果已经加载过了，那么直接返回这个类。 如果没有加载，就使用这个类对应的加载器的父类加载器判断， 一层一层的往上判断，最终会由BootstrapClassLoader判断。 如果BootstrapClassLoader判断都没有加载这个类, 那么就由BootstrapClassLoader尝试加载。 如果BootstrapClassLoader加载失败了， 就由BootstrapClassLoader的子类加载器们加载。 在jdk9之后，由于模块化的到来，双亲委派机制也变化了一点: 如果类没有被加载，那么会根据类名找到这个类的模块。 如果找到了这个类的模块， 就由这个类的模块加载，否则仍然使用父类加载器加载。 可以看出:在加载一个类时，是由下自上判断类是否被加载的。如果类没有被加载， 就由上自下尝试加载类。 "},"gitbook_doc/jdk-jvm-juc/JVM常量池.html":{"url":"gitbook_doc/jdk-jvm-juc/JVM常量池.html","title":"JVM常量池","keywords":"","body":" JVM常量池 Class常量池(静态常量池) 运行时常量池 字符串常量池(全局常量池) 包装类型缓存池 JVM常量池 Jvm常量池分为: Class常量池(静态常量池) 运行时常量池 字符串常量池(全局常量池) 包装类型缓存池 Class常量池(静态常量池) 当Java源文件被编译后，就会生成Class字节码文件。 Class常量池就存在于Class文件中(Class文件的Constant Pool中)。 Class文件常量池主要存放两大常量:字面量和符号引用。 字面量: 字面量分为文本字符串(如: \"abc\",1等)和用final修饰的成员变量(实例变量和静态变量) 符号引用: 符号引用包括三种：类的全限定名，方法名和描述符，字段名和描述符。 运行时常量池 运行是常量池是在类加载阶段，将class二进制数据加载到内存， 并将数据保存到方法区,其中class文件中的常量池将保存到 运行时常量池(数据都在方法区，常量池肯定也在方法区)。 也就是说一个Class文件常量池对应一个运行时常量池。 字符串常量池(全局常量池) 字符串常量池在jdk7之前都是存于永久代(永久代)之中,jdk7以后存于 堆区之中。 包装类型缓存池 包装类缓存池并不是所有的包装类都有，并且缓存池缓存的是一定范围内的数据。 拥有包装类型缓存池的类有:Integer,Byte,Character,Long,Short， 而Float，Double，Boolean都不具有缓存池。 包装类的缓存池缓存的范围基本都为: -128 - 127之间， Character的缓存范围为 0 - 127。 "},"gitbook_doc/jdk-jvm-juc/GC.html":{"url":"gitbook_doc/jdk-jvm-juc/GC.html","title":"GC","keywords":"","body":" GC 判断对象存活的方法 引用计数法缺点 什么是GC Root ? 垃圾回收算法 复制算法(Copying) 标记-清除算法(Mark-Sweep) 标记-整理算法(Mark-Compact) 分代收集算法 内存分配与垃圾回收策略 一次GC的过程 动态年龄阈值 垃圾回收器 Serial串行收集器 Serial Old 串行收集器(老年代版本) Parallel Scavenge 并行多线程收集器 Parallel Old 并行收集器(老年代版本) ParNew 多线程收集器 CMS 并发标记清除收集器 CMS收集器回收过程 G1 收集器 G1回收器的特点 G1收集器回收过程 GC 判断对象存活的方法 在垃圾回收器对堆内存回收前，需要判断对象是否存活。 引用计数算法: 给每个对象添加一个引用计数器,每当对象被引用, 对象的引用计数器就加1,当引用失效时,引用计数器就减1。 直到引用计数器为0,就代表对象不再被引用。 可达性算法: 通过GC ROOT的对象节点往下搜索,节点走过的路径被称为引用链。 如果一个对象不处于任何引用链,那么就可以判断此对象是不可达的。 引用计数法缺点 引用计数的主要缺陷是很难解决循环引用的问题: 也就是当2个对象互相引用的时候,除了彼此, 没有其它对象再引用这2个对象,那么他们的引用计数都为1,就无法被回收。 什么是GC Root ? 上面说通过GC Root对象搜索引用链,那么GC Root对象是什么对象, 或者说什么样的对象是GC Root对象。 可以作为GC Root对象的有: 虚拟机栈和本地方法栈区中的引用对象(stack) 方法区中类的静态属性引用的对象(static) 方法区中的常量引用的对象(final) 垃圾回收算法 常见的垃圾回收算法主要有以下4种: 复制算法 标记-清除算法 标记-整理算法 分代收集算法 复制算法(Copying) 将堆内存分为2块大小相等的内存空间， 每次只使用其中的一块内存，另一块则空闲。 当其中一块内存使用完后， 就将仍然存活的对象复制到另一块空闲内存空间，再清理已使用的内存。 复制算法的优点是不会产生连续的内存碎片，速度也很高效。 但是缺点更明显:每次只使用内存的一半，就代表可使用的内存减少了1/2，代价很高昂。 复制算法一般用于新生代。 因为新生代的GC非常频繁，每次GC的对象较多，存活的对象较少。 所以采用复制算法效率更高，复制时只需要复制少量存活的对象。 标记-清除算法(Mark-Sweep) 标记-清除算法分为2个步骤：标记和清除。 首先标记出所有可达(存活)的对象，在标记完成后， 统一回收所有未被标记(不可达)的对象。 标记-清除算法的缺点主要有2个: 标记和清除2个阶段的耗时都比较长，可以总结为效率较低。 对象在内存中的分布可能是不连续的，分散的，标记-清除后可能造成不连续的内存碎片。 当内存碎片过多后，后续想要分配较大的对象时，无法找到足够大的内存碎片， 可能又需要触发GC。 标记-清除算法一般用于老年代。 因为老年代中的对象存活率较高，几乎很少被回收， 所以标记-清除和标记-整理算法GC的时间不会太长， GC的对象相比新生代更少。 标记-整理算法(Mark-Compact) 标记-整理算法是对标记-清除算法的一种改进。 标记-整理算法与标记-清除算法的在标记阶段是相同的， 都是首先标记出所有可达(存活)的对象。 但标记之后并不直接清理未被标记(不可达)的对象， 而是使被标记(存活)的对象向内存一端移动，然后清理掉这一端外的内存。 标记-整理算法的优点是: 几乎不会如标记-清除算法那样产生不连续的内存碎片。 但，所谓慢工出细活,标记-整理的效率是比标记-清除要低的。 标记-整理算法和标记-清除算法一样，一般用于老年代。 分代收集算法 分代收集算法并不是指某一种具体的垃圾收集算法， 而是将复制，标记-清除，标记-整理等算法合理运用到堆区的不同空间。 比如新生代使用复制算法，老年代使用标记清除或标记整理算法。 现代的几乎所有的JVM都使用分代收集，毕竟每种算法都有优缺点， 结合它们的特点，对不同的环境采用不同的算法是非常明智的选择。 内存分配与垃圾回收策略 对象优先在eden区域被分配 大对象将直接进入老年代 (大对象是指需要大量连续的内存空间的对象，如长字符串，大数组等。) 长期存活的对象将进入老年代 一次GC的过程 对象优先在eden区被分配，当eden区内存不足时， JVM发起Minor GC。Minor GC的范围包括eden和From Survivor: 首先JVM会根据可达性算法标记出所有存活的对象。 如果存活的对象中，有的对象的年龄已经达到晋升阈值 (阈值是动态计算的，可以通过-XX:MaxTenuringThreshold设置最大年龄阈值)， 那么将已经达到阈值的对象复制到老年代中。 如果To Survivor空间不足以存放剩余存活对象， 则直接将存活的对象提前复制到老年代。 如果老年代也没有足够的空间存放存活的对象， 那么将触发Full GC(GC整个堆，包括新生代和老年代)。 如果To Survivor可以存放存活的对象， 那么将对象复制到To Survivor空间，并清理eden和From Survivor。 此时From Survivor为空， 那么From Survivor就成为了下一次的To Survivor， 此时To Survivor存放着存活的对象，就成为了下一次的From Survivor。 这样From Survivor与To Survivor就是不断交替复制的使用。 老年代的空间比新生代的空间要大， 所以老年代的Major GC要比Minor GC耗时更长。 根据垃圾回收器的不同，老年代的GC算法也不同。 动态年龄阈值 JVM并不要求对象年龄一定要达到 MaxTenuringThreshold 才会 晋升到老年代，晋升的年龄阈值是动态计算的。 如果在Survivor中，某个相同年龄阶段的所有对象大小的总和 大于Survivor区域的一半，则大于等于这个年龄的所有对象 可以直接进入老年代，无需等到MaxTenuringThreshold。 垃圾回收器 如果说垃圾回收算法是JVM对GC算法的方法论，那么垃圾回收器就是对GC算法的实现。 垃圾回收器主要分为以下几种收集器: Serial收集器 Parallel Scanvel收集器 ParNew收集器 CMS收集器 G1收集器 Serial串行收集器 Serial收集器为单线程环境设计,并只使用一个线程进行垃圾回收。 在回收时，会暂停用户线程,并不适用于并发环境。 Serial收集器在单线程环境中是很高效的,它没有多线程切换的消耗。 Serial收集器采用复制算法 Serial Old 串行收集器(老年代版本) 它是 Serial收集器的老年代使用的GC收集器，同样是一个单线程的垃圾收集器。 Serial Old收集器采用的是标记-整理算法。 /** 开启串行收集器使用 -XX:+UseSerialGC , * 这样默认新生代使用 Serial 收集器, * 老年代使用 Serial Old 收集器. * * 设置VM参数: * * -XX:+Xlogs:gc* 打印gc信息 * -XX:+PrintCommandLineFlags 打印java版本信息 * -XX:+UseSerialGC 使用串行GC */ //如果程序正常运行,日志会显示 : // 新生代的信息为: def new generation..... // 老年代的信息为: tenured generation..... Parallel Scavenge 并行多线程收集器 Parallel Scavenge是并行收集器，它使用多个垃圾回收线程一起工作, 但是仍然会暂停用户线程。 Parallel Scavenge与其它垃圾回收器不同的是它更关注于达到可控制的吞吐量。 吞吐量是CPU运行用户应用程序代码的时间与CPU总消耗的时间的比值: 吞吐量 = 应用程序代码运行时间 / (应用程序代码运行时间 + GC时间) Parallel Scavenge收集器采用复制算法 Parallel Old 并行收集器(老年代版本) 它是 Parallel Scavenge 的老年代版本,同样是一个并行多线程的收集器。 Parallel Old收集器采用标记-整理算法。 /** * * 设置 Parallel Scavenge 收集器的参数: * * -XX:+UseParallelGC * * ParallelGC老年代默认使用的 Parallel Old GC 回收器 * * 并行收集器打印的新生代的信息为: * PSYoungGen .... * * 老年代的信息为: * ParOldGen .... * */ ParNew 多线程收集器 它可以看做是多线程版的Serial收集器。 除了多线程外，ParNew收集器与Serial收集器几乎没啥区别。 PS:目前只有Serial和ParNew能作为CMS收集器的新生代收集器。 ParNew收集器采用复制算法 /** * * 设置ParNewGC回收器的参数为: * -XX:+UseConcMarkSweepGC * */ CMS 并发标记清除收集器 Concurrent Mark Sweep,并发标记-清除垃圾回收器。 它是一款老年代的收集器，是以达到最短回收停顿时间目标的收集器。 见名知意,CMS收集器使用的是标记-清除算法。 CMS在垃圾回收过程中，用户线程可以同时工作，无需暂停。 因为CMS收集器采用的是标记-清除算法，所以回收时可能会产生不连续的内存碎片。 PS: CMS收集器在jdk14中被删除了。 CMS收集器回收过程 初始标记(Stop The World，此阶段会暂停用户线程): 只标记与GC ROOT直接关联的对象。 并发标记: 对第一个阶段已经标记的对象进行Tracing，标记所有可达的对象。 重新标记(Stop The World,此阶段会暂停用户线程): 在第二个阶段，由于用户程序的运行， 可能有些对象之间的引用关系受到了影响，所以需要对这部分对象进行重新标记调整。 并发清除: 清除所有未被标记的对象。 /** * * 设置 CMS 收集器参数: * -XX:+UseConcMarkSweepGC * * 使用ConcMarkSweepGC收集器后,它的新生代使用的是: * ParNew收集器. * * 当ConcMarkSweepGC收集器出现异常时,会将CMS替换成Serial Old收集器 * * CMS回收分为4个阶段: * * 初始标记: (Stop the world 暂停用户线程) * 标记与GC Root直接可达的对象. * * 并发标记: * 从第一步标记的可达的对象开始,并发的标记所有可达的对象 * * 重新标记: (Stop the world 暂停用户线程) * 在第二部的并发标记阶段,由于程序运行导致对象间引用的关系发生变化, * 就需要重新标记 * * 并发清除: * 这个阶段不暂停用户线程,并且并发的去清除未被标记的对象 * */ G1 收集器 G1收集器可以说是目前最前沿的一款收集器，它是一款面向服务端的收集器。 G1收集器无需配合其他收集器就可以管理整个堆内存。 jdk9开始，G1成为jdk默认使用的垃圾回收器。 G1回收器的特点 并行和并发: G1能够充分利用多核cpu的优势，使垃圾回收与用户线程同时运行。 可预测的停顿: 降低GC停顿时间是CMS与G1收集器的共同目标。但是除了降低GC停顿时间， G1收集器还可以建立可预测的停顿时间模型。(...太np了 =_=) 空间整合: 个人认为这是G1收集器不同于其他收集器的最大亮点了。 在其他收集器中，堆区基本都分为新生代和老年代。 而在G1收集器中虽然仍然保留了新生代和老年代的概念，但已经不再是物理上的分隔了。 在G1收集器的堆内存模型中，内存被分割成了一块一块大小相等的Region， 在这些Region中，Region的类型也不同，有eden，survivor，old，humongous之分。 当有大对象时，对象会被分配到Humongous Region之中。 G1收集器回收过程 G1收集器与CMS收集器的回收过程相似 初始标记(Stop The World,此阶段会暂停用户线程): 只标记与GC ROOT直接关联的对象。 并发标记: 对第一个阶段标记的对象Tracing，标记所有可达的对象。 最终标记(Stop The World,此阶段会暂停用户线程): 在并发标记阶段，由于用户线程执行， 可能导致被标记对象之间的引用关系发生影响，需要对这些对象进行重新标记调整。 筛选回收: 不同于CMS的并发清除，G1收集器首先会对所有Region的回收价值和回收成本进行排序, 然后再进行回收。这样可以在有限的时间内获得最大的回收率。 /** * * 因为我的机器的jdk版本是11,所以无需指定垃圾回收器 * 指定G1回收器的参数是: -XX:+UseG1GC * * 1:初始标记:(Stop the world 暂停用户线程) * 标记所有与GC Root直接可达的对象 * * 2:并发标记 * 从第一个阶段标记的对象开始,trace标记 * * 4:最终标记:(Stop the world 暂停用户线程) * 在第二步并发标记的阶段,由于程序执行, * 导致被标记对象之间的引用关系发生变化,所以需要重新调整标记 * * 5:筛选回收: * 和CMS的并发回收不一样, * G1收集器首先会对所有Region的回收价值和回收成本进行排序, * 然后再进行回收。 * 这样可以保证在有限的时间内获得最大的回收率. * */ "},"gitbook_doc/jdk-jvm-juc/JVM调优相关内容.html":{"url":"gitbook_doc/jdk-jvm-juc/JVM调优相关内容.html","title":"JVM调优相关内容","keywords":"","body":" JVM调优相关 JVM常见参数 堆栈相关 GC相关 其他 Java常用调优命令和工具 JVM调优相关 JVM常见参数 堆栈相关 -Xss 调整线程栈大小。 -Xms 设置堆内存初始化大小。 -Xmx / -XX:MaxHeapSize=? 设置堆内存最大值。 -Xmn / -XX:NewSize=? 设置新生代大小。 -XX:NewRatio=? 设置老年代与新生代的空间占比。 如: -XX:NewRatio=2,那么老年代:新生代=2:1。 -XX:SurvivorRatio=? 设置eden与survivor的空间占比。 如: -XX:SurvivorRatio=2,那么eden:from survivor:to survivor=2:1:1 -XX:MetaspaceSize=? / -XX:PerGenSize=? -XX:MetaspaceSize=9m 设置元空间的初始化大小为9m,此参数只在jdk8以后的版本有效。 -XX:PerGenSize=9m 设置永久代的初始化大小为9m，此参数只在jdk8以前的版本有效。 -XX:MaxMetaspaceSize=? / -XX:MaxPerGenSize=? -XX:MaxMetaspaceSize=50m 设置元空间最大值为50m,此参数只在jdk8以后的版本有效。 -XX:MaxPerGenSize=50m 设置永久代的最大值为50m,此参数只在jdk8以前的版本有效。 -XX:+HeapDumpOnOutOfMemoryError 此参数使程序发生OOM时，dump错误堆栈信息。 -XX:HeapDumpPath=? -XX:HeapDumpPath=/home/log 此参数指定发生OOM时，dump错误堆栈信息存放的日志文件或目录。 此参数只在 -XX:+HeapDumpOnOutOfMemoryError 开启时生效。 GC相关 -XX:+PrintGCDetails / -Xlog:gc* 打印GC的日志信息。 -Xlog:gc* 在我使用的版本(jdk11)是更受推荐的。 -XX:+TraceClassUnloading / -Xlog:class+unload=info 打印类卸载的日志信息。 -Xlog:class+unload=info 在我使用的版本(jdk11)是更受推荐的。 -XX:+UseSerialGC 使用Serial串行回收器。 -XX:+UseParallelGC 使用Parallel并行回收器。 -XX:ParallelGCThreads=? 设置并行收集的线程数,如-XX:ParallelGCThreads=5。 -XX:+UseConcMarkSweepGC 使用CMS收集器，它默认的新生代搜集器为ParNew。 可以与参数: -XX:+UseSerialGC 一起使用，就替换掉了ParNew， 使用Serial作为CMS的新生代收集器。 -XX:+UseG1GC 使用G1收集器。 -XX:MaxTenuringThreshold=? 设置新生代对象晋升到老年代的最大年龄阈值。 其他 -server / -client -server:以服务端模式运行应用程序，server模式适用于服务端应用程序。 JVM在此模式下，会对服务端运行效率做很大优化。 -client:以客户端模式运行应用程序，client模式适用于客户端桌面程序(GUI)。 JVM在此模式下，会对客户端运行做很大优化。 Java常用调优命令和工具 jps(个人认为非常重要) jps 命令类似于 linux的 ps 命令，不过ps命令是用于查看系统进程的， 而jps用于查看当前系统运行的java进程。 jps -q 只输出java进程id jps -l 输出java进程main函数的详细路径 jps -v 输出java进程时指定的jvm参数 jps -m 输出java进程执行时main函数的参数 jstat jstat用于查看java进程的运行状态. jstat -class pid 用于查看java进程类的情况 jstat -compiler pid 用于查看java进程编译的情况 jstat -gc pid 用于查看java进程gc的情况 jinfo jinfo 查看正在运行的java进程的jvm参数 jinfo -flag MetaspaceSize pid 查看java进程的jvm的元空间大小 jinfo -flag MaxHeapSize pid 查看java进程的jvm的最大堆的大小 ... jmap jmap 既可以dump java程序的快照，也可以查看对象的统计信息。 jmap -heap pid 查看java进程堆的信息 jmap -histo pid 查看java进程对象的信息 jmap -dump:file=filename pid 生成java进程jvm的堆快照到指定文件 jstack jstack用于分析java线程栈信息 jstack pid jconsole jconsole 是jdk提供的对java程序进行分析的GUI界面工具。 "},"gitbook_doc/jdk-jvm-juc/Jdk新特性.html":{"url":"gitbook_doc/jdk-jvm-juc/Jdk新特性.html","title":"Jdk新特性","keywords":"","body":" Jdk新特性 Jdk8新特性 Jdk9新特性 jdk10新特性 jdk11新特性 jdk12新特性 jdk13新特性 Jdk14新特性 Jdk新特性 总结的不全，还请各位同学补充。 Jdk8新特性 Lambda / 方法引用 接口新增default方法 Stream API Optional API 新的时间API(java.time强烈推荐使用) 内置Base64工具 Jdk9新特性 PS: jdk9应该是继jdk8之后，又一个重要的版本，后续jdk的迭代，都是基于jdk9来完成的。 模块化: 模块化是jdk9或者说jdk8之后最大的改进，在jdk8及以前，jvm启动时，需要加载非常多的不需要的外部扩展类库，导致程序消耗的内存非常大， 并且在打包后，应用的归档包也是比较庞大。但是在模块化系统中，jvm只需要加载每个模块需要的类库，这就大大减少了jvm的开销。模块化的好处 当然不止我所说的这些，其中奥妙，还请各位同学实践出真知。 集合的工厂方法 try语句升级 接口新增private方法 响应式流(Doug Lea大师编写的，个人认为很重要，也是Webflux的基础) JShell 交互式编程环境 String改为byte数组实现(记住这个特性) PS: String类在jdk9后的每个版本，好像都会新增一些API，这个不再重复。 jdk10新特性 var类型推断: 这一功能在其他语言中早有实现，比如我接触过的c++的auto,js中的var(当然，js并不是强类型语言)。 其实我个人认为此特性意义不是特别重大，因为java本身就是强类型语言，var只能使用于局部变量推断。如果大量使用var, 反而可能造成代码可读性下降。 集合工厂方法，使用集合工厂创建的集合是不可变的集合 移除javah(在编写本地jni库时，需要javah生成c/c++头文件，javah被移除了，说明另有他法来解决这个问题)。 jdk10的特性还是有很多的，但是并没有像模块化这样大的改动。 jdk11新特性 java命令直接可以编译并运行java源文件 HttpClient: 长期以来，java类库之中只有一个HttpUrlConnection可以使用，且HttpUrlConnection使用起来较为麻烦。 Javascript引擎更换: Javascript引擎由Nashorn改为GraalVM。 String类新增了许多好用的API，如: strip,isBlank等。 jdk12新特性 switch语法糖 Unicode11支持 ... jdk13新特性 文本块 Socket API被重新实现 Jdk14新特性 instanceof 模式匹配 Record结构(实用) CMS收集器被删除了。 Parallel Scavenge 和 Serial Old这对组合被弃用了。(我觉着也是，本来Parallel Scavenge和Parallel Old， Serial和Serial Old这两对收集器各自搭配的挺好，Parallel Scavenge非要脚踏两只船) 展望: ZGC是11中引入的一款新的垃圾回收器。G1收集器本身已经很高效了，但是停顿时间这一块缺陷是所有收集器的缺点， 而ZGC不仅对停顿时间这个缺点做了大量优化，也提供了非常多当高级功能。 似乎ZGC的到来，是要主宰Java GC的未来了。。。 关于ZGC可以参考这篇文章: ZGC - 掘金 "},"gitbook_doc/linux-learning/Linux简介.html":{"url":"gitbook_doc/linux-learning/Linux简介.html","title":"Linux简介","keywords":"","body":"Linux Linux博大精深，操作系统更是奥妙无穷，仅仅靠我入门不久学习的知识和这一篇略显苍白的描述， 肯定是无法将Linux的知识讲述清楚的。 因此希望各位同学看本篇文章的时候，一定要抱着批评的态度阅读。 我更希望各位同学已经是对Linux有一定了解了再来阅读本篇文章， 这样你可以更加清楚本篇文章的不当之处。 Linux是什么? Linux是一个自由的类Unix操作系统。 Linux遵循GNU(GPL)许可证，并且Linux的很多系统软件和库都是由GNU项目支持的。 Linux支持多用户，多任务，多线程，多cpu等众多特性。 Linux与Unix的区别 开源与闭源: Linux是开源的；Unix则是对源代码实行知识产权保护的商业软件，这也应该是它们最大的区别。 硬件兼容: Unix系统大多都有与之配套的硬件；Linux则可以运行在多种硬件平台上。 Linux起源 在Unix系统被发明之后，广受追捧,各大科技公司都纷纷研究自己的Unix系统。 后来由于Unix的商业化和闭源，一位叫Richard Stallman的大叔就提出了自由软件精神， 发起了GNU项目，旨在开发出一款开源的类Unix的系统。 于是各个拥有开源精神的大牛们响应号召，为GNU添砖加瓦，开发了各种应用程序,如核心的gcc,glibc。 但最终由于GNU系统缺少系统内核而一直没有完成，此时一位叫Linus的同学就在Unix系统的启发下开发出了Linux内核， 并遵循GPL开源证书，顺理成章的，各位大牛把GNU与Linux一起打包发布为:GNU/Linux。 所以Linux不应该被单纯的认为是Linux内核，而是GNU / Linux。 GNU代表了什么? GNU代表的是一种追求自由的精神，让用户享有对软件的源代码阅读， 修改的权利。软件公司也可以靠提供服务和训练来盈利。 "},"gitbook_doc/linux-learning/操作系统的内核.html":{"url":"gitbook_doc/linux-learning/操作系统的内核.html","title":"操作系统的内核","keywords":"","body":" 操作系统的内核 Linux内核 操作系统的用户态与内核态 为什么要有用户态与内核态? 用户态切换到内核态的几种方式 物理内存RAM(Random Access Memory 随机存储器) 虚拟内存(Virtual Memory) Swap交换空间 操作系统的内核 操作系统的内核是操作系统的核心部分。 它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。 图源: 简书 (如有侵权,请联系俺,俺会立刻删除) Linux内核 我们常说的Linux，其实是指基于Linux内核开发的操作系统。 常见的Linux系统发行版有:Debian,RedHat,Ubuntu,Suse,Centeos等等。 操作系统的用户态与内核态 unix与linux的体系架构：分为用户态与内核态。 用户态与内核态与内核态是操作系统对执行权限进行分级后的不同的运行模式。 为什么要有用户态与内核态? 在cpu的所有指令中，有些指令是非常危险的，如果使用不当，将会造成系统崩溃等后果。 为了避免这种情况发生，cpu将指令划分为特权级(内核态)指令和非特权级(用户态)指令。 对于那些危险的指令只允许内核及其相关模块调用，对于那些不会造成危险的指令，就允许用户应用程序调用。 内核态(核心态,特权态): 内核态是操作系统内核运行的模式。 内核态控制计算机的硬件资源，如硬件设备，文件系统等等，并为上层应用程序提供执行环境。 用户态: 用户态是用户应用程序运行的状态。 应用程序必须依托于内核态运行,因此用户态的态的操作权限比内核态是要低的， 如磁盘，文件等，访问操作都是受限的。 系统调用: 系统调用是操作系统为应用程序提供能够访问到内核态的资源的接口。 用户态切换到内核态的几种方式 系统调用: 系统调用是用户态主动要求切换到内核态的一种方式， 用户应用程序通过操作系统调用内核为上层应用程序开放的接口来执行程序。 异常: 当cpu在执行用户态的应用程序时，发生了某些不可知的异常。 于是当前用户态的应用进程切换到处理此异常的内核的程序中去。 硬件设备的中断: 当硬件设备完成用户请求后，会向cpu发出相应的中断信号， 这时cpu会暂停执行下一条即将要执行的指令，转而去执行与中断信号对应的应用程序， 如果先前执行的指令是用户态下程序的指令，那么这个转换过程也是用户态到内核台的转换。 物理内存RAM(Random Access Memory 随机存储器) 物理内存是计算机的实际内存大小，它直接与CPU交换数据，也被称为主存。 虚拟内存(Virtual Memory) 虚拟内存是操作系统为了更高效率使用物理内存的一种概念，它是对物理内存的抽象。 windows上的虚拟内存和Linux上的swap交换空间都是虚拟内存的一种实现技术。 Swap交换空间 简单理解: 当某个应用程序所需的内存空间不够了， 那么系统会判断当前物理内存是否还有足够的空闲可以分配给应用程序。 如果有，则应用程序直接进入内存运行；如果没有，系统就根据某种算法(如:LRU)挂起一个进程， 将挂起的进程交换到虚拟内存Swap中等待，并将应用程序调入内存执行。 虚拟内存是被虚拟出来的，可以使用硬盘(不仅仅是硬盘)来作为虚拟内存。 这就是为什么当我们运行一个所需内存比我们计算机内存还大的程序时，仍然可以正常运行，并感受不到内存的限制的原因。 "},"gitbook_doc/linux-learning/进程间通信.html":{"url":"gitbook_doc/linux-learning/进程间通信.html","title":"进程间通信","keywords":"","body":" 进程间通信 什么是进程间通信(InterProcess Communication)? 进程间通信的方式 管道/匿名管道 命名管道 信号 信号量 消息队列 共享内存 Socket套接字 进程间通信 什么是进程间通信(InterProcess Communication)? 每个进程都有自己的地址空间，所以一个进程内的数据对其他进程来说是独立的。 要想在进程间交换数据就需要通过内核完成。 在内核中开辟一块缓冲区，一个进程把数据拷贝到内核缓冲区，另一个进程再把数据从缓冲区读取出来。 内核提供的这种进程间交换数据的机制就叫进程间通信。 进程间通信的方式 管道/匿名管道 管道/匿名管道有如下特点: 管道是半双工的，数据只能向一个方向流动。因此当双方互相通信时，需要建立2个通道。 匿名管道由于没有命名，所以只能用于具有亲缘关系的父子进程或兄弟进程之间。 管道的实质是内核缓冲区，它可以被看做一个FIFO先进先出的队列， 管道一端的进程将数据写入队列尾部，另一端进程从头部读取数据。 管道发送的是无数据格式的字节流，这就要求进行管道通信的2端进程需要预先定义好数据格式。 管道通信模型: 命名管道 命民管道提供一个路径名与之关联，并以文件的形式存于文件系统中。 这样即使进程之间不存在亲缘关系，只要进程可以访问到该路径，就可以通过命名管道进行通信。 信号 信号是用于进程间互相通信，通知进程某个事件已经发生的一种机制。 比如在Linux 信号驱动IO模型中，当内核准备好数据后，便发送信号通知用户应用进程数据已准备好。 信号量 信号量是一个计数器，通常作为多进程访问共享数据的同步机制。 消息队列 消息队列有如下特点: 消息队列是消息链表，具有特定的格式，它存放在内核中并由消息队列标识符标识。 消息队列也是队列，也遵循FIFO先进先出的原则。 共享内存 为了进程间的数据交换，内核在内存中分了一块内存，并允许多个进程操作这一块内存空间，它的效率非常高。 但由于多个进程共享一份内存，所以需要依靠某种同步机制来实现进程间的同步与互斥,如:信号量等。 Socket套接字 套接字是一种通信机制，客户端进程与服务端进程可以通过套接字进行双向通信。 即可以使2台机器的进程完成通信。 Socket通信模型: "},"gitbook_doc/linux-learning/Linux文件系统.html":{"url":"gitbook_doc/linux-learning/Linux文件系统.html","title":"Linux文件系统","keywords":"","body":" Linux文件系统 Inode Inode是什么?有什么作用? 文件类型 普通文件 目录文件 链接文件 设备文件 管道文件 Socket套接字文件 Linux目录树 Linux常见目录说明 Linux文件系统 Linux/Unix的设计哲学是一切皆文件，Linux以文件的形式对计算机中的资源和数据进行管理。 反应在Linux上的文件类型就是: 普通文件,目录文件(文件夹),设备文件，链接文件(软链接和硬链接)， 管道文件，Socket套接字文件等等。而这些种类繁多的文件被Linux使用目录树进行管理。 所谓的目录树就是以根目录 / 为主，向下呈现分支状的一种数据结构。 Inode inode是linux/unix文件系统和硬盘存储的基础，如果理解了inode， 将会对我们学习如何将复杂的概念抽象成简单概念有重大帮助。 Inode是什么?有什么作用? 文件存储在硬盘上，硬盘的最小存储单位是扇区(Sector),每个扇区存储512字节(0.5kb)。 操作系统读取硬盘的数据时，不会一个扇区一个扇区的读取，这样做效率较低，而是一次读取多个扇区， 即一次读取一个块(block)。块由多个扇区组成，是文件读取的最小单位，块的最常见的大小是4kb， 约为8个连续的扇区组成。文件数据存储在块中， 但还需要一个空间来存储文件的元信息metadata，如文件拥有者，创建时间，权限，大小等。 这种存储文件元信息的区域就叫inode，译为索引节点。 每个文件都有一个inode，存储文件的元信息。 使用 stat 命令可以查看文件的inode信息。每个inode都有一个号码， Linux/Unix操作系统不使用文件名来区分文件，而是使用inode号码区分不同的文件。 inode也需要消耗硬盘空间，所以在格式化硬盘的时候，操作系统会将硬盘分为2个区域， 一个区域存放文件数据，另一个区域存放inode所包含的信息， 存放inode的区域被称为inode table。 文件的inode信息: 文件类型 普通文件 普通文件是指txt,html,pdf等等的这样应用层面的文件类型， 用户可以根据访问权限对普通文件进行访问，修改和删除。 目录文件 目录也是一种文件，打开目录实际上是打开目录文件。 目录文件包含了它目录下的所有文件名以及指向这些文件的指针。 链接文件 链接文件分为符号链接(软链接)文件和硬链接文件 硬链接(Hard Link):硬链接的文件拥有相同的inode，因为操作系统是靠inode来区分文件的， 2个inode相同的文件，就代表它们是一个文件。 删除一个文件并不会对其他拥有相同inode的文件产生影响，只有当inode相同的所有文件被删除了， 这个文件才会被删除。换言之，你建立一个文件的硬链接，这个文件和硬链接它们的inode是相同的, 无论你删除的是硬链接还是源文件，都不会对彼此造成影响,除非你把硬链接和源文件都删除， 这个文件才被删除。 符号链接(软链接)(Symbolic Link): 符号链接类似于Windows上的快捷方式，它保存了源文件的路径。 当符号链接被删除时，并不会影响源文件。但是当源文件被删除时，符号链接就找不到源文件了。 软链接和硬链接: 设备文件 设备文件分为块设备文件和字符设备文件,设备文件一般存于/dev目录下。 字符设备文件: 字符设备是依照先后顺序被存取数据的设备，通常不支持随机存取， 此类设备可以按字节/字符来读取数据， 如键盘，串口等等。 块设备文件: 块设备是可以被随机存取数据的设备，应用程序可以访问块设备上任何一块位置。 块设备以块的方式读取数据，在windows下也称为簇，块设备不支持字符的方式寻址。 如硬盘，软盘，光碟等等。 字符设备与块设备最根本的区别就是它们是否可以被随机访问。 如键盘，当我们在键盘上敲下一个单词: \"word\"的时候， 那么系统肯定是需要按照顺序来进行读取word的字节流(字符流)的，随机访问在此时是没有意义的。 管道文件 管道文件一般用于进程间通信，使用mkfifo命令可以创建一个管道文件。 Socket套接字文件 套接字文件被用于网络进程之间的通信，既可以使2台不同的机器进行通信，也可以用于本机的Socket网络程序。 Linux目录树 所有可操作的计算机资源都存在于目录树这个结构中，对计算资源的访问，可以看做是对这棵目录树的访问。 linux目录树: Linux常见目录说明 boot: boot目录存放系统启动引导时的文件。 bin: bin目录存放可执行的二进制命令文件，这些命令一般是普通的基本程序，主要用于具体应用，如: ls , ln , less , more , cp , cat等等。 sbin: sbin目录存放的也是可执行的二进制文件，这些目录是系统的基本命令，主要用于系统的基本管理，如: shutdown , reboot等等。 etc: etc目录存放了系统所需的配置文件，如/etc/passwd记录了系统的用户，/etc/group记录了用户组等等，非常重要。 proc: proc是一个虚拟目录，它是当前系统内存的映射，可以通过这个目录获取正在系统当前的一些信息。 opt: opt目录存放可选的应用程序包。 dev: dev目录存放设备文件。 mnt: mnt目录一般是系统用户或管理员临时挂载设备的目录。 media: 当有设备插入计算机后，会自动挂载到media目录。 sys: sys目录存放系统硬件设备的驱动程序的信息。 usr: usr目录存放了系统的所有的共享文件，库文件等等，是最终要的目录之一。 lib: lib目录存放了系统所需的库文件。 lib64: lib64一般是64位系统才会有的目录，存放的也是系统所需的库文件。 tmp: tmp目录存放了系统或程序产生的临时文件。 var: var目录存放了系统运行时需要改变数据的文件。 run: run目录存放了系统启动以来的信息，当系统被重启后，这个目录会被清空。 srv: srv目录存放了某些服务启动后需要提取的文件。 home: home目录是普通用户的目录。 root: root目录是系统超级管理的目录。 lost+found: 这个目录一般是空的，当系统被非法关闭或操作后，这里可能就会生成一些文件。 "},"gitbook_doc/linux-learning/Linux用户权限.html":{"url":"gitbook_doc/linux-learning/Linux用户权限.html","title":"Linux用户权限","keywords":"","body":"Linux用户权限 Linux用户的权限也就是用户对文件的访问权限。Linux下文件的权限一般包括读，写，执行三种，对应字母:r,w,x。 Linux的权限角色有 拥有者，群组，其他三种。每个文件可以根据这三种粒度设置不同的权限。 通常情况下，一个文件只能归属一个用户和用户组。如果一个用户想要拥有对某个文件的权限，可以把这个用户添加到具备权限的用户组，一个用户可以拥有多个用户组。Linux系统可以使用chmod命令进行权限操作,使用chown可以更改文件拥有者。 chmod权限模式 chmod的权限模式为: [ugoa...][+-=][rwx...][files] u代表文件拥有者，g代表与该文件拥有者属于同一个用户组的用户，o代表其他人，a表示ugo的缩写。 +代表给指定的角色增加权限 , -代表取消指定的角色的权限 , =代表给指定的角色赋予指定的权限 rwx代表权限 ， 可以使用数字标识 r = 4 , w = 2 , x = 1， 所以rwx=7 , rw = 6 , rx=5 , wx=3。 使用chmod更改文件权限 我们先创建一个名叫a.txt的文件,它刚被创建后的权限如下: 当使用chmod更改a.txt的权限后如下: "},"gitbook_doc/linux-learning/Linux运行级别.html":{"url":"gitbook_doc/linux-learning/Linux运行级别.html","title":"Linux运行级别","keywords":"","body":"Linux运行级别 Linux总共有0-6这7种运行级别，使用runlevel可以查看当前系统运行级别。 0: 代表系统处于关机状态。 1: 代表系统处于单用户无网络连接状态，此时只允许超级管理员进行系统操作，不允许非root用户进行登录。 2: 代表系统处于多用户无网络连接状态。 3: 代表系统处于多用户正常状态。 4: 代表系统处于用户自定义状态。 5: 代表系统处于多用户GUI图形化界面。 6: 代表系统处于重启状态。 "},"gitbook_doc/linux-learning/Shell.html":{"url":"gitbook_doc/linux-learning/Shell.html","title":"Shell","keywords":"","body":"shell 什么是shell? shell是一个命令解释器，它是用户使用linux的桥梁，它将用户输入的命令解释后传递给linux内核。同时shell也是一种解释型的程序设计语言(shell脚本)，可以直接被执行，这个是它非常强大的一个特性，我们可以使用shell来编写一些小程序替我们完成那些重复的工作。 shell种类 一般在linux系统下的shell默认为bash, 当然还有其他的 c shell(csh), k shell(ksh)等等，在windows系统下有cmd，还有powershell这种跨平台的shell，总之shell的种类是非常多的。我当前的系统是ubuntu18.xx, 默认的shell是bash。 第一个shell脚本 我们先创建一个名叫 test.sh 的文件，文件并不一定要以.sh结尾，只是便于判断他是一个脚本文件。 这是 test.sh 脚本的内容: 第一行的 #!/bin/bash 是作为一个通用的标记,他告诉系统使用什么命令解释器执行脚本，即使用什么shell执行脚本， 此处的 /bin/bash 代表的就是 bash。 第二行的 echo \"hello shell !\" > hello.txt 的作用是输出 \"hello shell !\" 这段文本到 hello.txt 文件中去，hello.txt如果不存在，则会自动被创建。 在文件被创建后并不能立即执行，因为它对拥有者的权限默认为 rw, 所以我们需要把文件的权限更改为可执行， 或者增加可执行权限，这样文件才能被执行: 现在就可以执行shell脚本了: 可以看到在目录下多了一个 hello.txt文件，它的内容正是: \"hello shell !\" 。 此外关于shell的其他知识，建议学习: 菜鸟教程 "},"gitbook_doc/linux-learning/Linux命令大全.html":{"url":"gitbook_doc/linux-learning/Linux命令大全.html","title":"Linux命令大全","keywords":"","body":"Linux命令大全 Linux命令有很多，数不胜数，我们也没有必要完全背下来。但是当我们需要用到一个命令的时候，不能望而生畏， 这里我也找到了一份Linux命令大全，也是菜鸟教程的: Linux命令大全 ，希望各位同学能够好好练习。 "},"gitbook_doc/linux-learning/完全使用GNU_Linux学习.html":{"url":"gitbook_doc/linux-learning/完全使用GNU_Linux学习.html","title":"完全使用GNU/Linux学习","keywords":"","body":" 完全使用GNU/Linux学习 为什么要写这篇文章? 为什么我要从Windows切换到Linux? Linux作为日常使用的缺点 硬件驱动问题 软件问题 你真的需要完全使用Linux吗? 结尾 我使用Debian/Ubuntu时遇到的问题 IDEA编辑Markdown预渲染问题 wifi适配器找不到 XMind安装 Fcitx候选框的定位问题 完全使用GNU/Linux学习 喔，看到这个标题千万不要以为我要写和王垠前辈一样的内容啊，嘿嘿。不过在这里还是献上王垠前辈的那篇文章的链接吧:完全用Linux工作。 为什么要写这篇文章? 首先介绍本篇文章产出的时间，现在是2020/04/06。在三，四天之前，我其实并没有写这篇文章的打算，但是这三，四天以来，我一直在忙活从Ubuntu18换到Debian10 Buster的事情，没有时间写代码，手确实有些痒了。你可能想象不到，我这个之前一直使用Ubuntu的人，只是切换到Debian就花这么长时间，你可能以为我是在劝退各位同学，其实不是的，我只是想表达：我对Linux并不熟悉，这其中一部分原因是我使用的是对用户较为友好的发行版Ubuntu，另一部分原因是我仍然没有那么大的动力去学习Linux，即使它一直作为我的日常使用。 这篇文章并不是吹嘘或贬低Windows和Linux系统，而是想记录一下我一直以来使用Linux作为日常学习的心得，以及这几天再度折腾Debian以来的感触。 为什么我要从Windows切换到Linux? Windows是商业软件，这使它具备易用的性质。Linux是自由软件，这使得它拥有开源的性质。 易用软件通常带来的是对用户的友好度，以致于Windows发展至今，被许许多多的普通用户所采用。自由软件通常带来的是其社区的发展，所以你现在可以在网上看到许多如 ask ubuntu 这样的论坛。 我非常赞同《完全用Linux工作》中的一个观点: UNIX 不是计算机专家的专利。 我对这句话的理解就是：即使你学习或工作的方向不是计算机，但你仍然可以去学习Unix/Linux，如果你是计算机方向的同学，那么，你就更应该去学习Unix/Linux了。 但这只是我从Win切换到Linux的一部分原因，另一个很重要的原因是我受够了Windows的 “易用性”。这里的易用性并不是说我排斥Windows的人性化，而是因为人性化给我带来了很多学习上的困难。举个很简单的栗子：你在学习一项技术的时候，无论是否有面试造火箭的需要，你是否都会好奇想了解其原理和实现，即使你可能知道它很复杂。 为什么你会好奇一个事物的源头？ 我个人认为的答案是：有趣的事情就在眼前，为什么不去了解它呢？ 而Windows只是有趣，但它并不在“眼前”。 我个人的体验哈，不知道有没有同学和我一样的经历，在很多时候，你的Windows可能会出现一些莫名奇妙的问题，但你却不知道如何解决它，你只能求助搜索引擎，当你解决完问题后，你不会想要去了解为什么会发生这种问题，因为Windows太庞大了。 就比如: 我现在安装了Git，使用起来没有任何问题。但等到过一段时间后，Git莫名奇妙的不能使用了，明明你啥都没干。更甚之，有一些流氓问题或流氓软件不能被解决和被屏蔽。 问题出现了，总得要解决吧，所以此时你开始在互联网上查询相关问题的解决方法，如果你的运气好，那么有人可能遇到过和你出现相同的问题，你也因此可能会得到答案。不过一般的答案只是教你怎么解决的，如打开注册表，添加或删除某个key，你不会想要知道为什么做，因为对于初学者来说，当你看到注册表那么多的内容时，你只想着不出错就行了，哪还有心思去学习这玩意啊。如果你的运气不好，且并没有更换系统的打算，那么你可能会将就着使用，但此时，你的心里可能已经衍生了对Windows的厌烦情绪。 我对流氓软件的定义是：当你想让一个软件如你的想法停止运行或停止弹出广告的时候，这个软件不能或不能做的很好的达到你的要求时，这就是一个流氓软件。你也许会说，每个人都有不同的要求，软件怎么可能达到每个人的标准呢？但我指的是停止和停止弹出广告等这样最基本的诉求，如果一个软件连最基本的诉求都实现不了，又何必再使用它呢？ 综上所述，我从Window切换到Linux的最主要的原因有：学习和自由。 是的，你不得不承认Linux是你学习计算机的非常好的环境，与C/C++天然的集成，比你在Windows上冷冰冰的安装一个IDE就开始敲起代码来，显得多了那么一点感觉。 还有一点，可能有的同学和我一样，刚接触Linux的时候，是在Windows上安装一个虚拟机环境或使用Docker来进行学习。不可否认，这确实是在Windows上学习Linux的主要途径了，但是你有没有感觉到，你在采取这种方式学习的时候，对Linux始终有种陌生感，似乎我只是在为了学习而学习。 产生这种想法的主要原因就是你没有融入到Linux环境之中，当你融入到Linux环境之中时，你不再只是需要学习那些操作命令，你会不可避免的遇到某个你从来没有接触过的问题，这个问题不是你在Windows上“丢失图标”的那种烦人问题，而可能是令你有些害怕的因为Nvidia的驱动而黑屏的问题。你也会在互联网上查询为什么会出现这种问题，但你得到的并不是“修改注册表”这种答案，而是会学习到：为什么Nvidia在Linux上会出现这种问题？我怎么做才能解决驱动问题？其他驱动是否也有类似Nvidia这种问题？ 当你解决问题后，你的电脑开始正常工作了，你便开始使用它作为你的日常使用... 关于使用Linux学习的原因的最后一点是我认为自己不够慎独，不够克制。当我使用Windows的时候，并不能完全克制住自己接触那些新鲜游戏的念头，我玩起游戏来，通常会连续很长时间，可能是一天-_-。不过我并不是说Linux上没有游戏，相反，Linux是对很多游戏的支持是很好的，你可以玩到很多游戏，但你是否会因为使用Linux对游戏不再那么执着，至少我是如此了。这一点可以归结为“使用Linux对戒游戏有帮助吧” ，哈哈。 再谈谈自由： 我对自由的理解是：软件在你的掌控之中，你可以了解它的每一部分，你可以去到你想到达的地方，不受任何限制，这只取决于你愿不愿意。 来看看基本的Linux目录吧： 这些目录你可能有很多都不认识，但没关系，因为这就是Linux系统(大部分)所有的目录了，你稍微了解下，就知道这些目录里放的是什么文件了。 这也是我个人的体验而已，总之，Linux的自由是一种开源精神，比我描述的可大的多。至于Windows，我到现在连C盘的目录放了些什么都不太熟悉，但我并不是在贬低Windows，因为这就是Windows易用性的代价，相应的，Linux作为自由软件，它也有很多缺点。 Linux作为日常使用的缺点 硬件驱动问题 硬件驱动问题一般是在安装Linux时会出现的问题，根据个人电脑配置的不同，你的电脑的硬件驱动可能与要安装的Linux发行版不兼容，导致系统出现相应的问题。我这几天对驱动问题最深刻的体会就明白了为啥Linus大神会吐槽: “Nvidia Fuck You”。很多驱动厂商对Linux系统是闭源的，你可以下载这些厂商的驱动，但是能不能用，或者用起来有什么毛病都得你自己买单。 随着Linux开始在普通用户中变得流行起来，我相信今后Linux的生态会发展的越来越好，且现在很多Linux发行版对各种硬件的兼容性也越来越好，就以我之前使用的Ubuntu18来说，Nvidia，Wifi，蓝牙等驱动使用都是没啥问题的。我现在使用的Debian10 Buster对Nvidia的支持可能还不是那么好，使用起来总有一些小毛病，不过无伤大雅，其实没毛病我还有点不适应，不是说Debian是Ubuntu的爸爸吗，哈哈。 软件问题 不得不承认的一点是Linux的软件生态确实没有Windows那么丰富，你在考虑切换系统之前，必须先调查清楚Linux上是否有你必需的软件，你所需的软件是否支持跨平台或者是否有可替代的应用。我个人对软件要求较为简单，大部分都是生产力工具，其他的应用如娱乐软件之类的都可以使用网页版作为替代。如果你在Linux系统上想尝试游戏的话，我认为也是OK的，因为我也尝试过Linux Dota2 ，体验非常好(不是广告-_-)。不过大多数国内游戏厂商对Linux的支持都是很差的，所以如果过不了这道坎，也不要切换系统了。 软件问题其实可以分为2部分看待，一部分就是刚刚介绍过的生态问题，另一部分就是当你在使用某些软件的时候，总会出现某些小Bug。 就以Fcitx来说，Fcitx是一款通用的Linux输入法框架，被称为小企鹅输入法，很多输入法都是在Fcitx之上开发的，如搜狗，Googlepinyin，Sunpinyin等。使用过Fcitx的同学可能会遇到这种问题：当你在使用Fcitx在某些软件上打字时，候选框并不会跟随你光标的位置，而是总会固定在某一个位置，并且你无法改变，这个问题是我目前见过的最大Bug。不过这个Bug只在部分软件上有，在Chrome，Typora上都没有这个问题，这让我怀疑是软件的国际化问题，而非Fcitx问题。 所以第二个部分总结起来就是某些软件可能会出现某些未知的Bug，你得寻求解决的办法，或者忍耐使用，使用Linux也是得牺牲一些代价的。 你真的需要完全使用Linux吗? 说到这里，其实我想借用知乎某位前辈的话来表达一下我的真实想法: “Linux最好的地方在与开放自由，最大的毛病也是在这里。普通人没有能力去选择，也没有时间做选择。透明就一定好么？也有很多人喜欢被安排啊！“ (知乎 - 汉卿) 就像我开头说过的: “我对Linux并不熟悉，这其中一部分原因是我使用的是对用户较为友好的发行版Ubuntu，另一部分原因是我仍然没有那么大的动力去学习Linux，即使它一直作为我的日常使用。” 我使用Linux是为了学习和自由，我确实在Linux上感受到了自由，且学到了很多东西，但我却一直沉溺在这种使用Linux带来的满足感之中，并不能真正理解Linux给我们带来的到底是什么。 这次从Ubuntu切换到Debian的原因是我想尝试换个新的环境，但是当我花了3，4天后，我明白了：我只是呆在一个地方久了，想换个新地方而已，但老地方不一定坏，因为我都没怎么了解过这个老地方，就像当初我从Windows换到Linux那样，我都没有深入的了解过Windows就换了，那一段时间我还抱怨Windows的各种缺点，现在看来，非常可笑。 结尾 一文把想说的话几乎都给说了，个人文笔有限，且本文主观意识太强，如果觉得本文不符合您的胃口，就当看个笑话吧。 我使用Debian/Ubuntu时遇到的问题 以下内容是我在Debian10 Buster下遇到的问题以及相关解决办法， 使用Ubuntu和Debian其他版本的同学也可借鉴。 PS:欢迎各位同学在此处写下你遇到的问题和解决办法。 IDEA编辑Markdown预渲染问题 解决这个问题花了我很长时间。 当我安装IDEA后，使用它编辑markdown文件的时候，就出现了如下图所示的情况: 你可以看到右边渲染的画面明显有问题。刚开始的时候我一度怀疑是IDEA版本的问题， 于是我又安装IDEA其他版本，但也没有任何作用，这时我怀疑是显卡的原因: 可以看到使用的是Intel的核显，于是当我查询相关资料，使用脚本将核显换为了独显，这里没留截图，当你换到独显后， 图形会显示独显的配置，使用nvidia-smi命令可以查看独显使用状态。 于是我满怀期待的打开IDEA，但还是无济于事。当我以为真的是Debian的Bug的时候， 我又发现Bumblebee可以管理显卡，何不一试？于是我安装Bumblebee后，使用optirun命令启动IDEA，没想到啊， 还真是可以: 我真的就很奇怪，同样是使用了独显，为什么optirun启动就可以正常显示。 于是我后来又查询optirun是否开启了gpu加速，但很可惜，我并没有得到相关答案，不过这让我确定了这个问题出现在 显卡上。如果有知道原因的同学，敬请告之，感激不尽。 wifi适配器找不到 我猜(不确定)这个问题应该发生在大多数使用联想笔记本的同学的电脑上，不止Debian，且Ubuntu也有这个问题。 当安装完系统后，我们打开设置会发现wifi一栏显示 “wifi适配器找不到” 此类的错误信息。 这个问题的大概原因是：无线网络适配器被阻塞了，需要手动将电脑上的wifi开关打开，而在我的笔记本上并wifi开关， 所以可以猜测是联想网络驱动的问题。 可以使用 rfkill list all命令查询你的wlan是否被阻塞了，没有此命令的同学可以使用 sudo apt-get install rfkill 安装，当wlan显示Hard blocked: true , 就证明你的无线驱动被阻塞了。 解决办法是将阻塞无限驱动的那个模块从内核中移除掉，直接在 /etc/modprobe.d 目录下编辑 blacklist.conf文件，其内容为: blacklist ideapad_laptop 文件名不一定要与我的一致，但是要以.conf结尾。 你可以将modprobe.d目录下的文件理解为黑名单文件， 当Linux启动时就不会加载conf文件指定的模块， 这里的 ideapad_laptop 就是我们需要移除的那个无线模块。 后遗症： 当我们移除 ideapadlaptop 模块后，以后开机的时候，有时会出现 蓝牙适配器找不到的情况,之前在Ubuntu上却并未发现这种问题， 看来Debian在驱动方面没有Ubuntu做的好，不过这也是可以理解的， 而且大多数时候还是可以正常使用的--。 XMind安装 XMind是使用Java编写的，依赖于Openjdk8。所以在Linux上使用XMind， 首先需要有Openjdk8的环境。 其次启动的时候需要编写Shell脚本来启动(不是唯一办法，但却是非常简单的办法)，没想到吧，我也没想到， 这也是我趟过很多坑才玩出来的。 首先我们需要准备一张XMind的软件启动图片:XMind.png， 这个我已经放到目录 下了,需要的同学请自取。 其次我们进入XMind_amd64目录下，32位系统的同学进入XMind_i386目录， 我们创建并编辑 start.sh 脚本，其内容为: #!/bin/bash cd /home/guang19/SDK/xmind/XMind_amd64 (这个路径为你的XMind脚本的路径) ./XMind 这个脚本的内容很简单吧，当启动脚本的时候，进入目录，直接启动XMind。 脚本写完后需要让它能够被执行，使用 chmod +x start.sh 命令让start.sh可以被执行。 此时你可以尝试执行 ./start.sh 命令来启动XMind，启动成功的话， 就已经完成了99%了，如果启动不成功，可以再检测下前面的步骤是否有误。 如果以后你只想用Shell启动XMind的话，那么到此也就为止了，连上面所说的图片都不需要了。 如果你想更方便的启动的话，那么就需要创建桌面文件启动。 在Debian/Ubuntu下，你所看到的桌面文件，都存储在 /usr/share/applications 目录下面(也有的在.local/share/applications目录下)，这个目录下文件全是以.desktop结尾。 我们现在就需要在这个目录下创建xmind.desktop文件(名字可以不叫xmind)。 其内容为: [Desktop Entry] Encoding=UTF-8 Name=XMind Type=Application Exec=sh /home/guang19/SDK/xmind/XMind_amd64/start.sh Icon=/home/guang19/SDK/xmind/XMind.png 我们暂时只需要理解Icon和Exec属性。 Icon就是你在桌面上看到的应用的图标，把Icon的路径改为你XMind.png的路径就行了。 再看Exec属性,当我们在桌面上点击XMind的图标的时候，就会执行Exec对应的命令或脚本， 我们把Exec改为start.sh文件的路径就行了，别掉了sh命令，因为start.sh是脚本， 需要sh命令启动。 以上步骤完成，保存desktop文件后，你就可以在桌面上看到XMind应用了。 Fcitx候选框的定位问题 这个问题贴一张我处境的截图就明白了: 可以看到我的光标定位在第207行，但是我输入法的候选框停留在IDEA的左下角。 为什么我要说停留在IDEA的左下角？因为就目前我的使用而言，这个问题只在IDEA下存在， 不仅是Debian，Ubuntu也存在这种问题。 我个人认为这应该是IDEA的问题，查到的相关文章大部分都是说Swing的问题，看来这个问题还真是比较困难了。 如果有同学知道解决办法，还请不吝分享，非常感谢。 这两天抽了点时间就尝试写了个swing的小demo，想康康这bug是不是出在IDEA上，结果加上我的测试以及又一次探索， 让我确定了这个bug就是jdk swing的bug，目前存于linux下(不知道mac下有没有)。 "},"gitbook_doc/datastructure-algorithm/数据结构与算法简介.html":{"url":"gitbook_doc/datastructure-algorithm/数据结构与算法简介.html","title":"数据结构与算法简介","keywords":"","body":"数据结构与算法 往往有人把数据结构与算法混为一谈，以为学习数据结构就是在学习算法，或者以为学习算法 就是在学习数据结构，这样的理解是不对的。 数据结构与算法属于两种不同的学科，数据结构主要讲的是数据存储的问题，而算法主要讲的 是数据处理的问题，这种学科的内容是不一样的。不过既然都涉及都数据层面， 且有些数据结构是随着某种算法的诞生而被发明出来的，所以前辈计算机科学家们把它们归为一门课程。 如有错误之处，请多多指教 PS:部分的图片来源于网上，如有侵权，请联系俺，俺会立刻删除。 由于数据结构与算法部分的内容不比普通的某一项技术，它的内容实在是很多，且较为复杂，我会尽我最大能力持续更新的。 此外关于数据结构与算法,我推荐一个非常好的可视化网站: 数据结构与算法可视化 "},"gitbook_doc/datastructure-algorithm/数据结构.html":{"url":"gitbook_doc/datastructure-algorithm/数据结构.html","title":"数据结构","keywords":"","body":" 数据结构与算法 数据结构 线性数据结构 线性表 队列 栈 非线性数据结构 稀疏数组 Hash表 树 二叉树 满二叉树 完全二叉树 二叉查找树(BST) 数据结构 线性数据结构 线性数据结构是一个有序数据元素的集合，其中数据元素之间的关系是一对一的关系。 常见的线性结构有：线性表，栈，队列，数组等。 线性表 线性表(list)是最基本的数据结构，它是由零个或多个数据元素组成的有序集合， 这个有序不是指的排序后的有序，而是指添加元素的顺序，假如2在1之前被放入了集合中，那么2肯定在1前面，不会因为大小原因而 改变他们在集合中的顺序。 链表的实现方式主要有数组和链表。 数组实现的线性表，访问速度是其最大的优点，但由于数组的容量是固定的，所以如果采用数组作为线性表的实现， 那么就涉及到扩容，调整元素位置等问题，这些操作是非常消耗性能的，因此数组实现的线性表适用于读多写少的环境。 链表可以分单向链表,双端链表和双向链表(双向循环链表)。 单向链表的每个节点都保存了指向下一个节点的引用(称为next)，单向链表的尾节点的下一个节点 的引用为null。 双端链表的每个Node都保存了前后2个节点的引用，双端链表的first节点的前一个节点为null, last节点的后一个节点为null。 双向链表的每个Node都保存了前后2个节点的引用， 双向链表的first节点的前一个节点指向last节点， last节点的最后一个节点指向first节点。 无论是单链表和双链表，因为每个节点都保存了其他节点的引用，所以是比较消耗内存的。 链表实现的线性表，增删改操作的性能是其最大的优点，但链表并不支持下标这种RandomAccess随机访问的方式， 所以链表实现的线性表适用于写多读少的环境。 线性表实现源码(链式队列使用的是双端链表实现的): ArrayList LinkedList 队列 队列（queue）是一种采用先进先出(FIFO)策略的抽象数据结构，即最先进队列的数据元素， 同样要最先出队列。队列跟我们排队买票一样，先来排队的肯定先买票，后来排队的的后买到票。 队列有两个重要的概念，一个叫队首，一个叫队尾，队首指向的是第一个元素，而队尾指向的是最后一个元素。 队列有两个主要的操作：入队(enqueue)操作 和 出队(dequeue)。 入队操作是将一个元素添加到队尾，出队操作就是取出队首的元素。 队列的底层可以用数组和链表实现。 对于数组实现的队列来说，其容量肯定是有限制的，当队列满了之后，如何能够再次使用呢？ 这就是循环队列的作用了。 队列实现源码: 循环队列 链式队列 PS:关于队列，可以参考: 掘金 - 队列 , 写得很详细。 栈 栈(stack)是一种后进先出(LIFO)的数据结构，即最后进入栈中的元素，最先出栈。 栈有两个主要的操作: 入栈(push) 和 出栈(pop)。 入栈操作是将一个元素置于栈顶，出栈操作是将栈顶元素取出。 栈同样可以由数组和链表实现，对于数组实现的栈来说，其容量肯定也有限制，所以个人认为实现栈的较好的方式是链表。 栈实现源码：栈 非线性数据结构 非线性数据结构中各个数据元素不再保持在一个线性序列中，每个数据元素可能与零个或者多个其他数据元素发生联系。 根据关系的不同，可分为层次结构和群结构。 常见的非线性数据结构主要有矩阵(多维数组)，广义表，树，图等。 稀疏数组 稀疏数组又被称为稀疏矩阵，它本质上是一个二维数组。 当一个数组中大部分元素都未被使用，仅有少部分被占用，这就造成了空间的浪费。 为了解决这种问题，可以使用稀疏数组来保存该数组的数据，稀疏数组可以理解为原数组被压缩后的数组。 如图所示，有大部分的空间是无用的: 我们使用稀疏数组进行压缩,稀疏数组的第一部分所记录的是原数组的行数和列数，即元数据。 第二部分记录原数组中的有效值的位置和值。这样经过压缩后的数组不再需要分配原数组那么大的空间，解决了空间浪费的问题。 稀疏数组源码: 稀疏数组 Hash表 Hash表是一种能够快速访问，快速删除与修改的数据结构。我们知道数组的访问速度非常快，链表的增删改等操作的效率也是 非常高的，那有没有一种数据结构能够融合数组和链表的优点呢?可以说Hash表就是这样一种数据结构。 Hash表是通过关键码(key)进行访问，通过Hash表存储数据的数据结构，它通过某种映射函数将关键码映射到 Hash表的某个位置从而进行快速定位。 其中映射函数被称为散列函数(Hash函数)，Hash表被称为散列表。 Hash表源码: HashTable 树 树是一种抽象的数据类型，它是由n个有限节点组成的一个具有层次关系的集合，因其结构像一棵倒状的树，所以称其为树结构。 树的重要的概念 节点(结点): 在树结构中，存储数据的每个元素都被称为节点，如上图中的A，B，C等都是节点。 父节点: 一个节点A如果包含了另一个节点B， 则节点A就是节点B的父节点，节点B是节点A的子节点。 兄弟节点: 若多个节点具有相同的父节点，这些节点就是兄弟节点。 根节点: 每棵非空的树都有且仅有一个根节点，上图的A就是根节点。 叶子节点: 如果一个节点没有任何子节点，那么此节点就是叶子节点，上图的K，L，F，G等都是叶子节点。 节点的度: 节点的度是指该节点拥有的子树数。 树的度: 一棵树中，最大的节点度就是这棵树的度。 树的深度: 从这个树的根节点出发，根节点处于第一层，其孩子节点处于第二层，依次往下。上图树的深度 为4或3(计算方法的不同而已，如果把根节点算作第0层，那么树的深度就是3)。 森林: 由m棵互不相交的树组成的集合被称为森林，上图中，以B，C，D为根节点的三棵子树就组成了森林。 二叉树 二叉树是一种特殊的树结构，二叉树的每个节点最多有2棵子树，即二叉树的节点的度最大为2。 二叉树主要有如下性质: 二叉树的第 i 层最多有 2^(i-1) 个节点。 如果二叉树的为 d，那么二叉树总共有不超过 (2^d)-1 个节点。 二叉树有许多不同的种类，如: 满二叉树，完全二叉树，排序二叉树(二叉搜索树，二插查找树)，平衡二叉树等。 和线性表一样，二叉树也有链表(节点)实现和数组实现，数组实现的二叉树被称为 顺序存储的二叉树。 二叉树实现: BinaryTree ArrayBinaryTree 满二叉树 满二叉树除了叶子节点，其他的节点的子树都为2,即除了叶子节点外， 它的每个节点的度都为2。 满二叉树主要有如下性质: 满二叉树的第 i 层的节点有 2(i-1) 个。 满二叉树的深度为 d ， 那么此满二叉树必须有: (2^d)-1 个节点。 具有 n 个节点的满二叉树的深度为: log2(n+1)。 完全二叉树 如果一棵二叉树除去最后一层节点外是满二叉树，且最后一层节点是从左到右紧密分布的，那么此二叉树是一棵 完全二叉树。 二叉查找树(BST) 二叉查找树(Binary Search Tree)也被称为为二插排序树(Binary Sort Tree)。 二叉排序树具有如下性质： 若根节点的左子树不为空，那么其左子树的所有节点的值必将小于根节点的值。 若根节点的右子树不为空，那么其右子树的所有节点的值必将小于根节点的值。 左右子树也都是二叉查找树。 BST的左子节点的值小于根节点的值小于右子节点的值，这代表着如果对一棵BST进行中序遍历，那么即可得到 一个有序的数列。 关于BST的操作，我觉得这里需要讲解一下他的删除操作。 BST的删除操作的节点需要分为三种情况，这里只探讨: 要删除的节点既有左子树也有右子树的情况。 当BST需要执行删除操作的时候，如果要删除的节点既有左子节点，也有右子节点，那么就需要找到这个节点的 前驱节点或者后继节点，然后使用前驱节点或后继节点替换掉要删除的节点，这里的前驱节点和后继节点并不是指 前一个或下一个，而是指按二叉树的中序遍历顺序来讲，遍历这个节点时，它的前一个节点和后一个节点。 如下图，要删除的节点为 47: 那么当中序遍历此BST时，47的前驱节点为37,后继节点为48，当使用前驱节点或后继节点替换掉要删除的节点后， 那么会有如下2种结果: 至于使用前驱还是后继，这个可以自行选择。 二叉查找树实现: BinarySearchTree "},"gitbook_doc/datastructure-algorithm/算法.html":{"url":"gitbook_doc/datastructure-algorithm/算法.html","title":"算法","keywords":"","body":" 算法 排序算法 查找算法 双索引算法-快慢指针(leetcode) 字符串匹配算法 算法 排序算法 常见排序算法有10种,这里借用一张图总结下: 冒泡排序源码: BubbleSort 选择排序源码: SelectionSort 插入排序源码: InsertionSort 希尔排序源码: ShellSort 快速排序源码: QuickSort 归并排序源码: MergeSort 堆排序源码: HeapSort 基数排序源码: RadixSort 计数排序源码: CountingSort 查找算法 二分查找算法源码: BinarySearch 插值查找算法源码: InterpolationSearch 斐波那契查找算法源码: FibonacciSearch 双索引算法-快慢指针(leetcode) 双指针，指的是在遍历对象的过程中，不是普通的使用单个指针进行访问，而是使用两个相同方向的指针进行扫描，从而达到相应的目的。 个人认为此算法就是充分利用数组有序这一特征，将本来需要两次遍历的操作简化为一次遍历，从而大大提高的程序的效率 下面用图来简单解释一下这个算法： 以leetcode第283题为例，需要看源码的小伙伴我会放链接在下方 首先我们定义两个指针，一个名为left，一个名为right 一开始left指针不动，right指针向后搜索，遇到一个非0的数字，将其覆盖到left指针位置上的元素，之后left指针的也向后移动 继续搜索，以不断的将非0元素往前挪，当right指针达到数组末端后，搜索结束。 可以看到，我们已经将所有非0元素都移动到数组的头部了，接下来对left+1位之后的元素全都赋值为0即可。 最终效果如下: leetcode-283题: 移动零源码：MoveZero leetcode-26题: 删除数组的重复项源码：DeleteDuplicate leetcode-27题: 移除元素源码：RemoveElement 字符串匹配算法 暴力匹配算法源码: ViolenceMatch kmp匹配算法源码: KMPMatch "},"gitbook_doc/design_pattern/设计模式简介.html":{"url":"gitbook_doc/design_pattern/设计模式简介.html","title":"设计模式简介","keywords":"","body":" 设计模式 什么是设计模式? 什么是GOF? 设计模式的分类 设计模式 参考: 菜鸟教程 - 设计模式 c语言中文网 - 设计模式 如有错误之处，敬请指教。 什么是设计模式? 设计模式是前辈计算机科学家们在应用程序开发中积累的解决某些特定问题的经验和套路。它不是某种特定的语法， 而是一套用来提高代码可复用性，可维护性，可读性，稳健性以及安全性的解决方案。 设计模式是软件开发的基石，如同大厦的一块砖石一样，巧妙合理的使用设计模式可以解决软件开发中的许多问题， 每种设计模式都描述了发生在我们周围不断发生的问题，以及该问题的核心方案，这也是设计模式被广泛使用的原因之一。 设计模式不限制于某一种语言，它适用于c++,c#,go,java等面向对象的语言。 PS:本篇文章中所有的代码将采用Java实现。 什么是GOF? 在1994年，Erich Gamma，Richard Helm ， Ralph Johnson和John Vlissides四人合著了一本书， 书名叫作(Design Patterns: Elements of Reusable Object-Oriented Software)《设计模式：可复用面向对象软件的基础》。 书中提出和总结了对于一些常见软件设计问题的标准解决方案，称为软件设计模式， 由于四位作者的被合称为 GOF， 所以书中的设计模式也被称为 \"四人帮设计模式(Gang of Four design patterns)\"。 设计模式的分类 GOF中总共有23种设计模式，这些设计模式并不是孤立存在的，有些设计模式是相互关联的，所以一个应用程序在很大程度上 可能会使用多种设计模式。 设计模式可分为3大类: 创建型模式(Creation Pattern): 创建型设计模式描述的是 \"怎样创建对象\",它的主要特点是将 对象的创建和使用分离， 这使得程序在创建对象与使用对象时更加灵活。 创建型模式有: 工厂方法模式 抽象工厂模式 单例模式 建造者模式 原型模式 结构型模式(Structural Pattern): 结构型设计模式描述的是 \"如何将类和对象按照某种布局，组成更大的结构\"，这种类型的设计模式 关注的是类和对象的组合。 结构型模式有: 代理模式 适配器模式 装饰器模式 桥接模式 组合模式 外观模式 享元模式 行为型设计模式(Behavioral Pattern): 行为型设计模式描述的是 \"类或对象之间怎样相互协作以及怎样分配各自的职责，完成单个类或对象无法完成的任务\"。 行为型设计模式有: 观察者模式 模板方法模式 策略模式 责任链模式 中介者模式 访问者模式 命令模式 解释器模式 迭代器模式 备忘录模式 状态模式 "},"gitbook_doc/design_pattern/七大原则.html":{"url":"gitbook_doc/design_pattern/七大原则.html","title":"七大原则","keywords":"","body":" 设计模式7大原则 开闭原则 里氏替换原则 依赖倒转原则 单一职责原则 接口分离原则 迪米特法则 合成复用原则 设计模式7大原则 参考: 菜鸟教程 - 设计模式 c语言中文网 - 设计模式 如有错误之处，敬请指教。 开闭原则 开闭原则的定义是对扩展开放，对修改关闭。当应用程序的需求需要改变或扩展时，我们应该在不修改其源代码的 情况下，也能完成相应的目的。 开闭原则的优点是:使得应用程序更易于维护和扩展。 里氏替换原则 里氏替换原则的定义是继承必须确保超类所拥有的性质在子类中仍然成立，即在基类出现的地方，其子类一定可以出现， 子类可以扩展基类的功能，但是尽量不要重写基类的功能。 里氏替换原则的优点是: 可以规范我们在正确的地方使用继承，而不至于造成继承的使用泛滥。 依赖倒转原则 依赖倒转原则是实现开闭原则的基础，它的意思是当我们在编写面向对象的应用程序时，我们需要针对接口或抽象类编程， 而不具体的依赖某个实现类，这样可以降低系统之间的耦合性。 依赖倒转原则的优点是: 通过抽象建立系统之间的关系，使得系统具有高度的可维护性和可扩展性。 单一职责原则 单一职责原则规定一个类应该有且仅有一个能够引起它变化的原因，否则此类应该被拆分。单一职责的意思是不应该让一个类 承担太多职责，否则如果一个职责修改，其他职责可能也会跟着修改，且如果一个客户端只需要这个类的一个职责时， 那么客户端不得不承受引入其他职责的代价。 单一职责原则的优点是: 提高了代码的可读性，不至于一个类里啥元素都有，且系统之间更加的高内聚与低耦合。 接口分离原则 接口分离原则描述的是当一个接口的功能和职责太多时，我们需要将这个大接口分割成若干小接口，每一个小接口只 服务于其对应的客户端。但是我们也需要控制每个小接口的粒度，如果粒度太小，那么会增加许多冗余的接口，不利于维护。 接口分离原则的优点是: 避免一个接口里含有不同的职责，每个接口的职责分明，与单一职责相似，都符合高内聚与低耦合的思想。 迪米特法则 迪米特法则又称 \"最少知道原则\" ， 它的定义是 \"只与你的朋友交谈，不与陌生人说话\"， 这句话的含义是如果两个软件实体或 服务之间无需直接通信，那么就不应当发生直接的相互调用，可以通过第三方实体或服务进行转发通信。 其目的是为了降低系统的耦合度。 迪米特法则的优点是: 降低系统的耦合性，减少系统之间的关联，也符合高内聚与低耦合的思想。 合成复用原则 合成复用原则规定 当我们需要复用一些系统的代码的时候，应该优先考虑组合或聚合的方式实现，其次再考虑使用继承的方式实现。 如果一个基类的功能太多，而你只想复用一部分功能，使用继承就意味着不需要的功能也会被添加到当前系统中来，这就造成了不必要的麻烦。 所以可以优先考虑使用组合的方式完成代码的复用。 合成复用原则的优点是: 使系统易于维护，提高代码的可读性。 "},"gitbook_doc/design_pattern/二十三种设计模式.html":{"url":"gitbook_doc/design_pattern/二十三种设计模式.html","title":"二十三种设计模式","keywords":"","body":" 23种设计模式 工厂方法模式(Factory Method) 抽象工厂模式(Abstract Factory) 单例模式(Singleton) 建造者模式(Builder) 原型模式(Prototype) 代理模式(Proxy) 适配器模式(Adapter) 装饰器模式(Decorator) 桥接模式(Bridge) 组合模式(Composite) 外观模式(Facade) 享元模式(Flyweight) 观察者模式(Observer) 模板方法模式(Template Method) 策略模式(Strategy) 责任链模式(Chain of Responsibility) 中介者模式(Mediator) 访问者模式(Visitor) 命令模式(Command) 解释器模式(Interpreter) 迭代器模式(Iterator) 备忘录模式(Memento) 状态模式(State) 23种设计模式 参考: 菜鸟教程 - 设计模式 c语言中文网 - 设计模式 如有错误之处，敬请指教。 工厂方法模式(Factory Method) 工厂方法模式的意图是 定义一个创建产品对象的工厂接口，将产品对象的创建延迟到具体子工厂类中，这满足创建型模式中对象的创建和使用分离 的原则。 工厂方法模式由 抽象工厂，具体工厂，抽象产品，具体产品等要素组成， 我们把需要创建的对象叫作产品，创建产品的对象叫作工厂。 抽象工厂: 抽象工厂声明了创建产品的接口，用户/客户端 通过抽象工厂来创建和访问产品。 具体工厂: 具体工厂是抽象工厂的实现，包含了创建产品的过程。 抽象产品: 抽象产品定义了产品的规范，描述了产品的主要特性和功能。 具体产品: 具体产品实现了抽象产品，它与具体工厂一一对应。 工厂方法模式的优点有: 隐藏具体实现细节: 用户只需要具体工厂就可以创建对应的产品，无需关心具体工厂是如何创建产品的。 符合开闭原则: 在系统需要添加新的产品时，只需要添加其对应的具体工厂，无需修改原有的工厂和产品。 工厂方法模式: 工厂方法模式实例: FactoryMethodTest 抽象工厂模式(Abstract Factory) 工厂方法模式考虑的是同一类产品的生产，如汽车工厂只生产汽车，空调工厂只生产空调。 而在现实中许多工厂是综合性的工厂，一家工厂可能既生产空调，又生产冰箱，这也是抽象工厂与工厂方法的区别， 抽象工厂考虑多类产品的生产，同一个工厂生产的位于不同等级的一组产品称为这个工厂的产品族。 抽象工厂模式的意图是 定义一个创建一组相关或相互依赖的工厂接口，使得用户可以使用这一个工厂就可以获取到不同的产品。 抽象工厂是工厂方法的升级版本，工厂方法只生产一个等级的产品，抽象工厂可以生产多个等级的产品。 抽象工厂和工厂方法一样，也由 抽象工厂，具体工厂，抽象产品，具体产品等要素组成，但抽象工厂可以创建 多个等级的产品。 抽象工厂模式: 抽象工厂实例:AbstractFactoryTest 单例模式(Singleton) 单例模式确保一个类只有一个实例，且该类提供访问这个唯一实例的方式。 单例模式有三个特点: 单例类只有一个实例。 该单例类的唯一实例必需由其自身创建。 该单例类必须提供访问其唯一实例的方式。 单例模式: 单例模式实例: Singleton 单例模式主要有两种写法:懒汉式和饿汉式。 网上有的文章说 \"饿汉式的单实例在类加载阶段就实例化了，而懒汉式在第一次获取实例的时候才初始化。\" 这里我从Java/Jvm(其他语言暂且不论)的角度来反驳下这种观点，如果各位同学同意这种观点，且是从我写的类的生命周期 部分一直看过来的，那么请再好好思考下 \"类加载\" 三个字 ， 如果你对JVM有兴趣，也可分析下我的观点是对是错。 在JVM中，类的生命周期有5个阶段 : 加载 ， 连接 ， 初始化，使用和卸载。这几个阶段我已在类的生命周期 里详细讲解过了，这里再回顾下 加载，连接，初始化几个阶段，至于使用和卸载与本观点论述的内容不大，所以不做描述。 加载: JVM类加载器将class字节码读取到内存，将其保存在方法区，并在堆区生成该类的唯一Class对象。 连接: 连接分为三个阶段:验证，准备，解析。 验证: 在验证阶段，JVM会对class字节流进行验证，验证其是否符合JVM规范且是否会对JVM造成恶意损害。 准备: 准备阶段会为类的静态变量初始化零值，基本数据类型如int会赋予0，引用类型会赋予null。 解析: 解析阶段会将常量池中的符号引用解析为直接引用。 初始化: 初始化阶段是类加载阶段的最后一个阶段，在初始化类后，就可以使用类做我们需要做的事情了，如创建对象实例。 我们明白: 饿汉式的单例是属于类的静态变量的，只有当类初始化时，才会初始化其静态变量， 仅凭这点就可以推翻网上谣传的观点了，但还没有完。 类在何种情况下会被初始化呢? 解决了这个问题，就可以彻底明白饿汉式实例化的时间了，你也就不会跟着大众喊出 \"饿汉式单例浪费内存空间。\" 这种 毫无根据的话语了。 只有当我们主动使用类时，这个类才会被初始化，也才会初始化单例了。 根据我查阅的资料，我总结了类主要涉及8种情况会被主动使用: 当jvm执行new指令时会初始化类，即当程序创建一个类的实例对象。 当jvm执行getstatic指令时会初始化类，即程序访问类的静态变量(不是静态常量，常量归属于运行时常量池)。 当jvm执行putstatic指令时会初始化类，即程序给类的静态变量赋值。 当jvm执行invokestatic指令时会初始化类，即程序调用类的静态方法。 当使用反射主动访问这个类时,也会初始化类,如Class.forname(\"...\"),newInstance()等等。 当初始化一个子类的时候，会先初始化这个子类的所有父类，然后才会初始化这个子类。 当一个类是启动类时，即这个类拥有main方法，那么jvm会首先初始化这个类。 MethodHandle和VarHandle可以看作是轻量级的反射调用机制，而要想使用这2个调用， 就必须先使用findStatic/findStaticVarHandle来初始化要调用的类。 综上所述，只考虑正常使用时，初始化类的情况有: 创建类的实例对象；调用类的静态方法；给类的静态变量赋值；访问类的静态变量。 而当我们访问饿汉式单例的时候，几乎都是通过静态方法获取单例(不排除有人会用反射去获取单例)，所以大部分情况都是当我们获取单例的时候才会初始化单例， 所以 \"饿汉式的单实例在类加载阶段就实例化了\"，\"饿汉式单例浪费内存空间\" 此类观点就不攻自破了。 建造者模式(Builder) 创建者模式的定义是将一个复杂对象的构造与它的表示分离，使用同样的构建过程可以创建不同的表述。简单理解就是 创建者模式将一个复杂对象，分解为多个简单的对象，然后通过这些简单对象再构造成最终的目标对象。 比如一台计算机由很多部件组成，如cpu，gpu，主板等等，我们不可能从一家厂商那里购买到所有的部件(不考虑整机)， 所以我们需要根据我们自己事先计划好的计算机的配置，然后从各个不同的厂商那里购所需的部件，然后组装成我们理想的计算机。 建造者模式使得一个复杂对象被分解为多个简单对象，且可以对每个简单对象进行定制，客户端也无需关系创建对象的细节。 建造者模式由 指挥者，抽象建造者，具体建造者以及产品等要素组成: 产品: 产品是由多个部件组成的对象，它需要由具体建造者建造其各个组件。 抽象建造者: 抽象建造者声明了 创建产品各个组件以及获取最终复杂产品的接口。 具体建造者: 具体建造者实现了抽象建造者定义的接口，定义了创建复杂产品各个组件的逻辑。 指挥者: 指挥者负责调用建造者完成对复杂产品的创建。 建造者模式: 建造者实例: BuilderTest 原型模式(Prototype) 原型模式的定义是用一个已经创建的对象实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。 在有的应用程序中，存在着一些相同或相似的对象，如果这些相同或相似的对象都使用构造函数来创建，可能会比较消耗资源， 而用原型模式就可以快速创建与原对象相同的对象。 原型模式由 抽象原型类以及具体原型类等要素组成: 抽象原型类: 定义了具体原型类必须实现的接口(在java中，抽象原型类就是Cloneable接口)。 具体原型类: 具体原型是抽象原型的实现，具体原型的实例是可以被复制的。 原型模式: 原型模式实例:PrototypeTest 在java中，原型模式还涉及深拷贝与浅拷贝的问题，我推荐一篇写的非常好的文章建议各位同学学习: CSDN - Java中的clone方法 - 原型模式 代理模式(Proxy) 代理模式的意图是 为目标对象提供一种代理，用以控制客户端对于目标对象的访问。 举个栗子: 我(客户端)需要访问某个网站的服务器，但是目标网站的服务器的IP又不可能直接暴露在公网之上， 所以目标服务器通过提供代理服务器来解决这种问题。我们访问目标网站时，实际请求的是代理服务器，然后再由代理服务器 将请求转发到真正的处理任务的服务器(反向代理)，在这个过程中，代理服务器充当了保护或中介的角色，将真正的服务器隐藏了起来。 代理模式由 抽象主题，真实主题以及代理主题等要素组成: 抽象主题: 抽象主题声明了真实主题需要实现的接口。 真实主题: 真实主题就是目标对象，它需要实现抽象主题。 代理主题: 代理主题与真实主题一样，都需要实现抽象主题，但代理主题内部包含了真实主题，当访问代理主题的时候， 它会使用真实主题去处理，同时可以控制与扩展真实主题的功能。 代理模式: 代理模式实例: ProxyTest 适配器模式(Adapter) 适配器模式的意图是 将一个接口转换成客户希望或需要的另一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 比如一个播放器只能播放 .mp4 格式的视频，如果是 .flv 格式的视频这个播放器就不能播放了，此时我们就可以 创建一个视频适配器，将 .flv 格式的视频适配成播放器所需的格式的视频。 适配器模式实例: AdapterTest 装饰器模式(Decorator) 装饰器模式的意图是 在不改变原有系统结构的情况下，能够动态的给系统添加一些新的职责和功能。 比如我买了一辆车，我想给这辆车添加一个尾翼，添加尾翼后，并没有对原来的车身的其他地方做出修改。 装饰器模式主要由 抽象构件 ， 具体构件 ， 抽象装饰和具体装饰等要素组成: 抽象构件: 抽象构件定义了具体构件的职责。 具体构件: 具体构件实现了抽象构件，需要通过装饰器来为其拓展功能。 抽象装饰: 抽象装饰也实现了抽象构件，并包含了具体构件，可以通过具体装饰对其拓展功能。 具体装饰: 具体装饰继承了抽象装饰，并对具体构件做出扩展。 装饰器模式: 装饰器模式实例: DecoratorTest 桥接模式(Bridge) 桥接模式的意图是将抽象部分与实现部分分离，使得他们都可以独立的变化。桥接模式主要解决了在有多种变化的情况下， 泛滥使用继承的问题。某些对象具有多个方面的变化，如 有不同颜色和大小的字，有不同功率和品牌的小轿车等等。 桥接模式主要由: 抽象化角色，扩展抽象化角色，实现化角色和具体实现化角色组成： 抽象化角色： 抽象化角色即抽象类，它包含一个对实现化对象的引用 ，比如 字拥有对颜色的引用。 扩展抽象化角色： 扩展抽象化角色是对抽象化角色的扩展，比如 小字，大字，中等大小的字。 实现化角色： 实现化角色是定义了实现化的接口，供抽象化角色使用，如 颜色。 具体实现化角色： 具体实现化角色是实现化角色的实现，如 黑色，红色。 桥接模式: 桥接模式实例: BridgeTest 组合模式(Composite) 组合模式又称为 部分-整体模式，它是一种将对象组合成树状的层次结构的模式，用来表示“部分-整体”的关系。 比如一棵树上有许多树枝，每枝树枝上又有许多树叶，这就属于层次结构，我个人倾向于把它理解为 “一对多的关系”， 如何把这种关系合理的表现出来呢？就可以使用到我们的组合模式了。 组合模式主要由： 抽象构件，树叶构件和树枝构件等要素组成： 抽象构件： 抽象构件声明了树叶构件和树枝构件的公共接口。 树枝构件： 树枝构件实现了抽象构件声明的接口，它的作用是管理和存储子部件。 树叶构件： 树叶构件也实现了抽象构件声明的接口，它是组合中的叶子构件，没有子部件。 组合模式实例: CompositeTest 外观模式(Facade) 外观模式又被称为门面模式，外观模式的意图是通过为多个复杂的子系统提供一个一致的接口，使得这些子系统能够更容易的 被访问，客户端也不必关注内部子系统的实现细节，大大降低了应用程序的复杂度。 假设我们的某个系统有一个生成唯一ID 的接口，而生成唯一ID的方案有很多种，如雪花ID，UUID，自增ID等等，客户端只需要调用那一个生成ID的接口获取ID就行了， 无需关注这个接口到底使用的是哪种生成ID的方案。 外观模式实例：FacadeTest 享元模式(Flyweight) 享元模式的意图是 运用共享技术来支持大量细粒度对象的复用，以此达到节约内存，提高系统性能的目的。 在我们的应用中，有时需要创建一些相同或具有共同部分的对象，比如拥有相同姓名的用户，具有相同分辨率的屏幕等等。 享元模式由 抽象享元角色，具体享元角色，非享元角色和享元工厂等要素组成: 抽象享元角色： 抽象享元角色是所有具体享元角色的基类，为具体享元角色定义公共属性和接口 ，如Person。 具体享元角色： 具体享元角色实现了抽象享元角色定义的规范，如Programmer,Writer， 非享元角色： 非享元角色是不可共享的外部状态，如Person的age，height都可能不同，就不能共享，这些可以归纳为 Person的基本信息BasicInfo 享元工厂： 享元工厂负责创建和管理享元角色。 享元模式： 享元模式实例：FlyweightTest 观察者模式(Observer) 观察者模式的意图是 当多个事物之间存在一种一对多关系时，如果一个事物的状态发生改变，使得依赖这个事物的其他事物也能得到 通知，并做出相应改变，观察者模式有时被称为 发布-订阅者模式 或 模型-视图模式。在我们的生活当中，一个事物影响其他事物的 例子数不胜数：我们过马路时，当交通指示灯改变颜色，我们收到这一信息，便会做出反馈，我们知道该停下还是可以过马路； 如果我们在手机上订阅了某个节目的专栏，当专栏有新的信息时，便会推送给我们；当我们按下灯的开关时，灯会被点亮或熄灭。 观察者模式由 抽象主题角色，具体主题角色，抽象观察者角色和具体观察者角色等要素组成： 抽象目标角色： 抽象目标角色是具体目标角色的规范，它提供了用于管理观察者角色的方法，以及定义了通知观察者的 抽象方法。 具体目标角色(发布者)： 具体目标角色是观察者观察的具体目标，它实现了抽象目标角色定义的规范，当具体目标发生变化时， 便会通知所有的观察者。 抽象观察角色： 抽象观察角色是具体观察角色的规范，声明了当目标角色发生变化时，更新自身的方法。 具体观察角色(订阅者)： 具体观察角色实现了抽象观察角色定义的规范，它决定了当具体目标角色发生变化时，自身需要完成的逻辑。 观察者模式: 观察者模式实例：ObserverTest 模板方法模式(Template Method) 模板方法模式的意图是 定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类可以在不改变整体算法框架的 前提下，重新实现该算法的某些逻辑。 比如我们的程序中有一个保存用户信息的服务，保存用户的信息假设分为两个步骤：1.校验用户信息(check)。 2.使用jdbc将用户信息存入数据库(save)。 其中将用户信息存入数据库这一步骤几乎不会发生改变，但校验用户的信息这一步就可能有不同的方案， 所以校验用户信息这一步骤可以延迟到这个服务的子类中去，由不同的子类实现不同的逻辑。 模板方法模式由 抽象模板和具体子类等要素组成： 抽象模板： 抽象模板定义了整个服务或者算法的整体框架，它由一个模板方法和若干基本方法组成。 模板方法： 模板方法是抽象模板的核心，它定义了基本方法被调用的顺序。如保存用户信息的服务，是先check，还是先 save，保存用户信息的模板方法就负责整合这两个方法。 基本方法： 基本方法是整个服务或算法中的一个步骤，check和save都是基本方法，基本方法还可分为： 抽象方法： 抽象方法在抽象模板中声明，由具体模板实现。 具体方法： 具体方法在抽象模板中声明并实现，可以在具体模板中被重写。 钩子方法： 钩子方法在抽象模板中声明并实现，它的方法体通常是空的，可以在具体模板中实现， 所以钩子方法起到的是一个扩展的作用。 具体模板： 具体模板实现或重写了抽象模板中声明的抽象方法和钩子方法，具体的逻辑视这个具体模板的职责而定。 模板方法模式： 模板方法实例：TemplateMethodTest 策略模式(Strategy) 策略模式的意图是 定义一系列的算法，并将每个算法封装起来，使得它们可以相互替换，且算法的变化并不会影响到使用 该算法的客户端。 假设我要到一个地方去，有公交和地铁两种出行方式，无论选择何种方式出行，都可以使得我到目的地。 再假设我要将一个无序的序列排序成一个有序的序列，需要采用排序算法来解决问题，我可以在不同的环境下采用不同排序算法， 但无论何种排序算法其执行的结果最终肯定都是一致的。 策略模式由 抽象策略，具体策略和环境角色等要素组成： 抽象策略： 抽象策略声明了算法接口，各种不同的算法以不同的方式实现此接口。 具体策略： 具体策略实现了抽象策略，并提供了具体的算法逻辑。 环境角色： 环境角色持有一个策略的引用，最终客户端会使用环境角色来调用具体策略的算法。 策略模式： 策略模式实例：StrategyTest 责任链模式(Chain of Responsibility) 责任链模式又被成为职责链模式，它的意图是 为了避免一个请求发送者与多个请求处理者耦合在一起， 而将所有请求处理者连接成一条处理引用链，当有请求发送时，请求就会被这条请求处理引用链上的请求处理者处理。 责任链模式由 抽象处理者和具体处理者等要素组成： 抽象处理者： 抽象处理者声明了处理请求的接口， 且包含了当前处理者的下一个处理者。 具体处理者： 具体处理者实现了抽象处理者，当具体处理者处理完请求后，如果它还有后继处理者，那么就将请求 交予其后继处理者处理。 责任链模式： 责任链请求处理流程: 责任链模式实例: ResponsibilityChainTest 中介者模式(Mediator) 中介者模式遵循迪米特法则，它的意图是 使用一个中介者对象来封装一系列对象之间的交互，使原有对象之间不需要显示的调用彼此， 从而使其耦合松散。 中介者这个角色很好理解，比如在我们使用聊天应用程序进行通信的时候， 我们发送的消息是发送给了服务器，服务器收到消息后，再将消息发送给对方。 中介者模式由 抽象中介者，具体中介者，抽象同事角色和具体同事角色等要素组成： 抽象中介者： 抽象中介者声明了同事对象注册与转发同事对象信息的接口。 具体中介者： 具体中介者实现了抽象中介者，协调各个同事之间的交互关系。 抽象同事角色： 抽象同事角色声明了同事之间的交互接口。 具体同事角色： 具体同事角色实现了抽象同事角色声明的接口，当前同事角色与其他同事交互时，需要由中介者 负责协调。 中介者模式： 中介者模式实例： MediatorTest 访问者模式(Visitor) 访问者模式的意图是 将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的 前提下可以添加作用于这些元素的新操作，访问者模式将数据结构与操作分离，是行为模式中较为复杂的一种模式。 举个栗子： 公园(数据结构)中有多个景点(元素)，每个游客都可以访问这些景点，且对这些景点有各种不同的评价。 访问者模式由 抽象访问者，具体访问者，抽象元素，具体元素和对象结构等要素组成： 抽象访问者： 抽象访问者为每个具体元素都声明了访问具体它的接口。 具体访问者： 具体访问者实现了抽象访问者定义的接口，确定了访问一个元素时需要做的操作。 抽象元素： 抽象元素声明了被访问者访问的接口。 具体元素： 具体元素实现了抽象元素。 对象结构： 对象结构是包含了元素的容器，提供了让访问者访问容器中所有元素的方法，可以看作是 公园。 访问者模式： 访问者模式实例： VisitorTest 命令模式(Command) 命令模式的意图是 将请求封装为一个对象，使发出请求的责任和执行请求的责任分离开来，请求的发送者和接受者通过命令进行 沟通。 比如当我们使用电视遥控器(命令发送者)按下某个按钮(具体命令)时，电视机(命令接受者)就会做出相应的反馈。 命令模式由抽象命令，具体命令，命令发送者和命令接受者等要素组成： 抽象命令： 抽象命令声明了执行命令的接口。 具体命令： 具体命令是抽象命令的实现，它通过调用命令接受者的功能完成命令要执行的操作。 命令接受者： 命令接受者不与命令发送者直接交互，而是被命令所调用执行。 命令发送者： 命令发送者也不直接与命令的执行者交互，而是通过调用命令来执行相关请求。 命令模式： 命令模式实例： CommandTest 解释器模式(Interpreter) 解释器模式定义了 评估某种语言的文法，并建立一个解释器解释该语言的句子。如我们常见的SQL的解析，某种语言 的代码在执行之前也需要被解析。 解释器模式实例： InterpreterTest 迭代器模式(Iterator) 迭代器模式的意图是 提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 迭代器模式实例： IteratorTest 备忘录模式(Memento) 备忘录模式的意图是 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后在需要的时候 能够恢复到之前保存的状态。假设我们在更改一篇文章后，发现更改后的文章还没有更改之前好，于是就想将文章恢复到更改之前的状态， 此时就可以借助备忘录完成此需求。 备忘录模式由 备忘录，发起人和管理者等要素组成： 发起人： 发起人记录当前时刻的内部状态，提供创建备忘录和恢复备忘录数据的功能。 备忘录： 备忘录负责存储发起人的内部状态，并在需要的时候将内部状态传递给发起人。 管理者： 管理者负责管理备忘录，提供获取与保存备忘录的功能。 备忘录模式： 备忘录模式实例：MementoTest 状态模式(State) 状态模式的意图是 使得 一个对象的状态发生改变时，这个对象的行为也随状态的变化而变化，这种对象被称为状态对象。 举个栗子： 我们的服务器一般有start，active，destroy等状态，当服务器的状态发生改变时，其内部也需要做出相应的变化。 状态模式由 环境角色，抽象状态和具体状态等要素组成： 环境角色： 环境角色也被成为上下文，它维护一个当前状态，当状态发生改变时，其行为也将改变。 抽象状态： 抽象状态封装了环境角色在特定状态下的行为。 具体状态： 具体状态实现了抽象状态的行为。 状态模式： 状态模式实例：StateTest "},"gitbook_doc/computer_network/OSI七层模型.html":{"url":"gitbook_doc/computer_network/OSI七层模型.html","title":"OSI七层模型","keywords":"","body":"OSI OSI(Open System Interconnect开放式系统互联)。 它是ISO(国际标准化组织)提出的网络互联模型。 OSI模型定义了网络互联的7层框架: 应用层，表示层，会话层，传输层，网络层，数据链路层，物理层。 OSI七层模型 OSI网络互联模型为什么要分层? 分层可以把开放系统中信息交换的问题分解到具体的层级之中， 而各层可以对各自的功能独自做出修改和扩展，体现了解耦的思想。 物理层: 物理层的作用是利用光纤，电缆，双绞线等传输介质来传输比特流(光，电等信号)。 需要ISP(Internet Service Provider)互联网提供商的支持。 链路层: 链路层的作用是当网络层数据传输前，将数据封装成帧。 一个帧由一个数据字段和若干首部字段组成，网络层的数据报就存于数据字段中。 当帧中的数据作为bit传输时，接收方可能会判断错误，如0判断成1。 这种错误是由于在数据发送过程中外部因素如电磁噪声干扰所致， 许多链路层协议就提供了一种纠错机制，通过让发送节点在帧中包括差错bit，让接受节点进行差错检查。 网络层: 网络层主要负责数据的数据链路的寻址和路由选择。网络层的协议主要有:IP,ICMP等。 传输层: 传输层提供端口到端口的可靠的数据传输服务。传输层的协议主要有TCP和UDP。 会话层: 会话层的作用是负责建立，管理和终止主机之间的通信连接。 表示层: 表示层主要的作用是负责数据格式的转换。将应用层的数据转换为网络传输的格式， 或者将来自下一层的数据转为应用层协议能够处理的格式。 除此之外，数据的压缩解压，加密解密也都由表示层完成。 应用层: 应用层为应用程序提供SPI(Service Provider Interface)应用程序接口。 应用层协议是为应用程序提供服务保证的协议。 "},"gitbook_doc/computer_network/TCP_IP.html":{"url":"gitbook_doc/computer_network/TCP_IP.html","title":"TCP/IP","keywords":"","body":" TCP/IP TCP/IP协议族 HTTP(HyperText Transfer Protocol)(应用层协议) HTTPS(HyperText Transfer Protocol Secure) (应用层协议) SMTP(Simple Mail Transfer Protocol) (应用层协议) POP3(Post Office Protocol 3) IMAP(Internet Mail Access Protocol) DNS(Domain Name System) (应用层协议) FTP(File Transfer Protocol)(应用层协议) SFTP(Secure File Transfer Protocol) (应用层协议) Telnet / SSH(Secure Shell) (应用层协议) TCP协议(Transmission Control Protocol) TCP为什么可靠性较高? UDP协议(User Datagram Protocol) TCP与UDP主要区别 TCP如何保证可靠性传输? 为什么需要三次握手? TCP三次握手的过程 为什么需要四次挥手? TCP四次挥手的过程 TCP粘包和半包问题 解决TCP消息无边界的办法主要有以下几种: TCP/IP TCP / IP 不仅仅是指TCP和IP这两种协议，而是一系列网络协议的总和， 而这些协议中最核心的2个协议就是TCP和IP，所以被称为TCP/IP网络协议族。 TCP/IP协议族是互联网的基础通信架构。 TCP/IP协议族 HTTP(HyperText Transfer Protocol)(应用层协议) HTTP超文本传输协议。 是用于Web浏览器和Web服务器之间传递数据的协议。 HTTP协议以明文的方式发送内容，不提供任何方式的数据加密， 因此HTTP是一种不太安全的协议。 HTTPS(HyperText Transfer Protocol Secure) (应用层协议) HTTPS协议在HTTP协议的基础上加入了SSL协议， SSL依靠证书来验证服务器的身份， 所以HTTPS协议叫HTTP协议来说是安全的。 SSL协议作用于传输层协议和各种应用层协议之间。 SMTP(Simple Mail Transfer Protocol) (应用层协议) SMTP简单邮件传输协议。 它是用于从源地址到目标地址传输邮件的协议(发送邮件) POP3(Post Office Protocol 3) POP邮局协议的第3个版本。 POP3协议允许电子邮件客户端下载服务器上的邮件， 但是在客户端上的操作，并不会反馈到服务器上。 也就是说客户端的操作对服务端没有影响。(接受并处理邮件) IMAP(Internet Mail Access Protocol) IMAP交互式邮件存取协议。 它和POP3协议类似 ，但是IMAP对客户端的操作会反馈到服务端, 也就是说客户端邮件的状态与服务端邮件的状态是一致的。(接受并处理邮件) DNS(Domain Name System) (应用层协议) DNS域名系统。 DNS服务器可以看做是一个域名与IP地址相互映射的分布式数据库。 我们使用浏览器访问google的时候需要输入:google.com, 浏览器会帮我们到DNS服务器查询google.com对应的IP地址， 从而能够访问google的服务器。 FTP(File Transfer Protocol)(应用层协议) FTP文件传输协议。 FTP协议用于从一台主机将文件传输到另一台主机上。 但FTP协议在文件传输过程中，并没有为文件提供加密措施， 所以FTP协议是不安全的。 SFTP(Secure File Transfer Protocol) (应用层协议) SFTP安全文件传输协议。 SFTP实际可以看做是SSH的一部分。 当使用SFTP登录到目标主机后，可以使用SFTP指定的命令进行数据传输， 并且传输的数据是加密过的。 Telnet / SSH(Secure Shell) (应用层协议) Telnet和SSH都可以作为远程登录的协议。 但Telnet使用的是明文来传输数据，并且没有提供数据加密措施， 是不安全的协议。 而SSH协议则提供了对数据的压缩和加密，比Telnet要安全的多。 所以现在几乎大部分场景都使用的是SSH作为安全数据传输和远程登录的协议， 并且SSH提供了对SFTP的支持。 TCP协议(Transmission Control Protocol) TCP传输控制协议。 TCP协议是一种面向连接的，可靠的，基于字节流的传输协议。 TCP协议适用于对数据准确性和可靠性要求较高的应用，如文件传输等。 TCP为什么可靠性较高? TCP会采用校验和，确认应答与序列号， 拥塞控制，流量控制，超时重传， 连接管理(三次握手，四次回收)等机制保证传输的可靠性。 TCP报文格式: UDP协议(User Datagram Protocol) UDP用户数据报协议。 与TCP协议不同，UDP协议并不保证数据传输的可靠性， 无论目标主机是否可通信，UDP都会发送数据。 UDP适用于传输效率要求较高，允许一定数据丢失的应用，如语音，视频等。 PS:UDP报文格式 TCP与UDP主要区别 TCP基于连接，数据传输前需要做好可靠性准备工作(三次握手);而UDP是无连接的，只要有数据就可发送了。 TCP使用流量控制和拥塞控制等措施使传输更加可靠；而UDP则是不可靠的传输。 TCP是面向流的数据模式(无边界)；而UDP是面向报文的数据模式(有边界)。 TCP仅支持单播，点对点的数据传输；而UDP不仅支持单播，还支持组播，广播。 TCP首部开销较大(最大60字节，最小20字节);UDP首部开销较小(8字节)。 TCP如何保证可靠性传输? 校验和(16位) 在数据传输过程中，将发送的数据段分成若干个16位的整数。 将这些整数加起来，并且前面的进位不能丢弃，补在后面，最后取反， 得到校验和。 发送方在发送数据之前计算校验和，并将校验和填充到TCP报文中， 而接收方收到数据后，对数据以同样的方式进行计算，求出校验和， 与发送方的进行比对。如果比较失败，接收方将丢弃数据包。 确认应答ACK和序列号(32位) 在TCP传输过程中，每次接收方收到数据后，都会对发送方进行应答， 也就是响应ACK报文，这个报文中有对应的确认序列号。 超时重传 在TCP传输时，由于确认应答和序号机制，当发送方发送完数据后， 会等待接收方的ACK报文，并解析判断ACK报文(一般来说ACK为seq+1)， 如果发送方一直没有等到接收方的ACK报文，那么将重新发送一遍数据。 连接管理 连接管理是TCP数据传输前和连接断开时的工作， 包括三次握手与四次挥手的过程。 流量控制(滑动窗口) TCP连接发送端和接收端都有一个缓冲区，如果发送端的发送数据过快， 导致接收端来不及处理数据，缓冲区就被填充满了，那么接下来的数据， 接收方就会丢弃数据，导致丢包等连锁反应产生。 TCP根据接收端的处理能力，来决定发送端的发送速度，这个机制就是流量控制。 TCP的报文中，有一个16位的窗口字段， 窗口大小就是接收端计收数据缓冲区的剩余大小，这个数字越大， 代表接收端缓冲区的剩余空间越大。 接收端在发送ACK确认报文时，会将自己当前的窗口大小填入， 这样发送方就会根据ACK报文里的窗口大小的值改变自己的发送速度。 如果接收方窗口大小的值为0，那么发送方将停止发送数据， 并定期的向接收端发送窗口探测数据，让接收端把窗口大小告诉发送端。 拥塞控制(拥塞窗口) 滑动窗口是接收端的使用的窗口大小，用来告诉发送端接收端的缓存大小， 从而可以控制发送端的发送速度。 而发送端发送的速度则是使用发送端的窗口来实现的。 发送端的窗口就是拥塞窗口了，发送端的拥塞窗口不代表缓存， 而是指发送端每次最多可以发送的数据包数。 当TCP连接建立时，拥塞窗口被初始化为1，每次发送数据后， 收到一个ACK，拥塞窗口就增加一个报文段， 发送端取拥塞窗口与滑动窗口的最小值作为发送上限，从而实现拥塞控制。 如果网络比较拥堵，那么一次性发送大量数据将，可能产生大量的丢包， 继而发生一系列的连锁反应，如超时重传等。 拥塞控制就避免了一次性发送过多的数据，而导致的问题。 为什么需要三次握手? TCP是面向连接的，三次握手是客户端与服务端进行数据传输前的准备工作。 这样做是为了建立可靠的传输信道，尽可能的保证数据的安全。 TCP三次握手的过程 三次握手是指建立一个TCP连接时需要客户端和服务端总共需要发送3个数据包确认连接的建立。 TCP三次握手的过程: 1.第一次握手 客户端发送一个数据包发送给服务端。 该数据包的标志位SYN=1，表示客户端请求建立连接， 随机产生的序列号seq=J。 客户端进入SYN_SENT状态，等待服务端确认。 2.第二次握手 服务端收到客户端的数据包后，由标志位SYN=1判断客户端需要建立连接。 于是响应一个确认数据包给客户端。 该确认数据包的标志位SYN和标志位ACK都为1，确认序列号ack=J+1, 随机产生的序列号seq=K。服务端进入SYN_RCVD状态。 3.第三次握手 客户端收到服务端的确认数据包后，检查确认序列号ack是否为J+1, 标志位ACK是否为1。 如果正确则将发送最后一个数据包给服务端。 该数据包的标志位ACK为1,确认序列号ack=K+1。 服务端收到后检查确认序列号ack是否为K+1,标志位ACK是否为1。 如果正确，则客户端和服务端都进入ESTABLISHED状态。 三次握手后，客户端和服务端就可以传输数据了。 为什么需要四次挥手? 四次挥手是客户端与服务端关闭连接时的结尾工作。 TCP是全双工的，即:客户端可以通过这条TCP连接向服务端发送数据(上传)， 服务端也可以通过这条TCP连接向客户端发送数据(下载)。 因此，客户端和服务端都需要单独的关闭连接。 客户端关闭连接是关客户端到服务端的通信传输， 而服务端关闭连接是关闭服务端到客户端的通信传输， 所以需要四次挥手来保证2端的关闭。 TCP四次挥手的过程 四次挥手是指断开连接时，客户端与服务端总共需要发送4个数据包确认连接的断开。 客户端或服务端任意一方都可以发送断开连接的请求。 TCP四次挥手的过程: 1.第一次挥手(假设客户端请求断开) 客户端发送一个数据包给服务端，用于关闭Client到Server的数据传输。 数据包的标志位FIN=1，随机产生的序号seq=M，客户端进入FIN_WAIT_1状态。 2.第二次挥手 服务端收到客户端的断开请求后，将发送一个数据包响应给客户端。 该数据包的标志位ACK=1，确认序列号ack=M+1。 客户端收到后进入FIN_WAIT_2状态，服务端进入CLOSE_WAIT状态。 3.第三次挥手 服务端发送一个数据包给客户端，用于关闭服务端到客户端的数据传输。 数据包的标志位FIN=1，随机产生的序列号seq=N。 服务端进入LAST_ACK状态。 4.第四次挥手 客户端收到服务端的断开请求后，就可以关闭连接了。 于是发送最后一个数据包结束与服务端的连接，并进入TIME_WAIT状态。 最后一个数据包的标志位ACK=1，确认序列号ack=N+1。 服务端收到最后一个了数据包后，就关闭了连接，状态就为CLOSED， 如果服务端没有收到ACK,客户端可以重传。客户端等了一会儿， 最终没有收到响应，就代表服务端已经关闭连接了，客户端也就会关闭连接。 上面是客户端或服务端中的一段主动关闭，另一段被动关闭， 实际还可能还会出现同时发起关闭的情况: TCP粘包和半包问题 在TCP传输数据时，客户端发送数据，实际上是把数据写入到了TCP的缓冲区中， 粘包和半包也就可能在此时产生。 假设客户端给服务端发送两条数据: \"ABC\"和\"DEF\"， 服务端这边的接受可能会有多种情况: 可能是一次性收到了这两条消息:\"ABCDEF\", 也有可能分批收到了消息:\"ABC\",\"DEF\"或\"AB\",\"CD\",\"EF\"。 服务端一次性收到了所有数据包，这种情况就是粘包。 服务端分批收到数据包，这种情况就是半包。 如果客户端发送的包的大小比TCP的缓冲区要小， 并且TCP的缓冲区可以存放多个包，客户端一次性就可能向服务端发送多个包， 这时服务端从TCP缓冲区中就可能读取多个包，这种现象就叫粘包。 如果客户端发送的包的大小比TCP缓冲区要大， 那么这个数据包就可能被分为多个包，就需要多次发送， 而服务端第一次从缓冲区里获取的数据只是整个数据包的一部分，这时候就产生了半包。 粘包的主要原因是: 发送端发送的数据大小 半包的主要原因是: 发送端发送的数据大小 > Socket缓冲区大小， 服务端读取数据不够及时，只读取到了数据的一部分。 总结起来就是:多次发送可能共用一个传输，一个发送可能多占用多个传输。 其实归根到底，究其根本原因是: TCP是面向字节流的协议，消息之间没有边界。 而UDP虽然也可以一次性传输多个包或者多次传输一个包 但UDP的每个消息都是有边界的，因此不会有粘包和半包问题。 解决TCP消息无边界的办法主要有以下几种: 固定长度:这种方式是为消息设定一个固定长度。虽然实现简单，但缺点很大，如果消息的大小本身就比较小， 那么这样做就很浪费空间了。 分隔符:这种方式是为消息边界添加分隔符。这样做实现也是比较简单，也不再浪费空间， 不过当内容本身也有分割符时，那就需要转义了，就可能需要对整个内容进行扫描，效率上就比较低。 添加数据的长度字段:这种方式是为报文添加一个Length字段，存储消息的长度。 在Http协议的报文中，有一个字段为Content-Length，专门存储数据的长度,这种方式是比较好的。 "},"gitbook_doc/computer_network/HTTP.html":{"url":"gitbook_doc/computer_network/HTTP.html","title":"HTTP","keywords":"","body":" HTTP HTTP协议的特点 Cookie和Session的区别 没有Cookie是否依然能实现Session? 重定向和转发的区别 GET和POST区别 常见HTTP状态码 301与302的区别 HTTP请求方法 URI,URL,URN HTTP HTTP协议的特点 适用于客户端与服务端的通信架构: 如果一个应用使用了HTTP协议，那么肯定要有一端作为客户端， 另一段作为服务端，请求由客户端发出，服务端处理并返回请求的结果。 无状态: 无状态是指HTTP协议不对请求和响应之间的通信状态进行保存， 对于发送过的请求或响应的结果都不做持久化处理。 但无状态就意味着每个请求都是独立的，后续的请求如果需要用到前面使用过的信息/数据则需要重传， 这可能导致后续连接的数据较大。 于是Cookie和Session就应运而生,Cookie和Session可以保存用户的状态信息或数据。 无连接: 无连接是指每次TCP连接只处理一个请求，服务端处理完客户端的请求后， 立刻断开连接。采用这种方式，可以节省传输时间。 但是这样做的缺点也很明显，每个TCP连接只处理一个请求， 这样做不仅浪费资源，而且每次请求都需要建立连接，这就使得请求的效率也很低。 从HTTP/1.1开始支持Keep-Alive功能， Keep-Alive使得客户端与服务端的连接持续有效， 当客户端对服务端有多个请求时，都会使用这一条连接。 当超过Keep-Alive规定的时间，或者出现其他意外的情况时， 客户端与服务端的连接才会被断开。 Cookie和Session的区别 Cookie是在客户端保存用户信息的一种机制，用于记录用户的一些状态信息。 每一次客户端向服务端发出请求，都会携带Cookie信息。 Session是在服务端保存用户信息的一种机制。 一个Session对应一个客户端，当客户端第一次请求服务端时， 客户端就会被分配一个唯一SessionId，该SessionId依赖于Cookie，被存储在客户端。 因为SessionId采用Cookie保存，所以每次客户端请求服务端， 也都会携带该SessionId，这样，服务端就能根据SessionId识别客户端身份。 没有Cookie是否依然能实现Session? 个人认为是可以的。 Session将SessionId保存在客户端的Cookie中， 而Cookie不过是客户端保存请求状态的一种机制而已， 使用其他技术当然也可以实现这种机制， 如LocalStorage/SessionStorage也可以存储。 重定向和转发的区别 转发是服务端行为,客户端是无法感知转发的。重定向则是客户端行为。 转发属于一次请求，而重定向是客户端需要发送第二次请求。 GET和POST区别 GET和POST都是HTTP请求的一种方式，而HTTP协议是基于TCP协议的， 所以GET和POST都是基于TCP协议的。。-_- GET请求的语义是从服务器获取资源，无论获取多少次资源， 数据可能不同，但是并不会对服务器资源造成任何影响。 所以我认为GET请求是无副作用的，是幂等的，是可缓存的。 POST请求的语义是在服务器上创建资源，是会产生副作用的。 相同的2次POST请求会创建两份资源，因此POST请求是非幂等的，是不可缓存的。 其他方面，个人认为没啥不同。 只是对于不同的浏览器，GET请求和POST请求在这些浏览器上的表现也不同。 如GET请求和POST请求的URL长度不同,但是具体的长度，恐怕还得视浏览器而定吧。 常见HTTP状态码 HTTP状态码: 1** : 指示信息。服务端收到请求，继续执行操作。 2** : 成功。操作被成功接受并处理。 3** : 重定向。需要进一步的操作来完成请求。 4** : 客户端错误。由于客户端请求的错误，服务端无法处理请求。 5** : 服务端错误。服务端在处理请求的过程中发生了错误。 301与302的区别 301和302都代表客户端请求的资源发生了转移。 不同之处在于: 301: 客户端请求的资源已被永久移动到了新的位置，客户端会自动重定向新URL。 客户端以后的请求应该都使用新的URL。 302: 客户端请求的资源被临时移动到了新的位置，客户端可以继续使用原URL。 HTTP请求方法 GET POST PUT DELETE HEAD PATCH OPTION TRACE CONNECT URI,URL,URN URI ( uniform resource identifer):统一资源标识符 URI是一个互联网资源的唯一标识，但它并不代表资源在互联网上的位置， 它对于资源的作用相当于身份证号码对于我们的作用，起一个唯一标识的作用。 URL (uniform resource locator) : 统一资源定位符 URL是URI的子集，URL标识了资源在互联网上的位置， 一个URL只能访问到一个资源， URN(uniform resource name):统一资源名称 URN也是URI的子集，URN是对资源的命名，不能像URL一样对资源定位。 所以URL有着和URI相同的作用:都可以作为资源的唯一标识符，但URL也有URI没有的作用:对资源的定位。 "},"gitbook_doc/rdbms-learning/RDBMS简介.html":{"url":"gitbook_doc/rdbms-learning/RDBMS简介.html","title":"RDBMS简介","keywords":"","body":" RDBMS(Relational Database Manager System) 什么是RDBMS? 什么是关系型数据库? 关系型数据库(SQL)与非关系型数据库(NoSQL)的区别 RDBMS(Relational Database Manager System) 什么是RDBMS? 关系型数据库管理系统。 关系型数据库管理系统是管理关系型数据库的软件系统。 什么是关系型数据库? 关系型数据库是指采用了关系模型来组织数据的数据库。 关系型数据库以行和列的形式来存储数据，以便于用户理解。 关系型数据库一系列的行和列的集合被称为数据表，而数据库则由一组数据表构成。 关系型数据库(SQL)与非关系型数据库(NoSQL)的区别 关系型数据库与非关系型数据库主要有以下区别: 存储结构 关系型数据库按照结构化的方式存储数据，需要先定义好数据库表的字段，再存储数据。 这样做的好处就是可靠性比较高，但是如果后期应用需要功能，需要扩展表的话，会有些受限。 非关系型数据库存储的结构则不像关系型数据库那样固定，相对来说较为灵活， 可以根据数据内容调整数据库的结构。 存储方式 关系型数据库大多都使用行和列这样的表格关系存储数据。 非关系型数据库存储数据的方式是不固定的，有的采用K-V及键值对存储， 有的采用文档存储，还有的图数据库使用图结构存储。 SQL标准 关系型数据库采用结构化的语言SQL来对数据库进行操作，并且SQL已成为大多数关系型数据库的标准规范。 非关系型数据库则各自为战，一直没有一个统一的标准，每种厂商提供的数据库规范都不一样。 读写性能 关系型数据库强调数据的一致性，所以在遇到高并发读写操作时，会显得力不从心。 非关系型数据库强调BASE理论: Basically Available(基本可用), Soft-state(软状态), Eventual Consistency(最终一致性)， 它允许一定程度的数据不一致，但保证数据的最终一致性。 因此，面对高并发读写操作时，表现的会比关系型数据库好的多， 这也是redis,memcache这类高性能的NoSQL数据库被用于缓存的主要原因。 "},"gitbook_doc/rdbms-learning/RDBMS常见知识点.html":{"url":"gitbook_doc/rdbms-learning/RDBMS常见知识点.html","title":"RDBMS常见知识点","keywords":"","body":" RDBMS常见知识点 键是什么? 超键 / 候选键 / 主键 / 外键？ 联合主键(复合主键) DDL,DML,DCL,DQL,TCL DROP , TRUNCATE , DELETE区别 数据库设计范式(Normal Form)? 视图是什么? 视图与表有什么不同? 视图的优点 视图的缺点 存储过程是什么? 存储过程的优点 存储过程的缺点 触发器是什么? 临时表 连接 RDBMS常见知识点 键是什么? 键是描述数据表中某些特殊的属性列。 假设有一个学生表和一个班级表: 学生表有: 学号，身份证号，性别等属性 班级表有: 班级id，班级名等属性. 超键 / 候选键 / 主键 / 外键？ 超键: 超键是能唯一标识数据表中的记录的属性集的集合。 超键可以是一个属性，也可以是属性组合。 如学生表中的学号可以作为学生的唯一标识， 身份证号也可以作为学生的唯一标识,那么与这2个属性任意搭配的集合， 都可以作为学生表的超键: {[学号]，[身份证号],[学号,身份证号]，[学号，性别]，[身份证号，性别]...}等等。 候选键: 候选键是能唯一标识数据表中的记录的属性的集合。 可以把候选键看作是最小粒度的超键，即没有冗余的超键。 学生表中的候选键可以为:{[学号]，[身份证号]}。 但是不能为{[学号，身份证号]}，因为学号和身份证号同时存在就不是最小粒度了。 主键: 主键是能唯一标识数据表中的记录的属性列。 看起来主键和候选键区别不大。 但是候选键可以是一个集合：{[学号],[身份证号]}， 而主键是从候选键里选出一个属性出来，要么是{[学号]}，要么是{[身份证号]}。(不考虑联合主键哈) 外键: 外键是约束一个数据表和另一个数据表表的关系的属性列。 如何确定学生在哪个班级呢?或者说如何确定某个班级有哪些学生呢? 一个班级可以有多个学生.这就属于一对多关系。 可以在学生表中加一个班级id的外键列，与班级表的班级id关联， 这样就可以确定学生与班级的关系了。 联合主键(复合主键) 联合主键是以多个属性列共同组成的主键， 它们以组合的形式保证数据的唯一性。 DDL,DML,DCL,DQL,TCL DDL(Data Define Language:数据定义语言):DDL的功能是用于定义数据库中的模式(Schema)， 模式包含了表，视图，存储过程等集合。 这里准确来说应该是定义数据库的三级模式:外模式，模式(逻辑模式，概念模式)和内模式。 DDL的关键字有: CREATE , ALTER , DROP等。 DML(Data Manipulation Language:数据操纵语言):DML的功能是用于操作数据库中的数据。 DML的关键字有: SELECT , DELETE , UPDATE , INSERT等。 DCL(Data Control Language:数据控制语言): DCL的功能是用于设置数据库的用户权限。 DCL的关键字有: GRANT,REVOKE等。 TCL(Transaction Control Language:事务控制语言): TCL的功能是用于管理事务。 TCL的关键字有: BEGIN , COMMIT , ROLLBACK等。 DQL(Data Query Language:数据查询语言): 数据查询语言，它属于DML。 DQL的主要关键字就是SELECT了。 DROP , TRUNCATE , DELETE区别 DROP: 删除表(包括表的所有索引和数据)或数据库。DROP属于DDL，操作不可回滚。 TRUNCATE: 删除表中的所有数据，如果有字段是自增的，那么该字段将重新从0开始。 TRUNCATE属于DDL，操作不可回滚。 DELETE: 删除表中指定的数据，如果没有指定条件，将删除所有数据。DELETE属于DML，操作可回滚。 数据库设计范式(Normal Form)? 第一范式: 所有字段都是不可分解的原子属性。 假设在一张用户表中有一个字段为地址。 但地址其实就不是一个原子属性， 因为地址可以分成: 国家 + 省份 + 城市 + 区域 + 街道等属性， 所以需要以最小粒度为单位设置表的属性。比如年龄，它仅仅代表年龄，并不能再分割。 第二范式: 第二范式在第一范式的基础上，所有属性必须依赖于主键，不能依赖部分主键，即一个表中只能存储一种数据， 不能把多种数据都存入一张表中。 一个表必须所有的非主键属性都必须依赖于主键，而且不能只依赖部分主键， 假设有一张学生教师表，字段为: 学号，学生姓名 ，教师编号，教师姓名。 学号(PK) 学生姓名 教师编号(PK) 教师姓名　 1 　guang19 1 qsjz 其中学号与教师编号为联合主键，学生姓名依赖于学号，而并不依赖于教师编号，教师姓名也只依赖于教师编号， 不依赖于学号，所以学生表不符合第二范式。 正确的做法是将学生教师表拆分为2张表:学生表和教师表， 并添加一张中间表来建立学生与老师的关系(一对一或一对多)。 学生表的字段为:学号，学生姓名。 教师表的字段为:教师编号，教师姓名。 学生表: 学号(PK) 学生姓名 1 　guang19 教师表: 教师编号(PK) 教师姓名 1 　qsjz 中间表: 学号(PK) 教师编号 1 1 第三范式: 第三范式在第二范式的基础上，属性不能间接依赖于主键，必须直接依赖于主键。 假设有一张学生表的字段为: 学号，学生姓名，班级编号，班级名称。 学号(PK) 学生姓名 班级编号 班级名称　 1 　guang19 1 qsjz5 其中学号为主键，但是班级名称依赖于班级编号，班级编号再依赖于学号。 这样班级名称与学号之间并不是直接性依赖，而是传递性依赖。 可以将学生表拆分为:学生表和班级表，其中: 学生表的字段为: 学号，学生姓名，班级编号。 班级表的字段为:班级编号 ， 班级名称。 学生表: 学号(PK) 学生姓名 班级编号 1 　guang19 1 班级表: 班级编号(PK) 班级名称 1 　qsjz5 学生表的班级编号与班级表的班级编号就属于一种外键关系了， 2张表中也不再有传递依赖性的依赖关系了。 视图是什么? 视图是一种展示数据表特定结果的虚拟表。 它主要用来做查询结果集的展示。 视图与表有什么不同? 数据表是存在物理文件中的，而于视图是虚拟存在的内存表。 它实际上是一条编译好的Select SQL语句，没有任何数据。 对视图的增删改查等操作实际上是对视图的基表的操作。 视图的优点 视图的优点主要在于可以定制用户数据，让用户关注于其自定义数据本身，而不用理会无关的数据。 视图的缺点 视图的主要缺点在于修改限制。 虽然视图主要是用来做结果集的查询展示的， 但是仍然可以对视图做出修改的操作(实际上是对基表做出修改)。 如果一个视图由多张基表的结果集组成，那么修改操作会变得很麻烦。 存储过程是什么? 当用户使用DML SQL语句操作数据库时，数据库需要编译SQL后再执行。 而存储过程则是用户为了完成特定的任务， 而预先在数据库中定义好并经过数据库编译后的一组SQL， 用户通过指定存储过程的名字和参数来调用存储过程。 存储过程类似于编程语言中的方法函数。 存储过程的优点 存储过程的优点主要在于它允许我们像编写函数一样定义SQL语句，非常的灵活。 并且存储过程是直接经过预编译直接存储在内存之中的，执行速度是很快的。 存储过程的缺点 存储过程的缺点主要在于耗费数据库的资源，如果存储过程过多，那么占用的内存也是越多的。 有的文章说存储过程的可移植性差，但应用程序的技术选型不应该是预先敲定的吗? 触发器是什么? 触发器是在指定表上，当满足指定条件时会执行的SQL语句的集合。 可以把触发器看做是特殊的存储过程，不过存储过程需要手动调用， 而触发器则是满足指定条件就会被触发。 触发器的触发事件有: UPDATE,DELETE,INSERT 触发时间有:Before,After 触发器缺点同存储过程一样。 临时表 临时表与普通数据表是相似的，不过临时表只在当前的连接之中有效， 当连接断开时，临时表就被销毁了。 因此，临时表可以保存一些临时的数据。 使用SHOW TABLES命令是无法查看临时表的。 连接 连接分为:内连接，外连接和交叉连接。 内连接:内连接只返回连接表的匹配的数据。 外连接:外连接分为:左外连接，右外连接和全外连接。 左外连接:左外连接会返回左表所有的行，即使右表中没有匹配的行。 右外连接:右外连接返回右表所有的行，即使左表中没有匹配的行。 全外连接:全外连接返回所有表的所有行，即使表的数据不匹配。 交叉连接:交叉连接又称为笛卡尔积，它不同于其他连接，交叉连接无需指定条件。 它将一张表的每一行与另一张表的所有行全部匹配一遍，效率非常低，而且使用交叉连接的情况较少。 "},"gitbook_doc/rdbms-learning/索引.html":{"url":"gitbook_doc/rdbms-learning/索引.html","title":"索引","keywords":"","body":" 索引 什么是索引? 索引的优点 索引的缺点 B树和B 树区别 Hash索引 索引类型 主键索引(Primary Key) 二级索引(辅助索引) 聚集索引与非聚集索引 聚集索引 聚集索引的优点 聚集索引的缺点 非聚集索引 非聚集索引的优点 非聚集索引的缺点 非聚集索引一定回表查询吗(覆盖索引)? 覆盖索引 索引创建原则 单列索引 联合索引(多列索引) 最左前缀原则 索引创建注意点 最左前缀原则 选择合适的字段 不合适的字段 尽可能的考虑建立联合索引而不是单列索引 考虑在字符串类型的字段上使用前缀索引代替普通索引 使用索引一定能提高查询性能吗? 索引 什么是索引? 索引是一种用于快速查询和检索数据的数据结构。 常见的索引结构有: B树， B+树和Hash。 索引的作用就相当于目录的作用。 打个比方: 我们在查字典的时候，如果没有目录， 那我们就只能一页一页的去找我们需要查的那个字，速度很慢。 如果有目录了，我们只需要先去目录里查找字所在的页数，然后直接翻到那一页就行了。 索引的优点 索引最大的优点就是数据的检索效率高，这也是创建和使用索引的原因。 毕竟大部分系统的读请求总是大于写请求的。 索引的缺点 创建索引和维护索引需要耗费许多时间 : 当对表中的数据进行增删改的时候，如果数据有索引， 那么索引也需要动态的修改，会降低SQL执行效率。 占用物理存储空间 : 索引需要使用物理文件存储，也会耗费一定空间。 B树和B+树区别 B树的所有节点既存放 键(key) 也存放 数据(data); 而B+树只有叶子节点存放 key 和 data，其他的内节点只存放key。 B树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。 B树的检索的过程相当于对范围内的每个节点的关键字做二分查找， 可能还没有到达叶子节点，检索就结束了。 而B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程， 且叶子节点的顺序检索很明显。 Hash索引 Hash索引定位快 Hash索引指的就是Hash表，最大的优点就是能够在很短的时间内， 根据Hash函数定位到数据所在的位置，这是B+树所不能比的。 Hash冲突 知道HashMap或HashTable的同学，相信都知道它们最大的缺点就是Hash冲突了。 不过对于数据库来说这还不算最大的缺点。 Hash索引不支持顺序和范围查询(Hash索引不支持顺序和范围查询是它最大的缺点。) 试想一种情况: SELECT * FROM tb1 WHERE id B+树是有序的，在这种范围查询中，优势非常大， 直接遍历比500小的叶子节点就够了。 而Hash索引是根据hash算法来定位的，可能还要把 1 - 499的数据， 每个都进行一次hash计算来定位，就算不这样做，也需要全表扫描吧。 索引类型 主键索引(Primary Key) 数据表的主键列使用的就是主键索引。 一张数据表有只能有一个主键，并且主键不能为null，不能重复。 在mysql的InnoDB的表中，当没有显示的指定表的主键时， InnoDB会自动先检查表中是否有唯一索引的字段。 如果有，则选择该字段为默认的主键，否则InnoDB将会自动创建一个6Byte的自增主键。 二级索引(辅助索引) 二级索引又称为辅助索引，是因为二级索引的叶子节点存储着主键。 也就是说，通过二级索引，可以定位主键的位置。 唯一索引，普通索引，前缀索引等索引属于二级索引。 PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。 唯一索引(Unique Key): 唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据， 但是允许数据为NULL，一张表允许创建多个唯一索引。 建立唯一索引的目的通常都是为了该属性列的数据的唯一性，而不是为了查询效率。 普通索引(Index):普通索引的唯一作用就是为了快速查询数据。 一张表允许创建多个普通索引，并允许数据重复和NULL。 前缀索引(Prefix):前缀索引只适用于字符串类型的数据。 前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小因为只取前几个字符。 全文索引(Full Text):全文索引主要是为了检索大文本数据中的关键字的信息， 是目前搜索引擎数据库使用的一种技术。 Mysql5.6之前只有MYISAM引擎支持全文索引，5.6之后InnoDB也支持了全文索引。 二级索引: 聚集索引与非聚集索引 聚集索引 聚集索引即索引结构和数据一起存放的索引。 主键索引属于聚集索引。 在Mysql中，InnoDB引擎的表的.ibd文件就包含了该表的索引和数据， 对于InnoDB引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引， 叶子节点存储索引和索引对应的数据。 聚集索引的优点 聚集索引的查询速度非常的快，因为整个B+树本身就是一颗多叉平衡树， 叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。 聚集索引的缺点 依赖于有序的数据 因为B+树是多路平衡树，如果索引的数据不是有序的， 那么就需要在插入时排序，如果数据是整型还好， 否则类似于字符串或UUID这种又长又难比较的数据，插入或查找的速度肯定比较慢。 更新代价大 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。 非聚集索引 非聚集索引即索引结构和数据分开存放的索引。 二级索引属于非聚集索引。 MYISAM引擎的表的.MYI文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD文件的数据。 非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键。 非聚集索引的优点 更新代价比聚集索引要小：非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的 非聚集索引的缺点 跟聚集索引一样，非聚集索引也依赖于有序的数据 可能会二次查询(回表)：这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。 这是Mysql的表的文件截图: 聚集索引和非聚集索引: 非聚集索引一定回表查询吗(覆盖索引)? 非聚集索引不一定回表查询。 试想一种情况，用户准备使用SQL查询用户名，而用户名字段正好建立了索引。 SELECT name FROM table WHERE name='guang19'; 那么这个索引的key本身就是name，查到对应的name直接返回就行了，无需回表查询。 即使是MYISAM也是这样，虽然MYISAM的主键索引确实需要回表，因为它的主键索引的叶子节点存放的是指针。 但是如果SQL查的就是主键呢? SELECT id FROM table WHERE id=1; 主键索引本身的key就是主键，查到返回就行了。 这种情况就称之为覆盖索引了。 覆盖索引 覆盖索引即需要查询的字段正好是索引的字段，那么查找到该索引的字段就可以返回了，而无需回表查询。 如主键索引，如果一条SQL需要查询主键，那么正好根据主键索引就可以查到主键。 再如普通索引，如果一条SQL需要查询name，name字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。 覆盖索引: 索引创建原则 单列索引 单列索引即由一列属性组成的索引。 联合索引(多列索引) 联合索引即由多列属性组成索引。 最左前缀原则 假设创建的联合索引由三个字段组成: ALTER TABLE table ADD INDEX index_name (num,name,age) 那么当查询的条件为: num 或 (num AND name) 或 (num AND name AND age)时，索引才生效。 所以在创建联合索引时，尽量把查询最频繁的那个字段作为索引的最左(第一个)字段。 查询的时候也尽量以这个字段为第一条件。 但可能由于版本原因(我的mysql版本为8.0.x),我创建的联合索引， 相当于在联合索引的每个字段上都创建了相同的索引: 无论是否符合最左前缀原则，每个字段的索引都生效: 索引创建注意点 最左前缀原则 虽然我目前的Mysql版本较高，好像不遵守最左前缀原则，索引也会生效。 但是我们仍应遵守最左前缀原则。 选择合适的字段 不为NULL的字段: 索引字段的数据应该尽量不为NULL，因为对于数据为NULL的字段，数据库较难优化。 如果字段频繁被查询，但又避免不了为NULL，建议使用0,1,true,false 这样语义较为清晰的短值或短字符作为替代。 被频繁查询的字段: 我们创建索引的字段应该是查询操作非常频繁的字段。 被作为条件查询的字段: 被作为WHERE条件查询的字段，应该被考虑建立索引。 被经常频繁用于连接的字段: 经常用于连接的字段可能是一些外键列，外键列并不是一定要建立外键， 只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。 不合适的字段 被频繁更新的字段应该慎重建立索引: 虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。 不被经常查询的字段没有必要建立索引 尽可能的考虑建立联合索引而不是单列索引 因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗B+树。 如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后， 索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。 如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间， 且修改数据的操作效率也会提升。 考虑在字符串类型的字段上使用前缀索引代替普通索引 前缀索引仅限于字符串类型，较普通索引会占用更小的空间， 所以可以考虑使用前缀索引带替普通索引。 使用索引一定能提高查询性能吗? 大多数情况下，索引查询都是比全表扫描要快的。 但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。 "},"gitbook_doc/rdbms-learning/Mysql简介.html":{"url":"gitbook_doc/rdbms-learning/Mysql简介.html","title":"Mysql简介","keywords":"","body":"Mysql Mysql是什么? Mysql是一个关系型数据库管理系统，由瑞典Mysql公司开发，现属于Oracle旗下产品。 Mysql是最流行的关系型数据库之一，与它同类型的数据库还有SQL Server，Oracle，DB2等。 Mysql官方文档 Mysql架构 Mysql总体架构可分为三层:应用层,服务层，存储引擎层: 应用层: 应用层是Mysql体系最上面的一层，接受客户端的连接请求。 应用层包含:连接处理，用户认证，安全管理等内容。 连接处理: 当客户端向Mysql服务端发送一个请求后，Mysql 服务会从线程池中分配一个线程来对客户端连接进行处理。 用户认证: 当客户端请求连接后，服务端会根据用户名，密码等信息判断用户身份。 安全管理(权限): 当客户端连接到Mysql服务端后，Mysql服务会验证用户的权限，以便限制用户的操作。 服务层: 服务层用于处理客户端的操作，是Mysql的核心模块，主要包括:SQL解析器，优化器和缓存等。 SQL解析器: sql解析器用于解析SQL语句，如果SQL有错误，则会提示相应的错误信息，如果没有错，则执行SQL优化。 如果SQL是查询语句，那么首先会去缓存里查询，如果查询成功，就直接返回结果， 不进行接下来的SQL优化。 优化器: 优化器用于优化SQL语句，使得SQL的执行更加高效率。 缓存: 缓存SQL查询的结果，提高SQL查询的效率。 存储引擎层: 存储引擎是Mysql数据持久化的核心。 Mysql是数据库肯定要存储数据，存储数据就要与磁盘文件交互了。 并且Mysql是关系型数据库，核心由表和索引组成，存储引擎还要负责表与索引的创建与持久化。 "},"gitbook_doc/rdbms-learning/Mysql存储引擎.html":{"url":"gitbook_doc/rdbms-learning/Mysql存储引擎.html","title":"Mysql存储引擎","keywords":"","body":" Mysql存储引擎 InnoDB MyISAM MRG_MyISAM MEMORY BLACKHOLE CSV ARCHIVE FEDERATED PERFORMANCE_SCHEMA Mysql存储引擎 使用: SHOW ENGINES 命令可以查看Mysql所有的存储引擎。 InnoDB InnoDB是Mysql最常用的存储引擎之一。 InnoDB主要特点如下: 采用聚集索引: InnoDB采用聚集索引(B+树)，即数据和索引存放在一个文件(.ibd)中。 自适应Hash: 网上有的资料说InnoDB引擎支持Hash，也有的说不支持。 Hash索引的查询(定位)效率是很高的，只是范围和有序查询是它的硬伤，抛开这点， Hash索引的优势还是很大的。InnoDB会自适应优化，如果判断建立Hash索引确实能够提高效率， 那么InnoDB将自己建立相关索引。 参考: 58沈剑前辈的文章 支持事务: 在Mysql的所有引擎中，只有InnoDB支持事务，这也是它被广泛采用的原因之一。 既支持行锁也支持表锁 支持缓存: InnoDB支持缓存，既能缓存数据，也能缓存索引。 支持外键 支持全文索引: Mysql 5.6开始，InnoDB也支持全文索引。 MyISAM MyISAM也是Mysql最常用的引擎之一。 MyISAM主要特点如下: 采用非聚集索引: MyISAM引擎采用的是非聚集索引(B+树)，即索引和数据分开存放， 索引的叶子节点存放指向数据的指针。.MYD文件存放数据, .MYI文件存放索引。 不支持事务: MyISAM引擎不支持事务，可以说这是它最大的缺点了。 只支持表锁: 相比InnoDB，MyISAM只支持表锁，所以在并发方面，MyISAM可能没有InnoDB那么出色 不支持外键 支持全文索引 保存表的行数: MyISAM保存表的行数，而InnoDB不保存， 所以使用 SELECT COUNT(*)查询，MyISAM比InnoDB要快。 MRG_MyISAM MRG_MyISAM引擎也称为MERGE引擎。 它实际上是一系列相同的MyISAM表的集合，创建MERGE表时，必须指定成员表。 对MERGE表的操作，就是对成员表的操作，对成员表的操作，也会反馈到MERGE表中。 假如对MERGE进行INSERT，其实是INSERT到MERGE表的成员表上， 对成员表进行INSERT，结果也会体现到MERGE表上。 MERGE引擎要求merge的表必须都是MyISAM引擎， 且这些表必须拥有相同数据类型的属性列，相同顺序的索引， 但是也允许每个表对应列和索引的名称不同。 MEMORY MEMORY内存引擎。 MEMORY主要特点如下: 不支持持久化机制: MEMORY表的数据是存放在内存中的，且MEMORY不支持持久化机制。 也就是说一旦MYSQL服务器发生宕机等故障，那么数据是很可能丢失的。 所以MEMORY适用于临时数据表。 只支持行锁，不支持表锁 默认采用Hash索引，但支持B树索引: HASH索引执行条件SQL非常快，但如果是范围查询，则需要全表扫描。 BLACKHOLE BLACKHOLE 黑洞引擎。 它的主要特点如下: 不存储数据: 如它的名字一样:黑洞。 写入的数据都会被吞噬掉，只进不出， 不会存储数据，只有一个.sdi(.frm)文件，但是它会记录binlog。 分担主库压力: BLACKHOLE不存储数据就没有用了吗?当然不是。 在普通的主从架构下，slave节点直接连接master节点进行binlog同步压力是比较大的， 可以让BLACKHOLE引擎的数据库节点与master进行搭配， 再让其他slave节点与BLACKHOLE节点进行同步，这样就降低了master的压力。 CSV CSV文件存储引擎。 CSV主要特点如下: CSV引擎的表不支持主键和其他普通索引 所有字段必须为NOT NULL: CSV引擎的表要求所有字段必须非空。 逗号分隔列数据: CSV引擎如它的名字一样，以.csv文件存储数据，且列数据之间以逗号分割。 关于CSV文件还可以参考:CSV文件 ARCHIVE ARCHIVE归档引擎，主要解决了冷数据存储问题。 ARCHIVE主要有以下特点: 占用空间小: 同样的数据，ARCHIVE引擎表占用的空间要比InnoDB小80%左右。 不支持删除和修改: ARCHIVE引擎的表不允许UPDATE和DELETE操作。 支持行锁 不支持索引: ARCHIVE引擎的表支持主键和自增，但是不支持普通索引。 FEDERATED FEDERATED是一种特殊的存储引擎，在我的版本中(8.0.x)默认是不开启的。 FEDERATED引擎使得我们可以访问远程Mysql服务器中数据库的数据。 FEDERATED类型的表由两部分组成: 远程表: 远程表的引擎可以是Mysql支持的任何类型，如:InnoDB,MyISAM等。 本地存放的.frm文件，.frm文件是对表的定义，包含了指向远程表的字符串。 FEDERATED引擎的本地表并不存放数据，当对本地表进行操作的时候，实际上是操作远程表。 PERFORMANCE_SCHEMA PERFORMANCE_SCHEMA系统存储引擎，它是Mysql的一个特性, 主要用来收集Mysql数据库在运行时的性能， 具体参考:Mysql:PERFORMANCE_SCHEMA文档 "},"gitbook_doc/rdbms-learning/事务.html":{"url":"gitbook_doc/rdbms-learning/事务.html","title":"事务","keywords":"","body":" 事务 什么是事务? 事务的四大特性(ACID) DBMS中实现事务持久性的子系统是什么 事务的隔离级别 事务隔离级别引发的问题 快照读 事务 事务并不是Mysql独有的，只是因为某些知识点以Mysql为示例， 所以便把它放在了Mysql之后。 什么是事务? 事务是一组原子性的操作序列，操作执行成功的结果使数据库从一种状态变成另一种状态， 且状态是持久的。 事务的四大特性(ACID) 原子性(Atomicity): 原子性指事务是一组不可分割的操作，要么全部成功， 要么全部失败，不会出现一个操作成功，一个操作失败的情况。 一致性(Consistency): 一致性指事务对数据库操作前后，数据的完整性没有被破坏。 如用户A给用户B转账，无论转账是否成功，A和B的财产总和是不变的。 隔离性(Isolation): 隔离性指事务与事务之间是相互独立的，互不干扰的， 一个事务的失败，不会影响另一个事务。 持久性(Durability): 持久性指事务对数据库的操作产生的影响应该是持久的，不会因为外部原因而发生改变。 DBMS中实现事务持久性的子系统是什么? 原子性：事务是一组不可分割的操作单元，这组单元要么同时成功要么同时失败（由DBMS的事务管理子系统来实现）； 一致性：事务前后的数据完整性要保持一致（由DBMS的完整性子系统执行测试任务）； 隔离性: 多个用户的事务之间不要相互影响，要相互隔离（由DBMS的并发控制子系统实现）； 持久性:一个事务一旦提交，那么它对数据库产生的影响就是永久的不可逆的，如果后面再回滚或者出异常，都不会影响已提交的事务（由DBMS的恢复管理子系统实现的） 事务的隔离级别 读未提交(READ UNCOMMITTED): 一个事务读取另一个事务未提交的数据。 如果事务的隔离级别处于读未提交，那么可能产生脏读，换读和不可重复读等现象。 读已提交(READ COMMITTED): 一个事务读取另一个事务已提交的数据，可以避免脏读， 但是又可能会出现不可重复读和幻读。 可重复读(REPEATABLE READ): 一个事务没有结束时，它对表中的同一条记录读取的结果都是相同的， 也就避免了不可重复读，但还有可能发生幻读问题。 串行化(SERIALIZABLE): 串行化是最高的事务隔离级别，完全满足ACID四个特性。 同一时间，数据库只能执行一个事务，事务之间完全互不干扰，这样就防止了脏读，幻读和不可重复读等问题。 PS:但是串行化相当于串行模式，效率是很低的，所以请慎用。 事务的隔离级别 能否解决:脏读 能否解决:不可重复读 能否解决:幻读 RU × × × RC √ × × RR √ √ × S √ √ √ 事务隔离级别引发的问题 脏读: 脏读在事务隔离级别为读未提交的时候会出现。脏读指读取到了一个事务还没有提交的数据。 假设某张表的一个字段为money，有一条记录money的值为100。 一个事务A对数据做了修改: money=120，但没有提交。 此时，另一个事务B读取到了A修改但没有提交的数据 money=120，然后事务A回滚了， 使得money仍然为原值100，那么事务B读到的money=120就是脏数据。 使用2个mysql客户端窗口模拟多线程事务导致的脏读: 设置事务隔离级别为读已提交可以避免脏读发生。 SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED; 读已提交: 可以看到，虽然读已提交解决了脏读，但是又出现了不可重复读问题。 不可重复读: 不可重复读在事务隔离级别为读已提交时会出现。 不可重复读指一个事务还没有结束时，多次读取同一条数据，可能读取的结果不一样。 假设某张表的一个字段为money，有一条记录money值为100。 一个事务A读取了这条记录为100，此时， 另一个事务B对这条记录做出了修改money=120,并提交了。 当A再次读取money的时候，money为120 。 设置事务隔离级别为可重复读可以避免不可重复读的问题发生。 SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; 可重复读: 虽然可重复读避免了不可重复读问题，但幻读问题也随之而来。 在说幻读之前,得先讲讲MVCC(Multi-Versioned Concurrency Control)机制。 简单理解:这个机制保证了在某个时间，多个事务读取的是数据库的一个快照版本。 关于MVCC机制可以看这篇文章:MVCC 再来说幻读: 幻读: 了解了MVCC机制之后，再看幻读会觉得豁然开朗。 幻读在隔离级别为可重复读的时候会出现。 可重复读虽然避免了不可重复读的问题，但是它也使得当前事务无法感知其他事务的操作。 假设事务A查询了一张表为空表，此时，另一个事务B插入了一条记录并提交了， 当事务A也要插入记录的时候，却发生了主键冲突的错误，可当事务A再次查询的时候，仍然是空表呀。 幻读: 快照读 解决幻读之前先看看快照读： 快照读就是幻读。 个人理解幻读(快照读)是MVCC机制出现的一种问题。 先来看看另一种情况: 当前读 为什么上面处于可重复读级别的第一个事务在第一次读取之后，感知不到第二个事务的修改操作。 因为在事务一开启后，事务会选择第一次SELECT(事务开启后的第一次SELECT)的结果作为快照， 此后的读取，都是读取的这个快照版本，即使其他事务更新了数据， 当前事务也认为这个快照是最新版本，所以读取的仍然是快照版本，数据当然一致。 这就揭露了，可重复读实际上是快照读。 那为什么会发生上面当前读的情况? 因为第一个事务开启后，并没有立刻SELECT，也就是在事务一内并不存在快照版本， 那么事务一就会选择记录的最新版本作为结果。 最新的版本是什么时候? 是第二个事务修改数据并提交后，记录的版本也随之修改了，所以第一个事务能够感知到第二个事务的修改操作。 关于 ，还是和MVCC机制有关，这部分内容比较复杂，所以建议各位同学仔细阅读。 避免幻读的有2种方式:串行化和加锁。 看看加锁是如何解决幻读的: "},"gitbook_doc/rdbms-learning/数据库锁.html":{"url":"gitbook_doc/rdbms-learning/数据库锁.html","title":"数据库锁","keywords":"","body":" 数据库锁 数据库锁机制是什么 乐观锁和悲观锁 共享锁和独占锁(排他锁) 行锁，表锁，页锁 行锁基于索引 数据库锁 数据库锁机制是什么 在编程语言中，有线程同步机制保证多线程对共享资源进行访问时的安全问题。 在数据库中，也有事务锁同步机制，保证事务的一致性。 乐观锁和悲观锁 锁从宏观上分为乐观锁和悲观锁。 乐观锁: 乐观锁一般是用户实现的一种锁机制。乐观锁机制对与共享数据很乐观， 认为任何时候都不会发生线程安全或事务一致性问题，所以不会给数据加锁。乐观锁适用于读多写少的应用。 悲观锁: 数据库锁基本都属于悲观锁。悲观锁认为在任何环境下，数据都有可能发生共享一致性问题， 所以每次读写数据的时候都会加锁。 共享锁和独占锁(排他锁) 按锁的操作可划分为共享锁和独占锁。 共享锁: 共享锁允许多个事务同时访问同一份共享数据。 读锁就属于共享锁，多个事务可以同时对一份数据进行读操作。 独占锁: 独占锁保证了在任何时候，只能有一个事务获取锁，其他事务只能等待获取锁的事务释放锁。 写锁属于独占锁,一份数据同时只能被一个事务增删改。 行锁，表锁，页锁 锁按粒度来划分，还可以分成行锁，表锁，页锁。 行锁: 行锁是最小粒度的一种锁，只针对当前事务操作的行加锁，即在当前事务范围内， 其他事务无法对当前事务操作的行进行修改，事务产生冲突的概率很小。 行锁的加锁粒度最小，并发性高，但它的开销大，加锁慢，而且可能会出现死锁。 表锁:表锁是粒度最大的一种锁，它对整张表加锁，即当在当前事务范围内， 其他事务无法对表做出修改，所以事务产生冲突的概率较大。 表锁的粒度最大，并发性低，但它的实现较为简单，加锁很快，不会发生死锁。 页锁: 页锁是锁定粒度介于行锁和表锁中间的一种锁,并发性一般，可能会出现死锁。 行锁基于索引 在InnoDB中，行锁是基于索引实现的。 如果当前事务的操作没有基于索引，那么InnoDB将使用表锁。 这就意味着如果你使用一个SQL操作一条记录，但是这个条SQL的条件列并没有建立索引， 那么这条SQL即使操作一条记录，使用的也是表锁。 行锁升级为表锁的一种情况: "},"gitbook_doc/rdbms-learning/Mysql日志.html":{"url":"gitbook_doc/rdbms-learning/Mysql日志.html","title":"Mysql日志","keywords":"","body":" Mysql Log日志 binlog redolog undolog Mysql Log日志 Mysql中有几种重要的日志文件，Mysql需要依赖这些日志文件完成非常重要的功能 binlog 二进制日志可以说是Mysql最重要的日志，它记录了所有的DDL和DML(SELECT和SHOW不会记录)语句。 binlog的作用主要在于恢复记录和复制记录。 redolog 重做日志，它记录了事务的操作。 它的作用是确保事务的持久性，防止因为外部原因，数据没来得及写入磁盘而丢失。 在Mysql启动时，会根据redolog重新执行事务操作，从而达到事务持久性。 redolog在事务开启后就产生了，在事务的执行过程中，就被写入了redolog。 redolog日志大小是固定的，当事务执行完后，数据落盘后，redolog就会被覆盖掉。 undolog 回滚日志,它记录了事务开始前数据的一个版本状态，如果事务在记录到redolog时， 就出现了意外，就可以将数据回滚到事务之前的这个状态。 "},"gitbook_doc/rdbms-learning/Mysql优化.html":{"url":"gitbook_doc/rdbms-learning/Mysql优化.html","title":"Mysql优化","keywords":"","body":" Mysql优化 Explain执行计划 开启慢查询日志 为什么要求字段尽量设置为NOT NULL? 为什么主键尽量设置成有序和整型? Mysql优化 Explain执行计划 Explain是Mysql的优化神器，它可以对一条Query SQL进行分析， 并输出SQL的详细信息，以便于优化SQL。 下面是Explain解析一条SELECT的例子: EXPLAIN输出的属性如下: id: id是SELECT查询的序列号，表示一条SELECT或SELECT中子查询的执行顺序。主要有2种情况: 如果id相同，可以被认为是同一分组，执行顺序，从上到下。 如果id不同，id值越大，那么越先执行。 select_type: select_type是为了区分简单查询，复杂查询，子查询等查询类型。 select_type主要有如下值: SIMPLE: SIMPLE代表简单查询，SELECT中不包含子查询或联合查询等复杂查询。 SUBQUERY: SUBQUERY表示子查询部分。 PRIMARY: PRIMARY表示最外层查询，如果一个SELECT中包含子查询， 那么最外层的查询就是PRIMARY。 DERIVED: DERIVED代表查询的结果被放入临时表中。 如FROM 语句后跟着一个 SELECT ，那么SELECT的结果就可能被放入临时表中。 UNION: UNION代表联合查询，也就是UNION后的SELECT。 UNION RESULT: UNION RESULT代表联合查询的结果。 table: table表示查询的表。 partitions: partition代表查询匹配的分区，对于非分区表为NULL。 type: 查询类型，是衡量SQL的一个重要的指标。 一般来说，至少要达到range标准，最好达到ref。 它有如下值： SYSTEM: SYSTEM代表表只有一条记录，他是CONST类型的一个特例。 CONST: CONST表示表只最多只有一个匹配行，只用读一次， 因此优化器可以视查询结果为一个常量。 EQ_REF: EQ_REF代表唯一性索引扫描。比如以 unique key索引或主键作为条件， 这2种索引都是唯一的，表中只有一条记录与之匹配。 REF: REF代表非唯一索引匹配，返回匹配条件的所有行。 比如WHERE条件的列有索引，经过匹配后，发现有多条符合条件的记录。 RANGE: RANGE表示范围扫描。如使用>, INDEX: INDEX代表全表扫描，不过它与ALL的区别在于， INDEX是在索引列上进行全表扫描，一般要比ALL快。 ALL: ALL代表全表扫描，效率最低。 possible_keys: poosible keys代表查询涉及到的字段如果有索引会被列出来， 但不一定使用。 key: key代表查询实际使用到的索引，如果为NULL，则没有使用索引。 key_len: key_len表示使用索引的实际长度。 ref: ref表示与索引比较的列或这常量。 假设 WHERE以主键作为条件 id = 10,那么ref显示的就是const， 因为主键为10，就是一个常量。 rows: rows代表查询优化器估算查询结果集要扫描的行数， 也就是执行SELECT预计需要扫描的行数。 filtered: filtered代表SELECT执行时过滤掉的记录与rows的百分比。 filtered 显示的百分比 与 rows 的乘积就是查询时过滤掉的行数。 EXTRA: 显示SQL执行时的附加信息 开启慢查询日志 慢查询用于记录查询超过某个临界值的SQL。 使用: SHOW VARIABLES LIKE 'SLOW_QUERY_LOG'; 查看慢查询是否开启，如果显示OFF，那么执行: set GLOBAL SLOW_QUERY_LOG = on 开启慢查询日志。 使用: SHOW VARIABLES LIKE 'LONG_QUERY_TIME'; 查看慢查询临界值,单位: 秒。 使用: SET LONG_QUERY_TIME=2 设置慢查询临界值。 开启慢查询后，会在mysql的数据目录下产生xxx-slow.log慢查询日志文件， 一旦某条SQL超过了临界值，就会被记录到慢查询日志文件中。 为什么要求字段尽量设置为NOT NULL? 因为Mysql难以优化可以为空的字段，一旦字段被建立了索引， 那么就需要额外的空间来存储此当前行记录是否为NULL的标识。 为什么主键尽量设置成有序和整型? 这个问题其实在索引章节就说过了，索引的结构是B+树，B+树最大的优点就是顺序查找， 有序的主键当然有利于查找，像UUID等这种长字符串，连排序都难以解决。 "},"gitbook_doc/rdbms-learning/Mysql数据类型.html":{"url":"gitbook_doc/rdbms-learning/Mysql数据类型.html","title":"Mysql数据类型","keywords":"","body":" Mysql数据类型 UTF8和UTF8MB4 CHAR 和 VARCHAR TIMESTAMP 和 DATETIME 如何在TIMESTAMP和DATETIME之中选择? Mysql数据类型 UTF8和UTF8MB4 在mysql中，UTF8编码最多能存放3个字节的数据，也就是说几乎可以存放所有的中文汉字了。 但是如果遇到像emoji这种特殊的字符，那UTF8就不够用了。 在mysql5.5.3版本后，mysql加入了一种新的字符: UTF8MB4,他是UTF8的超集， 最多可以支持4个字节的数据，所以解决了UTF8兼容性的问题。 CHAR 和 VARCHAR CHAR是固定长度的字符串,VARCHAR是可变的字符串。 mysql5.0版本以上，CHAR(M)和VARCHAR(M)的长度M不再指字节长度， 而单纯的就是指长度。 如CHAR(1)和VARCHAR(1), 既可以存放1个中文字符，也可以存放1个英文字符，只不过它们底层的字节长度不同。 一个中文字符约等于3个字节，所以CHAR(1)如果存放一个中文字符，那么mysql底层的字节长度将为3。 (VARCHAR的实际最大长度还得看表的字符编码,UTF8最大只支持3个字节，UTF8MB4支持4个字节) 它们的主要区别如下: 长度上限不同: CHAR长度的上限为255个字节(约85个汉字), VARCHAR长度的上限为65535个字节(约21845个汉字)(mysql 5.0 以上)，和text相同。 占用空间不同: 使用CHAR 作为类型的话，无论该字段字符的长度是否小于指定长度，都会占用指定长度的空间。 而VARCHAR则是根据字符的长度来决定占用的空间，这就是可变长字符串的好处。 是否删除字符串末尾的空字符串: 如果字符串末尾有空格字符，CHAR则会删除其末尾的空格， 而VARCHAR不会删除字符串末尾的空格。 效率问题: 由于VARCHAR是可变长的，如果对字符串进行更改的时候， 那么也会修改其相应的空间，这在效率上就没有CHAR那么高效了。 TIMESTAMP 和 DATETIME TIMESTAMP和DATETIME都是用于表示时间的类型。 它们主要有如下区别: 占用空间不同: TIMESTAMP占用4个字节；DATETIME占用8个字节,因此TIMESTAMP更加节省空间。 时间范围不同: TIMESTAMP表示 1970 - 2038年；DATETIME表示: 1000 - 9999 年。 是否受时区影响: TIMESTAMP的时间会受当前Mysql的时区影响； 而DATETIME则不受影响，存进去什么时间，拿出来就什么时间。 默认值不同: TIMESTAMP字段的值如果为NULL，那么将会自动更新到当前时间； DATETIME存进去NULL，就默认为NULL。 如何在TIMESTAMP和DATETIME之中选择? 如果你想让一个时间字段表示的范围更大，且不受系统时区影响，那么建议选择DATETIME。 其他情况考虑TIMESTAMP吧。 "},"gitbook_doc/nosql-learning/NoSQL简介.html":{"url":"gitbook_doc/nosql-learning/NoSQL简介.html","title":"NoSQL简介","keywords":"","body":" NoSQL(Not-only SQL) 什么是NoSQL数据库? 为什么要有NoSQL数据库? 非关系型数据库与关系型数据库的区别 有哪些类型的NoSQL数据库? NoSQL(Not-only SQL) 什么是NoSQL数据库? NoSQL数据库泛指非关系型数据库，与关系型数据库不同，非关系型数据库并没有一种固定的存储数据的结构， 相对来说比较灵活。 为什么要有NoSQL数据库? 非关系型数据库与关系型数据库是一种相辅相成的关系，起到了互补的作用。 关系型数据库的数据看上去很直观且支持事务，保证了数据的一致性。 非关系型数据库读写速度块，在高并发的压力下仍有不俗的表现且数据结构丰富， 更多的是对关系型数据库的一种补充。 非关系型数据库与关系型数据库的区别 存储结构 关系型数据库按照结构化的方式存储数据，需要先定义好数据库表的字段， 再存储数据。这样做的好处就是可靠性比较高，但是如果后期应用扩展功能， 需要扩展表的话，会有些受限。 非关系型数据库存储的结构则不像关系型数据库那样固定，相对来说较为灵活， 可以根据数据调整数据库的结构。 存储方式 关系型数据库大多采用行和列这样的表格关系存储数据。 非关系型数据库存储数据的方式则不固定，有的采用K-V键值对存储， 有的采用文档存储，还有的图数据库使用图结构存储。 SQL标准 关系型数据库采用结构化的语言SQL来对数据库进行操作，并且SQL已成为大多数数据库的标准规范。 非关系型数据库则各自为战，一直没有一个统一的标准，每种厂商提供的数据库规范都不一样。 读写速度 关系型数据库强调数据的一致性，所以在遇到高并发读写操作时，会显得力不从心。 非关系型数据库强调BASE理论: Basically Available(基本可用),Soft-state(软状态),Eventual Consistency(最终一致性)。 它允许一定程度的数据不一致，但保证数据的最终一致性。 因此，面对高并发读写操作时，表现的会比关系型数据库好的多， 这也是redis,memcached这类高性能的NoSQL数据库被用于缓存的主要原因。 有哪些类型的NoSQL数据库? K-V键值对: K-V键值对类型的NoSQL数据库类似于Hash表，将数据存储在内存中， 操作速度非常的快，因此常被用于缓存数据库。K-V键值对类型的NoSQL数据库主要有:Memcached,Redis等。 文档: 文档类型的NoSQL数据库结构则不固定，无需像关系型数据库一样预先定义字段，它存储数据的方式类似于JSON， 可以清晰的描述数据之间的复杂关系。文档类型的NoSQL数据库主要有:MongoDB,CouchDB等。 列式存储: 列式存储的NoSQL数据库以列簇形式存储数据，将同一列的数据存储在一起， 这样可以分割为多列，查询速度是很快的，但是列式存储的数据库功能也会收到限制。 列式NoSQL数据库主要有:HBase等; 图结构: 图数据库主要用于构建节点的关系图谱，以图算法和图结构进行计算和存储。 图数据库主要有:Neo4j等。 "},"gitbook_doc/nosql-learning/Redis简介.html":{"url":"gitbook_doc/nosql-learning/Redis简介.html","title":"Redis简介","keywords":"","body":"Redis 推荐一个学习Redis的网站: RedisBook 什么是Redis? Redis是一个使用ANSI C语言编写，遵守BSD协议规范的开源的K-V类型的NoSQL数据库服务器。 Redis是当前最流行的K-V类型的NoSQL数据库之一，在通往系统架构的方向，它是我们不得不学习的知识。 Redis官网 "},"gitbook_doc/nosql-learning/Redis常见知识点.html":{"url":"gitbook_doc/nosql-learning/Redis常见知识点.html","title":"Redis常见知识点","keywords":"","body":" Redis常见知识点 Redis优缺点 Redis为什么这么快? Redis应用场景 为什么不用Map或Guava做缓存? Redis和Memcached异同 Redis IO模型 Redis 6 之前 Redis 6 之后 Redis常见知识点 Redis优缺点 Redis的优点: 性能高: 用c语言编写的应用我就没见过慢的~_~。 丰富的数据结构: 总体上来说Redis是以K-V形式存储数据，但是细分来说， 它支持STRING，HASH，LIST，SET，SORTED_SET，HyperLogLog等多种数据结构。 支持Lua脚本: Redis使用Lua脚本解释器来执行脚本，所以它支持Lua脚本。 支持事务: Redis是为数不多的支持事务的NoSQL数据库之一。 支持数据持久化: Redis支持rdb和aof两种数据持久化方式。 支持发布者/订阅者功能 支持主从模式: Redis支持Sentinel哨兵模式搭建高可用集群配置。 Redis的缺点: 受限于物理内存: Redis属于内存数据库，它在内存中存储数据的大小是受物理内存限制的， 所以它不适合存储海量数据。 Redis为什么这么快? 内存操作: Redis绝大部分操作都是基于内存的，想不快都难。 优秀的数据结构: Redis虽然支持的数据结构众多，但是它的每种数据结构都是专门设计和优化过的。 IO多路复用: Redis整体采用IO多路复用模型，核心操作使用单线程处理。 Redis应用场景 缓存: 缓存可能是Redis用的最多的场景了。由于Redis的高性能， 高并发场景下作为缓存服务数据库，再适合不过了。 并且Redis支持的key自动过期功能，更是可以定制热点数据的过期时间。 多功能业务场景: Redis支持多种丰富的数据结构，不仅可以存储简单的K-V数据，还可以使用Hash存储用户，商品等信息， List存储有序的数据，Set还有交集，并集，差集等功能。 分布式锁: Redis的操作具有原子性的，可以利用这点来完成分布式锁。 为什么不用Map或Guava做缓存? 因为无论是Map还是Guava，都属于本地缓存，数据都存在一个JVM进程内的。 如果是单机模式，这样做尚可。但如果是分布式或者Java应用有多个实例，那就不能保证每个JVM进程内的缓存是一致的， 所以需要使用Redis这种第三方数据库作为缓存容器。 Redis和Memcached异同 都属于内存数据库 持久化支持: Redis支持RDB和AOF两种持久化机制； Memcached只在内存中存储数据，不支持持久化机制。 数据结构: Redis从整体上来说是以K-V类型的为存储结构， 但它细分可以支持String，Hash，List，Set，Sorted Set等数据类型； Memcached只支持K-V类型的存储结构。 IO模型: Redis是以IO多路复用为模型的设计； Memached是以非阻塞IO为模型的设计。 事件库: Redis采用自制的AeEven事件库处理Socket事件; Memcached采用的是LibEvent事件库。 使用场景: 不考虑性能，Redis更适用于需要复杂数据结构，需要持久化的应用， 如果你的应用以后需要扩展，那么也可以选择Redis； Memcached则适用于高并发和只需要K-V数据结构的应用。 Redis IO模型 Redis IO模型按Redis的版本可以分为Redis 6之前和Redis 6之后。 Redis 6 之前 Redis是基于IO多路复用模型处理Socket请求的，关于多路复用的知识，我在 Linux五种IO模型中 已经说过了: IO多路复用模型依赖于操作系统的select/poll/epoll函数， epoll函数使得内核不断轮询客户端socket， 用户进程(线程)也需要阻塞在对epoll函数的调用上，当Socket有事件时， 用户线程便发起系统调用，处理Socket事件。 IO多路复用模型简单理解就是一个线程处理多个Socket连接。 所以可以把Redis看成是单线程模型，但并不是说Redis只有一个线程， 而是说它执行核心操作的线程只有一个，它还有其他辅助线程完成其它功能。 Redis这样设计就避免了多线程切换的开销和简化了Redis的设计。 Redis IO模型: Redis 6 之后 Redis 6 之前一个线程处理所有的Socket和核心操作， 这样做的好处就是单线程无需考虑像多线程切换带来的困扰， 简化了Redis的模型，但随之而来的也有性能上的瓶颈。 虽然Redis确实够快，但它的数据是在内存中操作的，会受到内存的限制。 且一个线程处理所有的Socket会带来网络IO的限制，并不能发挥多核CPU的优势。 那有什么办法既能够发挥多核cpu的优势，又不会复杂化Redis的架构呢？ Redis 6 之前的瓶颈主要在于内存和网络IO。 关于内存这块，只得靠Redis自身的优化和机器条件了。 但是网络IO这块就可以通过新增多线程来处理大量Socket连接来优化了。 Redis 6 正是通过新增监听线程来解决网络IO的瓶颈，线程监听到Socket事件后， 再交由main线程处理。 所以整体来说，Redis 6 在处理Socket事件上由单线程优化成了多线程，但核心操作还是由单线程执行。 Redis 6 IO模型： "},"gitbook_doc/nosql-learning/Redis数据结构.html":{"url":"gitbook_doc/nosql-learning/Redis数据结构.html","title":"Redis数据结构","keywords":"","body":"Redis数据结构 关于Redis数据结构这块，还是推荐学习: RedisBook Redis有着非常丰富的数据结构，这些数据结构可以满足非常多的应用场景， 如果对这些数据结构有一个比较清晰的认知，使用Redis也会更加得心应手。 Redis主要支持以下数据结构: String(字符串) List(双端链表) Hash(Hash字典) Set(无序集合) Sorted Set(有序集合) 数据结构 描述　 　实现 String 可以存储字符串，整型和浮点型等类型的数据。适合于简单的K-V数据场景。 Redis并没有使用字符数组来实现这一数据类型，而是自己定义了简单动态字符串(SDS: Simple Dynamic String)类型来实现这一数据结构。 List 既可以存储操作有序的数据，还可以当做栈来使用，它适合存储列表性质的数据。 List在Redis中使用的是双端链表和压缩列表实现的，这就解释了它为什么能在头尾操作元素。 C语言并没有双端链表的实现，所以Redis自定义了这一数据结构。 Hash Hash字典，也是关联数组，数组的每个元素都是key到value的映射，它适合存储对象这样的结构化数据。 Hash在Redis中是使用Hash字典和压缩列表实现的。 Set Set无序集合，它适合存储需要去重的元素，且有并集，交集，差集等功能在多个Set之间进行比较计算。 整数集合是Set的底层实现之一，当集合只有整型元素且元素数量不多的时候，Redis就会使用整数集合来实现Set。当新添加元素的时候，整数集合会根据新元素的类型自动扩容，并将所有元素的类型都转为与新元素一样的类型，在这个过程中，还需要保持原来的顺序不变，最后才添加新元素。 Sorted Set Sorted Set有序集合，可以看做加强版的Set。Sorted Set 与 Set不同的是，Sorted Set可以根据元素的score分数进行排序。它适合存储需要排序的不重复的元素。 Sorted Set在Redis中使用的是跳表实现的，跳表是一种有序且查询速度很快的数据结构。跳表每个节点都维持指向其他节点的指针，从而达到快速访问的目的。 "},"gitbook_doc/nosql-learning/Redis事务.html":{"url":"gitbook_doc/nosql-learning/Redis事务.html","title":"Redis事务","keywords":"","body":"Redis事务 Redis的事务主要依赖于WATCH ,UNWATCH,MULTI , EXEC, DISCARD等命令。 其中 MULTI , EXEC , DISCARD 分别对应关系型数据库的 BEGIN,COMMIT,ROLLBACK操作。 Redis事务执行过程 客户端使用MULTI命令开启事务，此时用户就可以开始发出要执行的命令。 如果命令为WATCH/MULTI/EXEC/DISCARD这四个中的任意一个， 那么会被直接执行，因为它们属于事务操作。 当执行DISCARD的时候，会清空事务队列并退出事务。 如果是普通命令，就将命令加入事务队列，然后当EXEC命令执行时， 事务中的队列将会被一一执行，最后执行的结果也是一个数组。 参考: Redis事务的设计与实现 Redis事务队列 在开启事务后，用户命令并不会被立刻执行，而是被添加到事务队列中， 这个队列其实是一个数组，每个数组元素由3部分组成: 要执行的命令(cmd) 命令的参数(argv) 参数的数量(argc) 命令被添加到队列中的结构大致如下: 命令被执行后，也会生成一个结果数组，Redis就将这个结果数组返回: Redis事务错误 Redis事务有两个错误时机。 EXEC执行命令之前出现错误: 在EXEC命令之前的错误，也就是开启事务后，用户发出了错误的命令， 参数数量不对或其他原因，服务端会累积这些错误。 当EXEC命令执行时，将拒绝执行事务，并返回错误原因，清空事务队列。 EXEC执行命令时出现错误: EXEC执行命令时出现错误，也就是用户发出的命令没有错， 但是在执行命令的时候出现了错误(可能是参数类型不对)，这时候仍然返回结果数组, 也就是说错误的命令并不影响其他命令的执行。 "},"gitbook_doc/nosql-learning/Redis缓存淘汰策略.html":{"url":"gitbook_doc/nosql-learning/Redis缓存淘汰策略.html","title":"Redis缓存淘汰策略","keywords":"","body":" Redis缓存淘汰策略(key回收) noeviction volatile allkeys Redis缓存淘汰策略(key回收) 当Redis使用的内存超出物理内存限制时， Redis的内存会和Swap(虚拟内存)频繁切换，造成Redis性能的急剧下降。 为了不让Redis内存占用超过物理内存占用，可以给Redis配置一个 maxmemory 的值， 当Redis占用内存超过了这个maxamemory的值， 那么Redis将启用缓存淘汰策略来删除内存中的key， 缓存淘汰策略可以通过 maxmemory-policy 设置。 缓存淘汰策略主要分为3类: noeviction volatile allkeys noeviction noeviction策略是Redis默认的淘汰策略。 它可以继续接受请求，但只执行读请求，不执行写请求， 这样做可以保证内存中的数据不会被修改和删除。 volatile volatile只淘汰设置了过期时间的key。 volatile有三种算法实现: volatile-lru:根据lru算法淘汰设置了过期时间的key，lru算法优先删除最近最少使用的key。 volatile-ttl:根据key的过期时间的长短 淘汰设置了过期时间的key，过期时间越小的key优先被删除。 volatile-random:随机淘汰设置了过期时间的key。 allkeys allkeys对所有key无差别淘汰 allkeys有两种算法实现: allkeys-lru: 根据lru算法淘汰所有的key，最近最少使用的key优先被删除。 allkeys-random:随机淘汰所有的key。 "},"gitbook_doc/nosql-learning/Redis持久化策略.html":{"url":"gitbook_doc/nosql-learning/Redis持久化策略.html","title":"Redis持久化策略","keywords":"","body":" Redis持久化 RDB(Redis Data Base) RDB优缺点 AOF(Append Only File) AOF优缺点 如何选择持久化策略? Redis持久化 Redis有2种持久化策略: RDB和AOF。 RDB(Redis Data Base) RDB是Redis默认的持久化策略，这种策略是把数据库的快照以二进制形式的副本保存在磁盘上。 RDB持久化触发条件 SAVE命令: 当客户端执行SAVE命令时，会阻塞Redis主线程进行数据持久化，直到持久化完成。Redis在阻塞期间不能处理客户端的请求。 BGSAVE命令: 当客户端执行BGSAVE命令时，Redis会fork一个子进程进行数据持久化，因此并不会阻塞Redis服务。 FLUSHALL命令: 当客户端执行FLUSHALL命令时，会清空Redis所有数据库的数据，并且也会触发数据同步。 save配置: Redis会按照配置文件中的save配置的条件进行数据同步，一旦满足条件，就会执行BGSAVE命令，即fork一个子进程进行同步。 shutdown: 当Redis服务关闭时，也会将数据同步到磁盘，以便下次启动时恢复。 RDB优缺点 RDB的优点: 文件体积小,恢复大数据较快 最大化Redis性能: Redis会fork出子进程进行数据同步，并不影响Redis的性能。 RDB的缺点: 数据安全性较低: 如果不显示的执行SAVE命令，那么Redis隔一段时间才会同步数据，可能会造成一定程度的数据丢失。 AOF(Append Only File) AOF策略是把已经执行过的命令以文本的方式追加到AOF文件的末尾，以此达到记录数据库状态的目的。 AOF默认情况下是关闭的，当配置选项 appendonly 设置为yes后才会进行AOF的持久化。 appendfsync指定了AOF的同步策略，它有三个可选值。 no: no代表Redis不亲自持久化，而是通过系统调用write函数每隔一段时间将数据写入文件。 这种情况下如果服务器发生故障，可能会有数据还没来得及同步就丢失了。 always: always表示Redis每次执行写操作都会将数据同步到文件中。 这种策略虽然保证了数据的安全性，但是对Redis的性能会有影响。 everysec: everysec是AOF默认的持久化策略，这种策略下， 系统每一秒都会将数据写入文件，兼顾了性能和数据安全性。 AOF优缺点 AOF优点: 数据安全性较高,秒级丢失 AOF缺点: 文件体积大,恢复大数据较慢 如何选择持久化策略? 两种持久化方式各有优缺点，可以选择混合的方式进行备份。 混合持久化后，文件的内容大部分都是RDB格式的，恢复起来较快， 以AOF的方式同步也能保证数据的安全性。 "},"gitbook_doc/amqp-learning/AMQP简介.html":{"url":"gitbook_doc/amqp-learning/AMQP简介.html","title":"AMQP简介","keywords":"","body":"AMQP(Advanced Message Queuing Protocol) 错误之处,敬请指教 PS:部分图片源于网络,如有侵权,请联系俺,俺会立刻删除。 什么是AMQP? Advanced Message Queuing Protocol 高级消息队列协议,是面向消息中间件提供的应用层协议。 基于此协议的消息提供者和接受者可实现消息的交互传递,且不受平台语言限制。 什么是消息队列? 消息是指在应用程序间传递的数据,消息可以是普通的文本字符串,也可以是复杂的对象。 消息队列是存储和收发消息的应用程序,它保证了应用程序间消息传递的可靠性。 为什么需要MQ / MQ使用场景? 系统解耦: 系统间通过MQ传递消息,而无需关心其他系统的处理。 流量削峰: MQ可以缓解短时间内的流量高峰,减少对服务器的压力。 日志处理: 复杂的系统架构必然需要处理大量日志,使用MQ可以传输日志。 广播: 在分布式系统中,可以通过MQ将消息广播到各个节点中。 MQ模型 所有的MQ的模型抽象出来都是一样的: 消费者(订阅者)订阅某个消息队列, 生产者(发布者)发布消息到消息队列,最后消息队列将消息发送给消费者。 MQ模型: "},"gitbook_doc/amqp-learning/RabbitMQ常见知识点.html":{"url":"gitbook_doc/amqp-learning/RabbitMQ常见知识点.html","title":"RabbitMQ常见知识点","keywords":"","body":" RabbitMQ 什么是RabbitMQ? RabbitMQ基本概念 RabbitMQ工作模型 RabbitMQ Exchange的类型 如何避免消息被重复消费? Broker重复消费消息 Consumer重复消费消息 解决消息的重复消费 如何保证消息传递的可靠性? Producer消息丢失 Broker消息丢失 Consumer消息丢失 RabbitMQ 什么是RabbitMQ? RabbitMQ是使用Erlang语言编写的支持AMQP协议的高性能消息队列, 目前由Pivotal(维护Spring项目的公司)公司在维护。 RabbitMQ最初起源于金融系统,被用于在分布式系统种存储和转发消息, 具有较好的易用性,可扩展性和并发性等优点。 RabbitMQ基本概念 先来看一张RabbitMQ的结构图: RabbitMQ主要设计以下概念: Message消息: 消息即需要发送的数据,由消息体和消息头组成。 消息体是消息的内容,消息头则是对消息体的描述,由许多Property属性组成, 如路由键,优先级,持久化等等组成。 Producer生产者: 生产者即发布者,它是发布消息的一方,可以是一个RabbitMQ客户端应用程序。 Consumer消费者: 消费者即订阅者,他是接受消息的一方,也可以是一个RabbitMQ客户端应用程序。 Exchange交换机: 交换机用于接受 生产者发布的消息,并按消息的路由规则将消息发送到指定的消息队列。 Binding绑定: 绑定一种规则,它将Exchange交换机与Queue消息队列 按照路由规则绑定起来。 Routing-Key路由键: 路由键即路由规则, Exchange交换机根据Routing Key将消息投递到指定的消息队列。 Queue消息队列: 消息队列是消息容器,用于存储和收发消息。一个消息可以投递到一个或多个消息队列中,直到Consumer将消息消费。 Connection网络连接: 无论是发布消息还是订阅消息,都需要通过TCP连接来完成。 Channel信道: 对于计算机服务器来说,网络是非常宝贵的资源,如果每次发布消息或订阅消息都需要建立连接, 那就太耗费Connection资源了,所以引入了Channel信道的概念。 发布消息和订阅消息通过Channel信道来完成,而多个Channel可以共享一条Connection ,这样就节约了很多Connection资源。 Virtual Host 虚拟主机: 虚拟主机是一个虚拟概念。 在Redis中,一个Redis服务器实例可以被分为16个库,但不是说这16个库的大小就是相等的, 比如1个库也可以占9g内存,其他15个库可以共占1g内存。 虚拟主机也是如此,它可以看做是一个独立的rabbitmq服务器实例, 它包含了属于他的一系列的Exchange,Queue等内容。 Broker: Broker即RabbitMQ服务实例。 RabbitMQ工作模型 PS:个人觉得RabbitMQ官网的文档实例写得挺好的,各位同学可以直接参照官网学习。 RabbitMQ主要有5种工作模型: Simple Queue: 简单模式。它是最简单的消息收发模型,生产者和消费者直接通过Queue进行消息的收发。 Work Queue: 工作模式 。工作模式与简单模式非常相似,但工作模式允许多个消费者争抢的消费同一个队列的消息。 Publisher / Subscriber (fanout): 发布者 / 订阅者模式 。 发布者/订阅者模式可以说是真正意义上的RabbitMQ的工作模型。 发布者不直接将消息发布到Queue,而是将消息发送到Exchange交换机,再由交换机将消息广播到与此交换机绑定的每个队列中,订阅者也只需要监听它自己的队列就行了。 Routing (direct): 路由模式。 发布者在发布消息的时候可以指定一个Routing Key路由键,接受者也会与其消息队列绑定一个路由键。 当消息被发送到Exchange后,Exchange根据路由键,将消息发送到拥有相同路由键的消息队列。 Topic (topic): 主题模式。主题模式是一种特殊的路由模式, 与路由模式不同的是: 主题模式的路由键支持通配符,可以做到更加多样化的路由键匹配。 RabbitMQ Exchange的类型 RabbitMQ共有3种Exchange类型: fanout: fanout即发布者 / 订阅者模式,发布者将消息发送到fanout类型的Exchange,Exchange再将消息广播给与此交换机绑定的Queue。 direct: direct即路由模式,Exchange根据Routing Key,将消息发送到匹配的队列中。 topic: topic也属于路由模式,不过它支持\"*\",\"#\"等通配符进行路由。 如何避免消息被重复消费? 消息被重复消费即消息的发送者发送了多次相同的消息给接受者,从而导致接受者出现业务上的问题。 这是一个非幂等的问题。我们知道在Http协议中,GET请求是幂等的,即多次相同的GET请求并不会对服务器的资源造成影响。 而POST请求或PUT请求(在POST和PUT请求的语义为\"添加\"的情况下)是非幂等的,多次相同的POST请求或PUT请求可能在服务器上创建多个资源。 先看一张图(请原谅我的画图技术 0_0): Broker重复消费消息 对于发布者来说,在它发送消息给MQ服务器后, MQ服务器收到消息后响应一个ACK给发布者代表它收到了发布者的消息,即图中的 1 - 2。 如果MQ响应的ACK由于网络原因丢失了,那么发布者会再次发送消息给服务器,这就造成了服务器收到了多次相同的消息的问题。 Consumer重复消费消息 对于 MQ服务器来说,当它发送消息给 Consumer客户端时, Consumer收到消息后响应ACK给MQ服务器代表它接收到了MQ服务器的消息,即图中的 3 - 4。 如果Consumer响应的ACK由于网络原因丢失了,那么MQ会再次发送消息给Consumer客户端,这也造成了Consumer多次消费相同的消息的问题。 解决消息的重复消费 无论是MQ服务器还是Consumer消费端,它们都需要判断消息的唯一性, 如果判断消息已经接受过了,那么可以选择不再处理相同的消息。 解决办法是可以给消息添加全局唯一的标识,如唯一ID,用于保证消息的全局唯一性。 如何保证消息传递的可靠性? 还是上面的那张图: 消息传递不可靠的情况主要有: Producer丢失消息: 即 1 - 2。 Broker丢失消息: 即消息存储在MQ Broker时丢失。 Consumer丢失消息: 即 3 - 4。 Producer消息丢失 Producer丢失消息有2种解决方案: transaction: 如果消息发送成功,则提交事务。如果在发送消息的过程中出现了异常,那么事务就会回滚。 虽然事务可以保证Producer发送消息的可靠性,但是会降低吞吐量, 所以更推荐使用confirm机制保证Producer发送消息的可靠性。 confirm: 当消息被发送给消息队列后,Broker会响应一个ACK给Producer。 如果Broker无法处理该消息,则返回一个NACK给Producer,Producer可以根据返回的NACK再做处理。 Broker消息丢失 Broker丢失消息一般是MQ服务器宕机或出现其它较为严重的问题, 所以Broker丢失消息的问题,可以通过持久化解决。 Consumer消息丢失 Consumer接受到消息队列的数据后,会自动回复Broker,Broker就会删除这条消息, 但如果Consumer在此时处理消息的过程中,发生了意外,消息就丢失了。 Consumer可以在处理完消息后再手动回复Broker,这样即使发生了意外,Broker也能重新发送消息给Consumer。 "},"gitbook_doc/tomcat/Tomcat.html":{"url":"gitbook_doc/tomcat/Tomcat.html","title":"Tomcat部分","keywords":"","body":" Tomcat常见知识点 什么是Tomcat? Tomcat运行模式有几种? Tomcat架构 Connector连接器 Container容器结构 JSP解析引擎 Tomcat常见知识点 什么是Tomcat? Tomcat是apache开源的一款优秀的Servlet容器, 如果对他的架构和原理有比较清楚的认识,那么会使我们对Java网络编程方面的能力有很大提升. Tomcat运行模式有几种? BIO模式 NIO模式 APR模式 关于BIO和NIO我已经在IO 中讲到过了,不熟悉或忘了的同学可以去看一下，这里比较陌生的就是APR模式了。 APR(Apache Portable Runtime)模式是异步IO模式。 我们知道AIO是我们的用户进程向内核发起系统调用后，直接返回， 不用阻塞在对内核的系统调用上，等内核有数据后才会通知用户进程，这种IO性能是非常高的， APR也是Tomcat应对高并发的首选模式。 但是AIO是需要依赖于操作系统的，Tomcat的APR模式也是需要依赖于它自己的JNI库， 即操作系统必须安装了APR库，Tomcat才能使用APR模式。 至于如何安装APR，这个在网上是由很多文章有写的，各位同学可以自行搜索。 Tomcat架构 在Tomcat中，最顶级的容器是Server，一个Tomcat只有一个Server，它代表着整个Tomcat容器。 一个Server至少包含一个Service，可以包含多个Service，有了Service就可以向外提供服务了。 Service的核心是Container和Connector，每个Service只有一个Container，但可以包含多个Connector。 Tomcat架构图: Tomcat中的Connector连接器叫做Coyote,Container容器叫做Catalina。 连接器的作用是把Socket请求Request转换为ServletRequest对象, 并把这个ServletRequest对象交由Container容器(Catalina)。 Container容器选择对应的Servlet处理,然后返回ServletResponse, 最后由Connector解析为Response,返回给客户端。 Container是Tomcat的Servlet容器,它包含了安全,会话,集群,Servlet实现等各方面的组件, 同时也是Tomcat 的启动入口(Bootstrap),它是Tomcat的核心,其模块都是为Catalina模块 服务的。 一下是Tomcat的执行流程: 除了连接器和Servlet容器部分,Service还包含了Naming提供JNDI命名模块,Juli日志模块 Connector连接器 Tomcat中的连接器叫Coyote,因为连接器更多的是处理Socket协议和Socket流方面的功能, 所以与Servlet的关系不大,Catalina作为Servlet容器,才是处理任务的关键。 Coyote支持的应用层协议有:Http1.1,Http2,AJP(Tomcat与其他Web服务器通信的协议). 它支持的IO模型有:BIO,NIO,APR等。 Coyote由ProtocolHandler和Adapter这2部分组成 ProtocolHandler使用EndPoint监听Socket请求,然后将Socket请求通过Processor转为Request。 Adapter将Request转为ServletRequest对象,交由Catalina处理。 Container容器结构 因为Container是Tomcat的核心,所以需要对它的结构有一个清晰的认知。 在Container之下,有一个Engine,它表示Servlet的引擎,它管理着多个Host容器, 一个Host容器代表一个虚拟主机,也可以说是Host是一个站点。 Host之下又可以包含多个Context容器,一个Context容器代表一个Web程序, 在Context里,就是若干的Wrapper(Servlet) JSP解析引擎 Tomcat中的JSP解析引擎是Jasper,所谓的JSP(JavaServer Page)经常被人说成可以写Java代码的Html, 我觉得这话说反了,应该说是可以写Html的Java类(Servlet)。 最终JSP是会被引擎解析成相应的Servlet类的,这些生成的Servlet都是继承自HttpJspBase, 最终Servlet是会调用_jspService方法把编译好的Html等字符串刷回浏览器。 所以只要不是前后端分离项目,那么像Thymeleaf,Freemarker,Jsp这类模板引擎, 都是通过Java的代码使用Response流(Socket流)把Html写回客户端浏览器的。 "},"gitbook_doc/netty-learning/Netty简介.html":{"url":"gitbook_doc/netty-learning/Netty简介.html","title":"Netty简介","keywords":"","body":"Netty 本文章部分摘抄自 《Netty in Action》(Netty实战)，再根据本人实际学习体验总结而成， 所以本文内容可能不那么全面，但是我尽量挑选Netty中我认为比较重要的部分做讲解。 学习Netty，相信大部分同学都会选择 《Netty in Action》 ， 这里我推荐它的一个Gitbook精髓版本的， 此版本的作者对《Netty in Action》做出了更为精简的概述，所以各位同学可酌情挑选阅读。 Netty in Action(精髓) 其次我认为只看书是不够的，这里我推荐一些关于Netty入门比较优秀的视频供各位同学参考， 推荐视频观看的顺序即下列顺序，各位同学不需要每个视频的每个章节都看，只需要挑选互补的内容学习即可： 韩顺平Netty教程 张龙Netty教程 索南杰夕Netty RPC实现 最后，在学习Netty之前，我们需要对 IO模型(网络IO模型)有一个大概的认知，可以参考我编写的： IO 。 如有错误之处，敬请指教。 Netty是什么? Netty是Red Hat开源的，一个利用Java的高级网络能力，隐藏其(Java API)背后的复杂性而提供一个易于使用的 NIO 客户端/服务端框架。 Netty提供了高性能和可扩展性，让你自由地专注于你真正感兴趣的东西。 Netty简化了网络程序的开发过程，使用它 我们可以快速简单地开发网络应用程序，比如客户端和服务端的通信协议，TCP和UDP的Socket开发。 Netty的特点 Netty作为一款优秀的网络框架，自然有令人折服的特点： 设计： 针对多种传输类型的同一接口。 简单但更强大的线程模型。 真正的无连接的数据报套接字支持。 链接逻辑复用。 性能： Netty的高性能是它被广泛使用的一个重要的原因，我们可能都认为Java不太适合 编写游戏服务端程序，但Netty的到来无疑是消除了这种见解。 较原生Java API有更好的吞吐量，较低的延时。 资源消耗更少(共享池和重用)。 减少内存拷贝。 健壮性： 原生NIO的客户端/服务端程序编写较为麻烦，如果某个地方处理的不好，可能会 导致一些意料之外的异常，如内存溢出，死循环等等，而Netty则为我们简化了原生API 的使用，这使得我们编写出来的程序不那么容易出错。 社区： Netty快速发展的一个重要的原因就是它的社区非常活跃，这也使得采用它的开发者越来越多。 "},"gitbook_doc/netty-learning/Netty组件.html":{"url":"gitbook_doc/netty-learning/Netty组件.html","title":"Netty组件","keywords":"","body":" Netty组件介绍 Bootstrap/ServerBootstrap Channel EventLoop ChannelFuture ChannelHandler ChannelPipeline 入站事件和出站事件的流向 进一步了解ChannelHandler 编码器和解码器 SimpleChannelInboundHandler Netty组件介绍 Netty有 Bootstrap/ServerBootstrap，Channel，EventLoop，ChannelFuture， ChannelHandler，ChannelPipeline，编码器和解码器等核心组件。 在学习Netty组件之前建议各位同学先编写一个Netty的Demo，你不必了解这个Demo的细节， 只需要让它在你的脑海中留下一个记忆，然后对照Demo来学习以下组件，会事半功倍。 Demo Bootstrap/ServerBootstrap Bootstrap和ServerBootstrap是Netty应用程序的引导类，它提供了用于应用程序网络层的配置。 一般的Netty应用程序总是分为客户端和服务端，所以引导分为客户端引导Bootstrap和服务端引导ServerBootstrap， ServerBootstrap作为服务端引导，它将服务端进程绑定到指定的端口，而Bootstrap则是将客户端连接到 指定的远程服务器。 Bootstrap和ServerBootstrap除了职责不同，它们所需的EventLoopGroup的数量也不同， Bootstrap引导客户端只需要一个EventLoopGroup，而ServerBootstrap则需要两个EventLoopGroup。 Channel 在我们使用某种语言，如c/c++,java,go等，进行网络编程的时候，我们通常会使用到Socket， Socket是对底层操作系统网络IO操作(如read,write,bind,connect等)的封装， 因此我们必须去学习Socket才能完成网络编程，而Socket的操作其实是比较复杂的，想要使用好它有一定难度， 所以Netty提供了Channel(io.netty.Channel，而非java nio的Channel)，更加方便我们处理IO事件。 EventLoop EventLoop用于服务端与客户端连接的生命周期中所发生的事件。 EventLoop 与 EventLoopGroup，Channel的关系模型如下： 一个EventLoopGroup通常包含一个或多个EventLoop，一个EventLoop可以处理多个Channel的IO事件， 一个Channel也只会被注册到一个EventLoop上。在EventLoop的生命周期中，它只会和一个Thread线程绑定，这个 EventLoop处理的IO事件都将在与它绑定的Thread内被处理。 ChannelFuture 在Netty中，所有的IO操作都是异步执行的，所以一个操作会立刻返回，但是如何获取操作执行完的结果呢？ Netty就提供了ChannelFuture接口，它的addListener方法会向Channel注册ChannelFutureListener， 以便在某个操作完成时得到通知结果。 ChannelHandler 我们知道Netty是一个款基于事件驱动的网络框架，当特定事件触发时，我们能够按照自定义的逻辑去处理数据。 ChannelHandler则正是用于处理入站和出站数据钩子，它可以处理几乎所有类型的动作，所以ChannelHandler会是 我们开发者更为关注的一个接口。 ChannelHandler主要分为处理入站数据的 ChannelInboundHandler和出站数据的 ChannelOutboundHandler 接口。 Netty以适配器的形式提供了大量默认的 ChannelHandler实现，主要目的是为了简化程序开发的过程，我们只需要 重写我们关注的事件和方法就可以了。 通常我们会以继承的方式使用以下适配器和抽象: ChannelHandlerAdapter ChannelInboundHandlerAdapter ChannelDuplexHandler ChannelOutboundHandlerAdapter ChannelPipeline 上面介绍了ChannelHandler的作用，它使我们更关注于特定事件的数据处理，但如何使我们自定义的 ChannelHandler能够在事件触发时被使用呢？ Netty提供了ChannelPipeline接口，它 提供了存放ChannelHandler链的容器，且ChannelPipeline定义了在这条ChannelHandler链上 管理入站和出站事件流的API。 当一个Channel被初始化时，会使用ChannelInitializer接口的initChannel方法在ChannelPipeline中 添加一组自定义的ChannelHandler。 入站事件和出站事件的流向 从服务端角度来看，如果一个事件的运动方向是从客户端到服务端，那么这个事件是入站的，如果事件运动的方向 是从服务端到客户端，那么这个事件是出站的。 上图是Netty事件入站和出站的大致流向，入站和出站的ChannelHandler可以被安装到一个ChannelPipeline中， 如果一个消息或其他的入站事件被[读取]，那么它会从ChannelPipeline的头部开始流动，并传递给第一个ChannelInboundHandler ，这个ChannelHandler的行为取决于它的具体功能，不一定会修改消息。 在经历过第一个ChannelInboundHandler之后， 消息会被传递给这条ChannelHandler链的下一个ChannelHandler，最终消息会到达ChannelPipeline尾端，消息的读取也就结束了。 数据的出站(消息被[写出])流程与入站是相似的，在出站过程中，消息从ChannelOutboundHandler链的尾端开始流动， 直到到达它的头部为止，在这之后，消息会到达网络传输层进行后续传输。 进一步了解ChannelHandler 鉴于入站操作和出站操作是不同的，可能有同学会疑惑：为什么入站ChannelHandler和出站ChannelHandler的数据 不会窜流呢(为什么入站的数据不会到出站ChannelHandler链中)？ 因为Netty可以区分ChannelInboundHandler和 ChannelOutboundHandler的实现，并确保数据只在两个相同类型的ChannelHandler直接传递，即数据要么在 ChannelInboundHandler链之间流动，要么在ChannelOutboundHandler链之间流动。 当ChannelHandler被添加到ChannelPipeline中后，它会被分配一个ChannelHandlerContext， 它代表了ChannelHandler和ChannelPipeline之间的绑定。 我们可以使用ChannelHandlerContext 获取底层的Channel，但它最主要的作用还是用于写出数据。 编码器和解码器 当我们通过Netty发送(出站)或接收(入站)一个消息时，就会发生一次数据的转换，因为数据在网络中总是通过字节传输的， 所以当数据入站时，Netty会解码数据，即把数据从字节转为为另一种格式(通常是一个Java对象)， 当数据出站时，Netty会编码数据，即把数据从它当前格式转为为字节。 Netty为编码器和解码器提供了不同类型的抽象，这些编码器和解码器其实都是ChannelHandler的实现， 它们的名称通常是ByteToMessageDecoder和MessageToByteEncoder。 对于入站数据来说，解码其实是解码器通过重写ChannelHandler的read事件，然后调用它们自己的 decode方法完成的。 对于出站数据来说，编码则是编码器通过重写ChannelHandler的write事件，然后调用它们自己的 encode方法完成的。 为什么编码器和解码器被设计为ChannelHandler的实现呢? 我觉得这很符合Netty的设计，上面已经介绍过Netty是一个事件驱动的框架，其事件由特定的ChannelHandler 完成，我们从用户的角度看，编码和解码其实是属于应用逻辑的，按照应用逻辑实现自定义的编码器和解码器就是 理所应当的。 SimpleChannelInboundHandler 在我们编写Netty应用程序时，会使用某个ChannelHandler来接受入站消息，非常简单的一种方式 是扩展SimpleChannelInboundHandler，T是我们需要处理消息的类型。 继承SimpleChannelInboundHandler 后，我们只需要重写其中一个或多个方法就可以完成我们的逻辑。 "},"gitbook_doc/netty-learning/Transport传输.html":{"url":"gitbook_doc/netty-learning/Transport传输.html","title":"Transport传输","keywords":"","body":" 传输(Transport) 传输API Netty内置的传输 零拷贝 内存映射（Memory Mapped） 文件传输(SendFile) 传输(Transport) 在网络中传递的数据总是具有相同的类型：字节。 这些字节流动的细节取决于网络传输，它是一个帮我们抽象 底层数据传输机制的概念，我们不需要关心字节流动的细节，只需要确保字节被可靠的接收和发送。 当我们使用Java网络编程时，可能会接触到多种不同的网络IO模型，如NIO，BIO(OIO: Old IO)，AIO等，我们可能因为 使用这些不同的API而遇到问题。 Netty则为这些不同的IO模型实现了一个通用的API，我们使用这个通用的API比直接使用JDK提供的API要 简单的多，且避免了由于使用不同API而带来的问题，大大提高了代码的可读性。 在传输这一部分，我们将主要学习这个通用的API，以及它与JDK之间的对比。 传输API 传输API的核心是Channel(io.netty.Channel，而非java nio的Channel)接口，它被用于所有的IO操作。 Channel结构层次： 每个Channel都会被分配一个ChannelPipeline和ChannelConfig， ChannelConfig包含了该Channel的所有配置，并允许在运行期间更新它们。 ChannelPipeline在上面已经介绍过了，它存储了所有用于处理出站和入站数据的ChannelHandler， 我们可以在运行时根据自己的需求添加或删除ChannelPipeline中的ChannelHandler。 此外，Channel还有以下方法值得留意： 方法名 描述 eventLoop 返回当前Channel注册到的EventLoop pipeline 返回分配给Channel的ChannelPipeline isActive 判断当前Channel是活动的，如果是则返回true。 此处活动的意义依赖于底层的传输，如果底层传输是TCP Socket，那么客户端与服务端保持连接便是活动的；如果底层传输是UDP Datagram，那么Datagram传输被打开就是活动的。 localAddress 返回本地SocketAddress remoteAddress 返回远程的SocketAddress write 将数据写入远程主机，数据将会通过ChannelPipeline传输 flush 将之前写入的数据刷新到底层传输 writeFlush 等同于调用 write 写入数据后再调用 flush 刷新数据 Netty内置的传输 Netty内置了一些开箱即用的传输，我们上面介绍了传输的核心API是Channel，那么这些已经封装好的 传输也是基于Channel的。 Netty内置Channel接口层次： 名称 包 描述 NIO io.netty.channel.socket.nio NIO Channel基于java.nio.channels，其io模型为IO多路复用 Epoll io.netty.channel.epoll Epoll Channel基于操作系统的epoll函数，其io模型为IO多路复用，不过Epoll模型只支持在Linux上的多种特性，比NIO性能更好 KQueue io.netty.channel.kqueue KQueue 与 Epoll 相似，它主要被用于 FreeBSD 系统上，如Mac等 OIO(Old Io) io.netty.channel.socket.oio OIO Channel基于java.net包，其io模型是阻塞的，且此传输被Netty标记为deprecated，故不推荐使用，最好使用NIO / EPOLL / KQUEUE 等传输 Local io.netty.channel.local Local Channel 可以在VM虚拟机内部进行本地通信 Embedded io.netty.channel.embedded Embedded Channel允许在没有真正的网络传输中使用ChannelHandler，可以非常有用的测试ChannelHandler 零拷贝 零拷贝(Zero-Copy)是一种目前只有在使用NIO和Epoll传输时才可使用的特性。 在我之前写过的IO模型中，所有的IO的数据都是从内核复制到用户应用进程，再由用户应用进程处理。 而零拷贝则可以快速地将数据从源文件移动到目标文件，无需经过用户空间。 在学习零拷贝技术之前先学习一下普通的IO拷贝过程吧， 这里举个栗子： 我要使用一个程序将一个目录下的文件复制到另一个目录下， 在普通的IO中，其过程如下： 应用程序启动后，向内核发出read调用（用户态切换到内核态），操作系统收到调用请求后， 会检查文件是否已经缓存过了，如果缓存过了，就将数据从缓冲区（直接内存）拷贝到用户应用进程（内核态切换到用户态）， 如果是第一次访问这个文件，则系统先将数据先拷贝到缓冲区（直接内存），然后CPU将数据从缓冲区拷贝到应用进程内（内核态切换到用户态）， 应用进程收到内核的数据后发起write调用，将数据拷贝到目标文件相关的堆栈内存（用户态切换到内核态）， 最后再从缓存拷贝到目标文件。 根据上面普通拷贝的过程我们知道了其缺点主要有： 用户态与内核态之间的上下文切换次数较多（用户态发送系统调用与内核态将数据拷贝到用户空间）。 拷贝次数较多，每次IO都需要DMA和CPU拷贝。 而零拷贝正是针对普通拷贝的缺点做了很大改进，使得其拷贝速度在处理大数据的时候很是出色。 零拷贝主要有两种实现技术： 内存映射（mmp） 文件传输（sendfile） 可以参照我编写的demo进行接下来的学习： zerocopy 内存映射（Memory Mapped） 内存映射对应JAVA NIO的API为 FileChannel.map。 当用户程序发起 mmp 系统调用后，操作系统会将文件的数据直接映射到内核缓冲区中， 且缓冲区会与用户空间共享这一块内存，这样就无需将数据从内核拷贝到用户空间了，用户程序接着发起write 调用，操作系统直接将内核缓冲区的数据拷贝到目标文件的缓冲区，最后再将数据从缓冲区拷贝到目标文件。 其过程如下： 内存映射由原来的四次拷贝减少到了三次，且拷贝过程都在内核空间，这在很大程度上提高了IO效率。 但是mmp也有缺点： 当我们使用mmp调用映射一个文件到内存后，如果另一个进程同时对这个文件阶段或是做出写的操作， 那么系统如果此时正在将数据write到目标文件，用户程序可能会因为访问非法地址而产生一个错误的信号从而终止。 试想一种情况：我们的服务器接收一个客户端的下载请求，客户端请求的是一个超大的文件，服务端开启一个线程 使用mmp和write将文件拷贝到Socket进行响应，如果此时又有一个客户端请求对这个文件做出修改， 由于这个文件先前已经被第一个线程mmp了，可能第一个线程会因此出现异常，客户端也会请求失败。 解决这个问题的最简单的一种方法就对这个文件加读写锁，当一个线程对这个文件进行读或写时，其他线程不能操作此文件， 不过这样处理并发的能力可能就大打折扣了。 文件传输(SendFile) 文件传输对应JAVA NIO的API为 FileChannel.transferFrom/transferTo 在了解sendfile之前，先来看一下它的函数原型（linux系统的同学可以使用 man sendfile 查看）： #include ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); sendfile在代表输入文件的文件描述符 in_fd 和 输入文件的文件描述符 out_fd 之间传输文件内容， 这个传输过程完全是在内核之中进行的，程序只需要把输入文件的描述符和输出文件的描述符传递给 sendfile调用，系统自然会完成拷贝。 当然，sendfile和mmp一样都有相同的缺点，在传输过程中， 如果有其他进程截断了这个文件的话，用户程序仍然会被终止。 sendfile传输过程如下： 它的拷贝次数与mmp一样，但是无需像mmp一样与用户进程共享内存了。 "},"gitbook_doc/netty-learning/ByteBuf容器.html":{"url":"gitbook_doc/netty-learning/ByteBuf容器.html","title":"ByteBuf容器","keywords":"","body":" ByteBuf--Netty的数据容器 ByteBuf实现 ByteBuf使用模式 字节级别的操作 索引访问 可丢弃字节 可读字节 可写字节 索引管理 查找操作 衍生缓冲区 ByteBufHolder ByteBuf分配 ByteBufAllocator Unpooled ByteBufUtil 引用计数 ByteBuf--Netty的数据容器 网络传输的基本单位是字节，在Java NIO中，JDK提供了Buffer接口，以及其相关的实现作为NIO操作 数据的容器，如ByteBuffer等等。 而Netty为了解决Buffer原生接口的复杂操作提供了ByteBuf， ByteBuf是一个很好的经过优化过的数据容器，我们可以将字节数据添加到ByteBuf中或从ByteBuf中获取数据， 相比于原生Buffer，ByteBuf更加灵活和易用。 Netty的数据处理主要通过两个API提供： abstract class ByteBuf interface ByteBufHolder 使用ByteBuf API能够给我们带来良好的编码体验，如 在读和写这两种模式切换时，无需像ByteBuffer一样调用flip方法。 容量可以动态增长，如StringBuilder之于String。 支持方法的链式调用，如\"a\".append(\"b\").append(\"c\")这种形式。 ... ByteBuf还有很多好处，上面列举的只是一部分，其他优点就需要各位同学慢慢了解了。 ByteBuf实现 ByteBuf维护了两个不同的索引：一个是用于读取的readerIndex ， 一个是用于写入的writerIndex。 当我们写入字节到ByteBuf后，writerIndex增加，开始读取字节后，readerIndex开始增加。读取字节直到 readerIndex和writerIndex到达同一位置（已经读取到末尾了），ByteBuf就变为不可读。 这就好比当我们访问数组时，超出了它的范围时，程序会抛出IndexOutOfBoundException。 当我们调用ByteBuf以read或write开头的方法时，将会增加这个ByteBuf的读索引或写索引，而诸如set或get 的方法则不会改变索引。 我们可以指定ByteBuf的最大容量，如果对ByteBuf的写入操作导致writerIndex超出了最大人容量，那么程序将会 抛出一个异常，ByteBuf的最大人容量是Integer.MAX_VALUE。 ByteBuf大致的结构和状态： ByteBuf使用模式 ByteBuf有多种使用模式，我们可以根据需求构建不同使用模式的ByteBuf。 堆缓冲区(HeapByteBuf)： 最常用的ByteBuf模式是将数据存储在JVM的堆空间中，实际上是通过数组存储数据， 所以这种模式被称为支撑数组（Backing Array ）。所以这种模式被称为支撑数组可以在没有使用池化的情况下快速分配和释放， 适合用于有遗留数据需要处理的情况。 直接缓冲区(DirectByteBuf)： 在Java中，我们创建的对象大部分都是存储在堆区之中的，但这不是绝对的，在NIO的API中， 允许Buffer分配直接内存，即操作系统的内存，这样做的好处非常明显： 前面在传输章节介绍过的零拷贝技术的 特点之一就是规避了多次IO拷贝，而现在数据直接就在直接内存中，而不是在JVM应用进程中，这不仅减少了拷贝次数， 是否还意味着减少了用户态与内核态的上下文切换呢？ 直接缓冲区的缺点也比较明显： 直接内存的分配和释放都较为昂贵，而且因为直接 缓冲区的数据不是在堆区的，所以我们在某些时候可能需要将直接缓冲区的数据先拷贝一个副本到堆区， 再对这个副本进行操作。 与支撑数组相比，直接缓冲区的工作可能更多，所以如果事先知道数据会作为 一个数组来被访问，那么我们应该使用堆内存。 复合缓冲区（CompositeByteBuf）： CompositeByteBuf为多个ByteBuf提供了一个聚合视图， 我们可以根据需要，向CompositeByteBuf中添加或删除ByteBuf实例，所以CompositeByteBuf中可能 同时包含直接缓冲区模式和堆缓冲区模式的ByteBuf。对于CompositeByteBuf的hasArray方法， 如果CompositeByteBuf中只有一个ByteBuf实例，那么CompositeByteBuf的hasArray方法 将直接返回这唯一一个ByteBuf的hasArray方法的结果，否则返回false。 除此之外，CompositeByteBuf还提供了许多附加的功能，可以查看Netty的文档学习。 字节级别的操作 除了普通的读写操作，ByteBuf还提供了修改数据的方法。 索引访问 如数组的索引访问一样，ByteBuf的索引访问也是从零开始的，第一个字节的索引是0,最后一个字节的索引总是 capacity - 1： 注意：使用getByte方式访问，既不会改变readerIndex，也不会改变writerIndex。 JDK的ByteBuffer只有一个索引position，所以当ByteBuffer在读和写模式之间切换时，需要使用flip方法。 而ByteBuf同时具有读索引和写索引，则无需切换模式，在ByteBuf内部，其索引满足： 0 这样的规律，当使用readerIndex读取字节，或使用writerIndex写入字节时，ByteBuf内部的分段大致如下： 上图介绍了在ByteBuf内部大致有3个分段，接下来我们就详细的介绍下这三个分段。 可丢弃字节 上图中，当readerIndex读取一部分字节后，之前读过的字节就属于已读字节，可以被丢弃了，通过调用 ByteBuf的discardReadBytes方法我们可以丢弃这个分段，丢弃这个分段实际上是删除这个分段的已读字节， 然后回收这部分空间： 可读字节 ByteBuf的可读字节分段存储了尚未读取的字节，我们可以使用readBytes等方法来读取这部分数据，如果我们读取 的范围超过了可读字节分段，那么ByteBuf会抛出IndexOutOfBoundsException异常，所以在读取数据之前，我们 需要使用isReadable方法判断是否仍然有可读字节分段。 可写字节 可写字节分段即没有被写入数据的区域，我们可以使用writeBytes等方法向可写字节分段写入数据，如果我们写入 的字节超过了ByteBuf的容量，那么ByteBuf也会抛出IndexOutOfBoundsException异常。 索引管理 我们可以通过markReaderIndex，markWriterIndex方法来标记当前readerIndex和writerIndex的位置， 然后使用resetReaderIndex，resetWriterIndex方法来将readerIndex和writerIndex重置为之前标记过的 位置。 我们还可以使用clear方法来将readerIndex和writerIndex重置为0，但是clear方法并不会清空ByteBuf的 内容，下面clear方法的实现： 其过程是这样的： 由于调用clear后，数据并没有被清空，但整个ByteBuf仍然是可写的，这比discardReadBytes轻量的多， DiscardReadBytes还要回收已读字节空间。 查找操作 在ByteBuf中，有多种可以确定值的索引的方法，最简单的方法是使用ByteBuf的indexOf方法。 较为复杂的查找可以通过ByteBuf的forEachByte方法，forEachByte方法所需的参数是ByteProcessor， 但我们无需去实现ByteProcessor，因为ByteProcessor已经为我们定义好了两个易用的实现。 衍生缓冲区 衍生缓冲区是专门展示ByteBuf内部数据的视图，这种视图常通过以下方法创建： duplicate slice order readSlice 这些方法都将以源ByteBuf创建一个新的ByteBuf视图，所以源ByteBuf内部的索引和数据都与视图一样， 但这也意味着修改了视图的内容，也会修改源ByteBuf的内容。如果我们需要一个真实的ByteBuf的副本， 我们应该使用copy方法来创建，copy方法创建的副本拥有独立的内存，不会影响到源ByteBuf。 ByteBufHolder 从表面理解起来，ByteBufHolder是ByteBuf的持有者，的确没有错。 ByteBuf几乎唯一的作用就是存储 数据，但在实际的数据传输中，除了数据，我们可能还需要存储各种属性值，Http便是一个很好的例子。 除了Http Content，还包括状态码，cookie等等属性，总不能把这些属性与Content存储在一个ByteBuf中吧， 所以Netty提供了ByteBufHolder。ByteBufHolder为Netty提供了高级特性的支持，如缓冲区持化，使得可以 从池中借用ByteBuf，并且在需要的时候自动释放。 以下是ByteBufHolder常见的方法： content: 返回这个ByteBufHolder所持有的ByteBuf。 copy： 返回ByteBufHolder的深拷贝，连它持有的ByteBuf也拷贝。 ByteBuf分配 前面介绍了ByteBuf的一些基本操作和原理，但却并未说明如何分配一个ByteBuf，这里将讲解ByteBuf的分配方式。 ByteBufAllocator 为了减少分配和释放内存的开销，Netty通过 ByteBufAllocator 实现了ByteBuf的池化。以下是ByteBufAllocator 的常见方法。 buffer: 返回一个基于堆或直接内存的ByteBuf，具体取决于实现。 heapBuffer： 返回一个基于堆内存的ByteBuf。 directBuffer： 返回一个基于直接内存的ByteBuf。 compositeBuffer： 返回一个组合ByteBuf。 ioBuffer： 返回一个用于套接字的ByteBuf。 我们可以通过Channel或这ChannelHandlerContext的alloc方法获取到一个ByteBufAllocator Netty提供了两种ByteBufAllocator的实现： PooledByteBufAllocator和UnpooledByteBufAllocator。 PooledByteBufAllocator池化了ByteBuf的实例以提高性能并最大限度的减少内存碎片，此实现的分配内存的方法 是使用jemalloc，此种 方法分配内存的效率非常高，已被大量现代操作系统采用。 UnpooledByteBufAllocator则不会池化ByteBuf， Netty默认使用的是PooledByteBufALlocator。 Unpooled 当Channel或ChannelHandlerContext未引用ByteBufAllocator时，就无法使用ByteBufAllocator来分配 ByteBUf，对于这种情况，Netty提供了Unpooled工具类，它提供了一系列的静态方法来分配未池化的ByteBuf。 ByteBufUtil ByteBufUtil是ByteBuf的一个工具类，它提供大量操作ByteBuf的方法，，其中非常重要的一个方法就是 hexDump，这个方法会以16进制的形式来表示ByteBuf的内容。另一个很重要的方法是equals，它被用于判断 ByteBuf之间的相等性。 引用计数 学习过JVM的小伙伴应该知道垃圾回收有引用计数法和可达性分析这两种算法判断对象是否存活，Netty就使用了 引用计数法来优化内存的使用。引用计数确保了当对象的引用计数大于1时，对象就不会被释放，当计数减少至0时， 对象就会被释放，如果程序访问一个已被释放的引用计数对象，那么将会导致一个 IllegalReferenceCountException异常。 在Netty中，ByteBuf和ByteBufHolder都实现了ReferenceCounted接口。 "},"gitbook_doc/netty-learning/ChannelHandler和ChannelPipeline.html":{"url":"gitbook_doc/netty-learning/ChannelHandler和ChannelPipeline.html","title":"ChannelHandler和ChannelPipeline","keywords":"","body":" ChannelHandler和ChannelPipeline ChannelHandler家族 Channel的生命周期 ChannelHandler生命周期 ChannelInboundHandler接口 ChannelOutboundHandler接口 资源管理 ChannelPipeline ChannelPipeline相对论 修改ChannelPipeline ChannelHandler的执行和阻塞 触发事件 ChannelHandlerContext ChannelHandlerContext的高级用法 ChannelHandler和ChannelPipeline 在Netty组件中我们已经介绍了ChannelHandler和ChannelPipeline的关系，这里我们将继续深入了解这两个核心 组件的细节。在学习本章内容之前，请各位同学温习一遍Netty组件部分的内容。 ChannelHandler家族 Channel的生命周期 在Channel的生命周期中，它的状态与ChannelHandler是密切相关的，下列是Channel组件的四个状态： 状态 描述 ChannelUnregistered Channel没有注册到EventLoop ChannelRegistered Channel被注册到了EventLoop ChannelActive Channel已经连接到它的远程节点，处于活动状态，可以收发数据 ChannelInactive Channel与远程节点断开不再处于活动状态 Channel的生命周期如下图所示，当这些状态发生改变时，将会生成对应的事件，ChannelPipeline中的ChannelHandler 就可以及时做出处理。 ChannelHandler生命周期 ChannelHandler接口定义了其生命周期中的操作，当ChanelHandler被添加到ChannelPipeline 或从ChannelPipeline中移除时，会调用这些操作，ChannelHandler的生命周期如下： 方法 描述 handlerAdded 当把ChannelHandler添加到ChannelPipeline中时调用此方法 handlerRemoved 当把ChannelHandler从ChannelPipeline中移除的时候会调用此方法 exceptionCaught 当ChannelHandler在处理数据的过程中发生异常时会调用此方法 ChannelInboundHandler接口 ChannelInboundHandler会在接受数据或者其对应的Channel状态发生改变时调用其生命周期的方法， ChannelInboundHandler的生命周期和Channel的生命周期其实是密切相关的。 以下是ChannelInboundHandler的生命周期方法： 方法 描述 ChannelRegistered 当Channel被注册到EventLoop且能够处理IO事件时会调用此方法 ChannelUnregistered 当Channel从EventLoop注销且无法处理任何IO事件时会调用此方法 ChannelActive 当Channel已经连接到远程节点(或者已绑定本地address)且处于活动状态时会调用此方法 ChannelInactive 当Channel与远程节点断开，不再处于活动状态时调用此方法 ChannelReadComplete 当Channel的某一个读操作完成时调用此方法 ChannelRead 当Channel有数据可读时调用此方法 ChannelWritabilityChanged 当Channel的可写状态发生改变时调用此方法，可以调用Channel的isWritable方法检测Channel的可写性，还可以通过ChannelConfig来配置write操作相关的属性 userEventTriggered 当ChannelInboundHandler的fireUserEventTriggered方法被调用时才调用此方法。 这里有一个细节一定需要注意：当我们实现ChannelInboundHandler的channelRead方法时，请一定要记住 使用ReferenceCountUtil的release方法释放ByteBuf，这样可以减少内存的消耗，所以我们可以实现一个 ChannelHandler来完成对ByteBuf的释放，就像下面这样： 一个更好的办法是继承SimpleChannelInboundHandler，因为SimpleChannelInboundHandler已经帮我们 把与业务无关的逻辑在ChannelRead方法实现了，我们只需要实现它的channelRead0方法来完成我们的逻辑就够了： 可以看到SimpleChannelInboundHandler已经将释放资源的逻辑实现了，而且会自动调用ChannelRead0方法 来完成我们业务逻辑。 ChannelOutboundHandler接口 出站数据将由ChannelOutboundHandler处理，它的方法将被Channel，ChannelPipeline以及ChannelHandlerContext调用 （Channel，ChannelPipeline，ChannelHandlerContext都拥有write操作），以下是ChannelOutboundHandler的主要方法： 状态 描述 bind 当Channel绑定到本地address时会调用此方法 connect 当Channel连接到远程节点时会调用此方法 disconnect 当Channel和远程节点断开时会调用此方法 close 当关闭Channel时会调用此方法 deregister 当Channel从它的EventLoop注销时会调用此方法 read 当从Channel读取数据时会调用此方法 flush 当Channel将数据冲刷到远程节点时调用此方法 write 当通过Channel将数据写入到远程节点时调用此方法 ChannelOutboundHandler的大部分方法都需要一个ChannelPromise类型的参数，ChannelPromise是 ChannelFuture的一个子接口，这样你就可以明白ChannelPromise实际的作用和ChannelFuture是一样的， 没错，ChannelPromise正是用于在ChannelOutboundHandler的操作完成后执行的回调。 资源管理 当我们使用ChannelInboundHandler的read或ChannelOutboundHandler的write操作时，我们都需要保证 没有任何资源泄露并尽可能的减少资源耗费。之前已经介绍过了ReferenceCount引用计数用于处理池化的 ByteBuf资源。 为了帮助我们诊断潜在的的资源泄露问题，Netty提供了ResourceLeakDetector，它将 对我们的Netty程序的已分配的缓冲区做大约1%的采样用以检测内存泄露，Netty目前定义了4种泄露检测级别，如下： 级别 描述 Disabled 禁用泄露检测。我们应当在详细测试之后才应该使用此级别。 SIMPLE 使用1%的默认采样率检测并报告任何发现的泄露，这是默认的检测级别。 ADVANCED 使用默认的采样率，报告任何发现的泄露以及对应的消息的位置。 PARANOID 类似于ADVANCED，但是每次都会对消息的访问进行采样，此级别可能会对程序的性能造成影响，应该用于调试阶段。 我们可以通过JVM启动参数来设置leakDetector的级别： java -Dio.netty.leakDetectionLevel=ADVANCED ChannelPipeline 在Netty组件中也介绍过了，ChannelPipeline是一系列ChannelHandler组成的拦截链，每一个新创建的Channel 都会被分配一个新的ChannelPipeline，Channel和ChannelPipeline之间的关联是持久的，无需我们干涉它们 之间的关系。 ChannelPipeline相对论 Netty总是将ChannelPipeline的入站口作为头部，出站口作为尾部，当我们通过ChannelPipeline的add方法 将入站处理器和出站处理器混合添加到ChannelPipeline后，ChannelHandler的顺序如下： 一个入站事件将从ChannelPipeline的头部（左侧）向尾部（右侧）开始传播，出站事件的传播则是与入站的传播方向 相反。当ChannelPipeline在ChannelHandler之间传播事件的时候，它会判断下一个ChannelHandler的类型 是否与当前ChannelHandler的类型相同，如果相同则说明它们是一个方向的事件， 如果不同则跳过该ChannelHandler并前进到下一个ChannelHandler，直到它找到相同类型的ChannelHandler。 修改ChannelPipeline ChannelPipeline可以通过添加，删除和修改ChannelHandler来修改它自身的布局，这是它最基本的能力， 一下列举了ChannelPipeline的一些修改方法： | 方法 | 描述 | | addXX | 将指定的ChannelHandler添加到ChannelPipeline中 | | remove | 将指定的ChannelHandler从ChannelPipeline中移除 | | replace | 将ChannelPipeline中指定的ChannelHandler替换成另一个ChannelHandler | ChannelHandler的执行和阻塞 通常ChannelPipeline中的每个ChannelHandler都是通过它（ChannelPipeline）的EventLoop线程来处理 传递给他的数据的，所以我们不能去阻塞这个线程，否则会对整体的IO操作产生负面影响。 但有时候不得已 需要使用阻塞的API来完成逻辑处理，对于这种情况，ChannelPipeline的某些方法支持接受一个EventLoopGroup 类型的参数，我们可以通过自定义EventLoopGroup的方式，使ChannelHandler在我们的EventLoopGroup内处理数据。 这样一来，就可以避免阻塞线程的影响了。 触发事件 ChannelPipeline的API不仅有对ChannelHandler的增删改操作，还有对入站和出站操作的附加方法，如下： ChannelPipeline的入站方法： 方法 描述 fireChannelRegistered 调用ChannelPipeline中下一个ChannelInboundHandler的channelRegistered方法 fireChannelUnregistered 调用ChannelPipeline中下一个ChannelInboundHandler的channelUnregistered方法 fireChannelActive 调用ChannelPipeline中下一个ChannelInboundHandler的channelActive方法 fireChannelInactive 调用ChannelPipeline中下一个ChannelInboundHandler的channelInactive方法 fireExceptionCaught 调用ChannelPipeline中下一个ChannelInboundHandler的exceptionCaught方法 fireUserEventTriggered 调用ChannelPipeline中下一个ChannelInboundHandler的userEventTriggered方法 fireChannelRead 调用ChannelPipeline中下一个ChannelInboundHandler的channelRead方法 fireChannelReadComplete 调用ChannelPipeline中下一个ChannelInboundHandler的channelReadComplete方法 fireChannelWritabilityChanged 调用ChannelPipeline中下一个ChannelInboundHandler的channelWritabilityChanged方法 ChannelPipeline的出站方法： 方法 描述 bind 调用ChannelPipeline中下一个ChannelOutboundHandler的bind方法，将Channel与本地地址绑定 connect 调用ChannelPipeline中下一个ChannelOutboundHandler的connect方法，将Channel连接到远程节点 disconnect 调用ChannelPipeline中下一个ChannelOutboundHandler的disconnect方法，将Channel与远程连接断开 close 调用ChannelPipeline中下一个ChannelOutboundHandler的close方法，将Channel关闭 deregister 调用ChannelPipeline中下一个ChannelOutboundHandler的deregister方法，将Channel从其对应的EventLoop注销 flush 调用ChannelPipeline中下一个ChannelOutboundHandler的flush方法，将Channel的数据冲刷到远程节点 write 调用ChannelPipeline中下一个ChannelOutboundHandler的write方法，将数据写入Channel writeAndFlush 先调用write方法，然后调用flush方法，将数据写入并刷回远程节点 read 调用ChannelPipeline中下一个ChannelOutboundHandler的read方法，从Channel中读取数据 ChannelHandlerContext ChannelHandlerContext代表的是ChannelHandler和ChannelPipeline之间的关联，每当有ChannelHandler 添加到ChannelPipeline中时，都会创建ChannelHandlerContext。ChannelHandlerContext的主要功能是 管理它所关联的ChannelHandler与同一个ChannelPipeline中的其他ChannelHandler之间的交互： ChannelHandlerContext的大部分方法和Channel和ChannelPipeline相似，但有一个重要的区别是： 调用Channel或ChannelPipeline的方法，如： //使用Chanel write Channel channel = ctx.channel(); ctx.write(xxx); //使用Pipeline write ChannelPipeline pipeline = ctx.pipeline(); pipeline.write(xxx); ，其影响是会沿着整个ChannelPipeline进行传播： 而调用ChannelHandlerContext的方法，如： //使用ChannelContext write ctx.write(xxx); 则是从其关联的ChannelHandler开始，并且只会传播给位于该ChannelPipeline中的下一个能够处理该事件的 ChannelHandler： 下面是一些比较重要的方法，有些和ChannelPipeline功能相似的方法就不再罗列了，各位同学可以直接查看原API。 方法 描述 alloc 获取与当前ChannelHandlerContext所关联的Channel的ByteBufAllocator handler 返回与当前ChannelHandlerContext绑定的ChannelHandler pipeline 返回与当前ChannelHandlerContext关联的ChannelPipeline ... ... ChannelHandlerContext的高级用法 有时候我们需要在多个ChannelPipeline之间共享一个ChannelHandler，以此实现跨管道处理（获取）数据 的功能，此时的ChannelHandler属于多个ChannelPipeline，且会绑定到不同的ChannelHandlerContext上。 在多个ChannelPipeline之间共享ChannelHandler我们需要使用 @Sharable注解，这代表着它是一个共享的 ChannelHandler，如果一个ChannelHandler没有使用@Sharable注解却被用于多个ChannelPipeline，那么 将会触发异常。 还有非常重要的一点：一个ChannelHandler被用于多个ChannelPipeline肯定涉及到多线程 数据共享的问题，因此我们需要保证ChannelHandler的方法同步。 下面是一个很好的例子： @Sharable public class UnsafeSharableChannelHandler extends ChannelInboundHandlerAdapter { private int count; @Override public void channelRead(ChannelHandlerContext ctx,Object msg) { count++; System.out.println(\"count : \" + count); ctx.fireChannelRead(msg); } } 上面这个ChannelHandler标识了@Sharable注解，这代表它需要被用于多个ChannelPipeline之间， 但是这个ChannelHandler之中有一个不易察觉的问题： 它声明了一个实例变量count，且ChannelRead方法 不是线程安全的。 那么这个问题的后果我相信学习了多线程的同学应该都明白，一个最简单的方法 就是给修改了count的变量的方法加synchronized关键字，确保即使在多个ChannelPipeline之间共享， ChannelHandler也能保证数据一致。 "},"gitbook_doc/netty-learning/Netty线程模型和EventLoop.html":{"url":"gitbook_doc/netty-learning/Netty线程模型和EventLoop.html","title":"Netty线程模型和EventLoop事件循环","keywords":"","body":" Netty线程模型和EventLoop 线程模型概述 EventLoop事件循环 任务调度 JDK任务调度 EventLoop任务调度 线程管理 线程分配 非阻塞传输 阻塞传输 Netty线程模型和EventLoop 由于线程模型确定了代码执行的方式，它可能带来一些副作用以及不确定因素， 可以说这是并发编程中最大的难点，因此，我们需要了解Netty所采用的线程模型，这样 在遇到相关问题时不至于手足无措。 线程模型概述 现代操作系统几乎都具有多个核心的CPU，所以我们可以使用多线程技术以有效地利用系统资源。在早期的 Java多线程编程中，我们使用线程的方式一般都是继承Thread或者实现Runnable以此创建新的Thread， 这是一种比较原始且浪费资源的处理线程的方式。JDK5之后引入了Executor API，其核心思想是使用池化技术 来重用Thread，以此达到提高线程响应速度和降低资源浪费的目的。 EventLoop事件循环 事件循环正如它的名字，处于一个循环之中。我们以前在编写网络程序的时候，会使我们处理连接的逻辑 处于一个死循环之中，这样可以不断的处理客户端连接。 Netty的EventLoop采用了两个基本的API：并发和网络。 Netty的并发包io.netty.util.concurrent是基于Java的并发包java.util.concurrent之上的， 它主要用于提供Executor的支持。Netty的io.netty.channel包提供了与客户端Channel的事件交互的支持。 以下是EventLoop类层次结构图： 在EventLoop模型中，EventLoop将有一个永远不会改变的Thread，即Netty会给EventLoop分配一个 Thread，在EventLoop生命周期之中的所有IO操作和事件都由这个Thread执行。 根据配置和CPU核心的不同， Netty可以创建多个EventLoop，且单个EventLoop可能会服务于多个客户端Channel。 在EventLoop中，事件或任务的执行总是以FIFO先进先出的顺序执行的，这样可以保证字节总是按正确的 顺序被处理，消除潜在的数据损坏的可能性。 任务调度 有时候我们需要在指定的时间之后触发任务或者周期性的执行某一个人物，这都需要使用到任务调度。 JDK任务调度 JDK主要有Timer和ScheduledExecutorService两种实现任务调度的方式，但是这两种原生的API的 性能都不太适合高负载应用。 EventLoop任务调度 上面介绍过的EventLoop类层次结构图，可以看到EventLoop扩展了ScheduledExecutorService， 所以我们可以通过EventLoop来实现任务调度，其编程模型如下： 使用Channel获取其对应的EventLoop，然后调用schedule方法给其分配一个Runnable执行。Netty的任务调度 比JDK的任务调度性能性能要好，这主要是由于Netty底层的线程模型设计的非常优秀。 线程管理 Netty线程模型的卓越性能取决于当前执行任务的Thread，我们看一张图就明白了： 如果处理Chanel任务的线程正是支撑EventLoop的线程，那么与Channel的任务会被直接执行。 否则EventLoop会将该任务放入任务队列之中稍后执行。 需要注意的是每个EventLoop都有自己的任务队列，独立于其他EventLoop的任务队列。 线程分配 每个EventLoop都注册在一个EventLoopGroup之中，一个EventLoopGroup可以包含多个EventLoop，根据不同的传输实现， EventLoop的创建和分配方式也不同。 非阻塞传输 我们说过一个EventLoop可以处理多个Channel，Netty这样设计的目的就是尽可能的通过少量Thread来支撑大量的Channel， 而不是每个Channel都分配一个Thread。 EventLoopGroup负责为每个新创建的Channel分配一个EventLoop，一旦一个Channel被分配给EventLoop，它将在 整个生命周期中都使用这个EventLoop及其Thread处理事件和任务。 注意：EventLoop的分配方式对ThreadLocal的使用是很有很大影响的。因为注册在一个EventLoop上的Channel 共有这一个线程，那么在这些Channel之间使用ThreadLocal，其ThreadLocal的状态都是一样的，无法发挥ThreadLocal 本来的作用。 阻塞传输 阻塞传输即OIO(BIO)，此种传输方式的EventLoop只会被分配一个Channel，如下图： 这样带来的会是线程资源的巨大消耗，导致并发量降低。 "},"gitbook_doc/orm-learning/ORM简介.html":{"url":"gitbook_doc/orm-learning/ORM简介.html","title":"ORM简介","keywords":"","body":"ORM(Object Relational Mapping) 什么是ORM? Object Relational Mapping : 对象关系映射。 它是一种解决数据库与简单对象(entity)之间关系映射的技术。 简单理解: ORM通过简单对象(entity) 与 数据库之间的映射关系，将描述对象持久化到数据库中。 JDBC的缺点 频繁创建数据库连接,浪费连接资源,不易维护 SQL语句存在硬编码，不易维护 结果集处理过程繁琐 "},"gitbook_doc/orm-learning/Mybatis简介.html":{"url":"gitbook_doc/orm-learning/Mybatis简介.html","title":"Mybatis简介","keywords":"","body":"Mybatis 什么是Mybatis? Mybatis是一款优秀的轻量级的半ORM框架。 Mybatis最大的优点就是无需像JDBC一样采用硬编码的方式进行持久化操作， 它允许我们定制SQL和对象与数据库之间的高级映射关系，极大的提高了持久化操作的灵活性。 为什么说Mybatis是半ORM框架? 与Hibernate不同，Hibernate属于全自动ORM框架，无需手写SQL， 且能够自动建立对象与数据库之间的映射关系，很方便。 但无需手写SQL也就意味着SQL优化方面可能不如Mybatis那么出色。 Mybatis则属于半自动ORM框架，因为Mybatis仅仅给我们省去了JDBC硬编码的形式， 但是在定义SQL和建立对象持久化关系方面，仍然给了我们很大自由， 它相对Hibernate更加灵活，扩展性更强。 "},"gitbook_doc/orm-learning/Mybatis常见知识点.html":{"url":"gitbook_doc/orm-learning/Mybatis常见知识点.html","title":"Mybatis常见知识点","keywords":"","body":" Mybatis常见知识点 Mybatis优点 Mybatis缺点 Mybatis适用场景 Mybatis架构 Mybatis SQL执行流程 Executor的类型 什么是延迟加载? 延迟加载原理 ${} 和 #{}的区别 Mybatis 模糊查询LIKE怎么写 Mybatis是如何获取生成的主键的? Mybatis动态SQL是什么? Mybatis插件原理 Mybatis一级缓存 一级缓存的原理 使得Mybatis一级缓存失效的方法 Mybatis二级缓存 Mybatis二级缓存的原理 Mybatis缓存的缺点 Mybatis常见知识点 Mybatis优点 消除了JDBC硬编码的方式，提高了应用的扩展性。 自定义SQL和对象持久化关系，带来了灵活性。 SQL和对象持久化关系都在配置里，解除了SQL与程序之间的耦合性。 Mybatis缺点 配置繁琐 Mybatis适用场景 功能复杂的应用: Mybatis足够的灵活，这保证了它能够面对较为复杂的应用场景。 考虑SQL优化的应用: SQL优化是一个很常见的问题，Mybatis允许我们自己编写SQL，这样一来就可以轻松的更新和优化SQL了。 Mybatis架构 Mybatis架构图: Configuration: Configuration可以说是贯穿整个Mybatis生命周期的一个核心配置组件, 它存储着Mybatis所有需要的属性和组件。 从解析阶段开始，到获取Mapper，都需要Configuration。 Configuration内部属性一览： SQLSession: SQLSession是Mybatis最顶级的API接口， 它封装了SQL的增删查改功能，但最终还是交由Executor去执行逻辑。 Executor: Executor执行器，是Mybatis的核心组件之一， 它负责调度StatementHandler来维护和执行SQL。 StatementHandler: StatementHandler负责JDBC的statement的操作，如SQL入参，执行SQL，封装结果集。 ParameterHandler: ParameterHandler负责将用户传递的参数转换成statement所需的参数。 TypeHandler: TypeHandler负责Java数据类型与JDBC数据类型的转换。 ResultSetHandler: ResultSetHandler负责处理statement执行后返回的结果集。 Mybatis SQL执行流程 配置解析: 由XML解析器解析配置文件(总配置文件，mapper文件),并将解析的结果保存到Configuration中。 使用配置环境信息构建SQLSessionFactory工厂: SQLSessionFactory提供了构建SQLSession的多种方式，可以指定Executor的类型和事务隔离级别等。 使用SQLSessionFactory创建SQLSession会话: SQLSessionFactory创建SQLSession后，其实是创建的DefaultSQLSession，它包含了Mybatis的环境配置Configuration和Executor执行器。 使用SQLSession获取用户需要的Mapper类: Mybatis底层是使用了jdk动态代理来实现目标Mapper的执行的，获取Mapper实际上是获取Mapper的代理类。 使用MapperProxy执行目标方法: 实际上最终是Executor调度StatementHandler执行statement。 Executor调用StatementHandler对Statement做出处理(包括参数处理，执行，结果集处理)。 StatementHandler调用ParameterHandler装配SQL参数并执行,最后使用ResultSetHandler封装结果集返回。 Executor的类型 SimpleExecutor: 简单执行器。 每次执行SQL就开启一个statement，用完后就关闭掉。 ReuseExecutor: 可重用的执行器。每次执行SQL先去缓存(Map)中找SQL对应的statement， 如果不存在就新创建statement，用完后并不关闭，而是放入Map缓存中，以待下次使用。 BatchExecutor: 批处理执行器。执行SQL时，会将statement添加到批处理中，等到最终executeBatch时，一起执行。 什么是延迟加载? 延迟加载又称按需加载，即在关联查询中(一对一或一对多)， 如果指定了延迟加载，那么并不会一次就把对象关联的数据查出来， 而是等到对象需要使用关联的数据时才会进行查询。 延迟加载原理 Mybatis的底层原理是ResultSetHandler在封装结果时， 判断对象的属性是否有关联查询(嵌套查询)，如果有，则使用动态代理创建该对象作为结果。 当对象需要使用它的某个属性时，比如a调用getB方法， 那么getB方法就会进入代理方法，如果getB为空，就查询B，并setB，这样就可以获取到a的B属性了。 ${} 和 #{}的区别 ${} 是将传入的参数直接显示在SQL中;#{} 把传入的参数当做字符串，会给参数加上引号 假设有2条SQL如下: 1. SELECT * FROM table WHERE id =${id}; 2. SELECT * FROM table WHERE id = #{id}; 如果传入的参数为 1 , 那么第一条SQL会被拼接成: SELECT * FROM table WHERE id = 1; 第二条SQL会被编译成: SELECT * FROM table WHERE id = \"1\"; ${}属于拼接符，需要进行字符串拼接;#{} 属于占位符，需要预编译 ${}不能防止SQL注入;#{}可以在很大程度上预防SQL注入 假如有一条SQL: SELECT * FROM table WHERE id = ${id} 假设传入的id为: 1 OR 1 = 1 ,那么字符串拼接后,SQL为: SELECT * FROM table WHERE id = 1 OR 1 = 1; 这条SQL无论如何都会执行成功。 如果将 ${id} 改为 #{id},那么经过预编译后，SQL为: SELECT * FROM table WHERE id = \"1 OR 1 = 1\"; 可以看到: #{} 是将参数作为一个字符串为条件的，这样就可以避免 OR 生效，防止SQL注入。 Mybatis 模糊查询LIKE怎么写 SELECT * FROM table WHERE name LIKE '%${name}%' (有注入风险) SELECT * FROM table WHERE name LIKE \"%\"#{name}\"%\" SELECT * FROM table WHERE name LIKE CONCAT('%',#{name},'%') Bind标签: select * from table where name like #{fuzzyName} Mybatis是如何获取生成的主键的? Mybatis有一个KeyGenerator接口，这个接口专门用于获取数据库生成的主键。 但其核心原理还是使用的JDBC 的 API : Statement的 getGeneratedKeys 方法获取的。 Mybatis动态SQL是什么? Mybatis允许我们在mapper文件内，以标签的形式编写动态SQL，用于逻辑判断和SQL拼接等功能。 Mybatis动态标签有: trim / where / set / if / choose / otherwise / bind / foreach 等。 实际上Mybatis的动态标签是依赖于OGNL的。 Mybatis插件原理 Mybatis允许我们编写插件对它核心的组件： Executor , StatementHandler, ParameterHandler, ResultSetHandler 这些核心组件的扩展 Mybatis底层实际上是使用jdk动态代理包装后的组件带替它原生的组件。 当执行这些组件的方法时，就会执行Interceptor的intercept方法。 当然，只是当执行我们指定要拦截的方法时，才会执行intercept方法。 见:Configuration: Mybatis一级缓存 一级缓存又称本地缓存，它属于SqlSession级别的缓存，默认是开启的。每个SqlSession都有自己的缓存。 同一个SqlSession查询到的数据都会放入它自己的缓存中，如果之后需要获取相同的数据， 那么会先从缓存中查找，如果没有才会去查询数据库，这样就降低了数据库的压力。 Mybatis一级缓存流程: Mybatis一级缓存源码: 一级缓存的原理 对于BaseExecutor来说，它内部维护了一个叫localCache的PerpetualCache对象。 PerpetualCache实现了Cache接口，它内部使用HashMap进行缓存。 所以可以简单理解为Mybatis的一级缓存是由HashMap存储的。 Mybatis一级存实现: 使得Mybatis一级缓存失效的方法 如果SQL相同，但是SQL的条件或参数不同，缓存会失效 在两次查询操作中间，如果进行了增删改操作，会清空本地缓存 不同的SqlSession，缓存会失效 手动清空SqlSession的缓存 Mybatis二级缓存 二级缓存又称全局缓存，它属于mapper级别的缓存,默认是关闭的，需要指定配置和标签才会开启。 多个SqlSession操作同一个Mapper是可以共享一个二级缓存的，但是要求Sql会话必须属于同一个Mapper。 Mybatis二级缓存流程: Mybatis二级缓存的原理 Mybatis二级缓存的Executor使用的是CachingExecutor， 在原生的Executor执行查询操作之前，它会先从二级缓存中查询，如果查询不到才会从一级缓存或数据库中查询。 Configuration创建CachingExecutor: Mybatis二级缓存源码: Mybatis缓存的缺点 Mybatis缓存设计缺陷: Mybatis的一级缓存是使用HashMap实现的，并没有指定容量限制， 虽然可以提高查询效率，但是设计上还有所欠缺。 容易引起脏读: Mybatis的缓存是属于Java进程内的缓存，在分布式环境下，缓存的不一致， 很容易引起数据的脏读。建议还是使用第三方容器，如Redis和Memcached等中间件存储缓存数据。 "},"gitbook_doc/orm-learning/Mybatis源码分析.html":{"url":"gitbook_doc/orm-learning/Mybatis源码分析.html","title":"Mybatis源码分析","keywords":"","body":" Mybatis源码分析 1. 解析配置文件，创建SQLSessionFactory 2. 开启java程序和数据库之间的会话： 3. 获取mapper代理对象: 4. 执行mapper接口方法: mybatis源码总结 Mybatis源码分析 1. 解析配置文件，创建SQLSessionFactory InputStream inputStream = CommonTest.class.getClassLoader().getResourceAsStream(\"mybatis-configuration.xml\"); SQLSessionFactory SQLSessionFactory = new SQLSessionFactoryBuilder().build(inputStream); 这一步首先读取了mybatis的configuration xml配置文件,用这个流构造了Factory的Builder,它底层是使用Mybatis自己的XMLConfigBuilder解析器去解析了这个Configuration文件, 然后调用了解析器的parse方法,SQLSessionFactory就被构造出来了: public SQLSessionFactory build(InputStream inputStream, String environment, Properties properties) { SQLSessionFactory var5; try { //文件解析器 XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); //构造SQLSessionFactory var5 = this.build(parser.parse()); } catch (Exception var14) { throw ExceptionFactory.wrapException(\"Error building SQLSession.\", var14); } finally { ErrorContext.instance().reset(); try { inputStream.close(); } catch (IOException var13) { } } return var5; } 根据上面代码可知,SQLSessionFactory被创建的核心是 XMLConfigBuilder的 parse方法, 也就是解析文件的那个步骤,它又是怎么解析的呢? 初次学Mybatis的时候,配置的那个Configuration全局文件里有很多属性对吧,各种节点, environment,mapper,setting...的,可想它内部肯定是对这些节点做了解析的: private void parseConfiguration(XNode root) { try { this.propertiesElement(root.evalNode(\"properties\")); Properties settings = this.settingsAsProperties(root.evalNode(\"settings\")); //解析全局配置文件的各个节点,并加载配置 this.loadCustomVfs(settings); this.loadCustomLogImpl(settings); this.typeAliasesElement(root.evalNode(\"typeAliases\")); this.pluginElement(root.evalNode(\"plugins\")); this.objectFactoryElement(root.evalNode(\"objectFactory\")); this.objectWrapperFactoryElement(root.evalNode(\"objectWrapperFactory\")); this.reflectorFactoryElement(root.evalNode(\"reflectorFactory\")); this.settingsElement(settings); this.environmentsElement(root.evalNode(\"environments\")); this.databaseIdProviderElement(root.evalNode(\"databaseIdProvider\")); this.typeHandlerElement(root.evalNode(\"typeHandlers\")); //解析mapper的各个元素(SQL,resultmap......) this.mapperElement(root.evalNode(\"mappers\")); } catch (Exception var3) { throw new BuilderException(\"Error parsing SQL Mapper Configuration. Cause: \" + var3, var3); } } 从上面代码可以分析,其实解析是大概分为2个部分的,一个是解析全局配置文件里的属性, 一个是解析mapper文件的各个属性,对已经学过mybatis的同学来说, 都知道mybatis底层是使用了动态代理模式来操作接口方法的,那么从第二个部分:解析mapper的部分就尤为重要了. 先看第一个部分,其实其第一个部分最重要的一点就是分析出了,mybatis所有的属性, 配置全部由一个Configuration对象保存了起来,随便抽一个方法: private void settingsElement(Properties props) { this.configuration.setAutoMappingBehavior(AutoMappingBehavior.valueOf(props.getProperty(\"autoMappingBehavior\", \"PARTIAL\"))); this.configuration.setAutoMappingUnknownColumnBehavior(AutoMappingUnknownColumnBehavior.valueOf(props.getProperty(\"autoMappingUnknownColumnBehavior\", \"NONE\"))); this.configuration.setCacheEnabled(this.booleanValueOf(props.getProperty(\"cacheEnabled\"), true)); this.configuration.setProxyFactory((ProxyFactory)this.createInstance(props.getProperty(\"proxyFactory\"))); this.configuration.setLazyLoadingEnabled(this.booleanValueOf(props.getProperty(\"lazyLoadingEnabled\"), false)); this.configuration.setAggressiveLazyLoading(this.booleanValueOf(props.getProperty(\"aggressiveLazyLoading\"), false)); this.configuration.setMultipleResultSetsEnabled(this.booleanValueOf(props.getProperty(\"multipleResultSetsEnabled\"), true)); this.configuration.setUseColumnLabel(this.booleanValueOf(props.getProperty(\"useColumnLabel\"), true)); this.configuration.setUseGeneratedKeys(this.booleanValueOf(props.getProperty(\"useGeneratedKeys\"), false)); ...... } 再看看Configuration类的属性,就印证了之前说过的, Configuration就是一个贯穿的mybatis整个生命周期的核心配置类: public class Configuration { protected Environment environment; protected boolean safeRowBoundsEnabled; protected boolean safeResultHandlerEnabled; protected boolean mapUnderscoreToCamelCase; protected boolean aggressiveLazyLoading; protected boolean multipleResultSetsEnabled; protected boolean useGeneratedKeys; protected boolean useColumnLabel; protected boolean cacheEnabled; protected boolean callSettersOnNulls; protected boolean useActualParamName; protected boolean returnInstanceForEmptyRow; protected String logPrefix; protected Class logImpl; protected Class vfsImpl; protected LocalCacheScope localCacheScope; protected JdbcType jdbcTypeForNull; protected Set lazyLoadTriggerMethods; ...... ...... } 之前说过,XMLConfigBuilder负责解析mybatis全局配置文件,而解析阶段又大致分为2个阶段:解析基本属性; 解析mapper文件.解析的基本属性都存放在了全局的Configuration之中. 再看mapperElement方法,也就是开始解析的那个方法,可以直接锁定XMLMapperBuilder类, 看这个类的名字就知道它是解析mapper文件的: private void mapperElement(XNode parent) throws Exception { if (parent != null) { Iterator var2 = parent.getChildren().iterator(); while(true) { while(var2.hasNext()) { XNode child = (XNode)var2.next(); String resource; if (\"package\".equals(child.getName())) { resource = child.getStringAttribute(\"name\"); this.configuration.addMappers(resource); } else { resource = child.getStringAttribute(\"resource\"); String url = child.getStringAttribute(\"url\"); String mapperClass = child.getStringAttribute(\"class\"); XMLMapperBuilder mapperParser; InputStream inputStream; if (resource != null && url == null && mapperClass == null) { ErrorContext.instance().resource(resource); inputStream = Resources.getResourceAsStream(resource); //开始解析mapper文件 mapperParser = new XMLMapperBuilder(inputStream, this.configuration, resource, this.configuration.getSQLFragments()); mapperParser.parse(); } else if (resource == null && url != null && mapperClass == null) { ErrorContext.instance().resource(url); inputStream = Resources.getUrlAsStream(url); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, url, this.configuration.getSQLFragments()); mapperParser.parse(); } else { if (resource != null || url != null || mapperClass == null) { throw new BuilderException(\"A mapper element may only specify a url, resource or class, but not more than one.\"); } Class mapperInterface = Resources.classForName(mapperClass); this.configuration.addMapper(mapperInterface); } } } return; } } } 在说解析mapper文件之前,先想想mapper文件里有什么,最核心的就那几个:resultMap,statement(也就是SQL), cache缓存,那么XMLMapperBuilder肯定也是围绕那个几个去解析的,或者还解析了其他的东西: public void parse() { if (!this.configuration.isResourceLoaded(this.resource)) { //这里是解析Mapper文件的核心,它内部把Mapper的select,delete,update,insert这些 //标签添加到了MapperStatement内,也就是SQL语句 this.configurationElement(this.parser.evalNode(\"/mapper\")); this.configuration.addLoadedResource(this.resource); //这一步也是非常重要的,它绑定了mapper文件的命名空间 this.bindMapperForNamespace(); } //解析还没有解析的各个元素 this.parsePendingResultMaps(); this.parsePendingCacheRefs(); this.parsePendingStatements(); } 其实mybatis解析mapper文件的步骤还是比较深入的,这里由于篇幅关系,怕说长了,就脱离此文的目的,还是以理解为主,直到mybatis做了哪些事情就行,但还是需要去看看mybatis到底是如何解析mapp文件的,也就是 上面的: this.configurationElement(this.parser.evalNode(\"/mapper\")); ...... 最后,回到解析Configuration文件的起点,解析完文件后,SQLSessionFactory就被build方法构造出来了,其实他是个DefaultSQLSessionFactory: public SQLSessionFactory build(Configuration config) { return new DefaultSQLSessionFactory(config); } 总结下SQLSessionFactory被创建的过程: 首先Mybatis使用XMLConfigBuilder文件解析器,解析全局配置文件,XMLMapperBuilder,解析mapper文件,解析完后将所有的属性封装在了Configuration对象中,然后使用这个全局的Configuration对象构造了DefaultSQLSessionFactory. 2. 开启java程序和数据库之间的会话： SQLSession SQLSession = SQLSessionFactory.openSession(); 从第一步创建SQLSessionFactory的过程可知,SQLSessionFactory是一个DefaultSQLSessionFactory,所以openSession也是调用了DefaultSQLSessionFactory的openSession: public SQLSession openSession() { return this.openSessionFromDataSource(this.configuration.getDefaultExecutorType(), (TransactionIsolationLevel)null, false); } 可以看到openSession是调用了openSessionFromDataSource方法,那它是怎么实现的呢: private SQLSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; DefaultSQLSession var8; try { Environment environment = this.configuration.getEnvironment(); //事物管理 TransactionFactory transactionFactory = this.getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //核心Executor执行器 Executor executor = this.configuration.newExecutor(tx, execType); //SQLSession就是DefaultSQLSession var8 = new DefaultSQLSession(this.configuration, executor, autoCommit); } catch (Exception var12) { this.closeTransaction(tx); throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + var12, var12); } finally { ErrorContext.instance().reset(); } 从上面代码可以看出,SQLSession是DefaultSQLSession,然后还创建了一个Executor这个核心的执行器对象,那么首先看看这个Executor是什么,为什么说它是核心呢？ 首先看看它内部的方法吧: public interface Executor { ResultHandler NO_RESULT_HANDLER = null; int update(MappedStatement var1, Object var2) throws SQLException; List query(MappedStatement var1, Object var2, RowBounds var3, ResultHandler var4, CacheKey var5, BoundSQL var6) throws SQLException; List query(MappedStatement var1, Object var2, RowBounds var3, ResultHandler var4) throws SQLException; Cursor queryCursor(MappedStatement var1, Object var2, RowBounds var3) throws SQLException; List flushStatements() throws SQLException; void commit(boolean var1) throws SQLException; void rollback(boolean var1) throws SQLException; ... ... } 从Executor内部方法可以看出,它负责执行Statement的执行操作.既然它是一个接口,那么必然有对应的实现类,这里先看configuration的newExecutor方法是怎么创建它的吧: public Executor newExecutor(Transaction transaction, ExecutorType executorType) { executorType = executorType == null ? this.defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Object executor; //批处理的Executor if (ExecutorType.BATCH == executorType) { executor = new BatchExecutor(this, transaction); } //可复用的Executor else if (ExecutorType.REUSE == executorType) { executor = new ReuseExecutor(this, transaction); } //简单的Executor,如果不做配置,那么就默认是它了 else { executor = new SimpleExecutor(this, transaction); } //缓存Executor,这里就是二级缓存的关键之处,把已经创建好的Executor,装成CachingExecutor if (this.cacheEnabled) { executor = new CachingExecutor((Executor)executor); } //插件拓展,原来插件是在创建Executor的时候被封装的. Executor executor = (Executor)this.interceptorChain.pluginAll(executor); return executor; } 从上面代码就可以分析出:创建SQLSession的时机其实是创建Executor的时机,也是封装plugin的时机, 也可以猜测Executor就是Mybatis的核心组件之一,负责执行一系列的SQL(Statement). 总结下第二步获取SQLSession的过程: 使用DefaultSQLSessionFactory的Configuration创建出对应类型的Executor, 并封装配置中的插件,再使用Executor和Configuration创建DefaultSQLSession, 由此可见Configuration从被构建出来,流转到了DefaultSQLSession之中. 3. 获取mapper代理对象: PersonMapper personMapper = SQLSession.getMapper(PersonMapper.class); 已经知到了上面返回的PersonMapper是一个MapperProxy对象,那么它是怎么被创建出来的呢? 回想下上面的几个步骤,DefaultSQLSession包含了Configuration,而Configuration是解析的全局配置文件和mapper文件被构造出来的,Configuration也包含了相应的属性, 所以MapperProxy应该也是从Configuration获取: public T getMapper(Class type) { return this.configuration.getMapper(type, this); } ----------------------------------------------------------- public T getMapper(Class type, SQLSession SQLSession) { return this.mapperRegistry.getMapper(type, SQLSession); } 可以看到最终是由MapperRegistry对象获取的,那MapperRegistry是如何获取的呢: ... private final Map, MapperProxyFactory> knownMappers = new HashMap(); public T getMapper(Class type, SQLSession SQLSession) { //获取mapper接口对应的工厂 MapperProxyFactory mapperProxyFactory = (MapperProxyFactory)this.knownMappers.get(type); if (mapperProxyFactory == null) { throw new BindingException(\"Type \" + type + \" is not known to the MapperRegistry.\"); } else { try { //使用工厂创建mapper接口 return mapperProxyFactory.newInstance(SQLSession); } catch (Exception var5) { throw new BindingException(\"Error getting mapper instance. Cause: \" + var5, var5); } } } MapperRegistry内部有一个map,保存着mapper接口的class到相应的MapperProxyFactory的工厂,当我们需要获取某个mapper接口的时候,就利用相对的工厂创建mapper接口代理对象. 需要搞清楚的是MapperProxyFactory是如何创建Mapper接口代理对象的呢?直接锁定newInstance方法: protected T newInstance(MapperProxy mapperProxy) { //jdk动态代理 return Proxy.newProxyInstance(this.mapperInterface.getClassLoader(), new Class[]{this.mapperInterface}, mapperProxy); } public T newInstance(SQLSession SQLSession) { //首先创建MapperProxy对象,MapperProxy对象实现了InvocationalHandler接口,所以它可以被jdk动态代理 MapperProxy mapperProxy = new MapperProxy(SQLSession, this.mapperInterface, this.methodCache); return this.newInstance(mapperProxy); } 上面代码就不解释了,直接总结第3步吧: Configuration从MapperRegistry里获取对应的Mapper接口的代理工厂MapperProxyFactory,MapperProxyFactory使用jdk动态代理创建Mapper接口的动态代理对象. 4. 执行mapper接口方法: personMapper.selectPersonById(1L); 上面分析到由SQLSession获取的Mapper对象其实是MapperProxyFactory创建的MapperProxy代理对象,那么SQL代码的执行也肯定是在MapperProxy类的invoke中了,所以直接锁定MapperProxy类的invoke方法: public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { //如果是Object类的方法 if (Object.class.equals(method.getDeclaringClass())) { return method.invoke(this, args); } //jdk8后的接口允许默认方法,所以在这里做判断 if (method.isDefault()) { if (privateLookupInMethod == null) { return this.invokeDefaultMethodJava8(proxy, method, args); } return this.invokeDefaultMethodJava9(proxy, method, args); } } catch (Throwable var5) { throw ExceptionUtil.unwrapThrowable(var5); } MapperMethod mapperMethod = this.cachedMapperMethod(method); //真正执行Statement的入口 return mapperMethod.execute(this.SQLSession, args); } 根据invoke方法可以分析出,invoke方法执行的也就是MapperMethod的execute方法,那MapperMethod是什么呢?先猜下：在mybatis所有核心组件基本都是xxxHandler命名,所有与接口有关的基本都是mapperxx命名,mapperProxy是接口代理对象,mapperMethod就有可能是接口的执行方法. 看下mapperProxy的属性: public class MapperProxy implements InvocationHandler, Serializable { private static final long serialVersionUID = -6424540398559729838L; private static final int ALLOWED_MODES = 15; private static final Constructor lookupConstructor; private static final Method privateLookupInMethod; private final SQLSession SQLSession; private final Class mapperInterface; //存储着mapperProxy对应接口的方法 private final Map methodCache; .... } 再看刚才在invoke方法里的一个细节: public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ..... //根据接口要调用的方法,获取对应的mapperMethod MapperMethod mapperMethod = this.cachedMapperMethod(method); //使用获取到的mapperMethod执行SQL return mapperMethod.execute(this.SQLSession, args); } 已经可以确定了,在调用mapper接口方法的时候,mapperProxy内部已经维护了对应接口的所有方法,只等我们调用的时候execute执行了. 那mapperMethod是如何执行的呢? public Object execute(SQLSession SQLSession, Object[] args) { Object result; Object param; switch(this.command.getType()) { //insert操作 case INSERT: param = this.method.convertArgsToSQLCommandParam(args); result = this.rowCountResult(SQLSession.insert(this.command.getName(), param)); break; //update操作 case UPDATE: param = this.method.convertArgsToSQLCommandParam(args); result = this.rowCountResult(SQLSession.update(this.command.getName(), param)); break; //delete操作 case DELETE: param = this.method.convertArgsToSQLCommandParam(args); result = this.rowCountResult(SQLSession.delete(this.command.getName(), param)); break; //select操作 case SELECT: if (this.method.returnsVoid() && this.method.hasResultHandler()) { this.executeWithResultHandler(SQLSession, args); result = null; } else if (this.method.returnsMany()) { result = this.executeForMany(SQLSession, args); } else if (this.method.returnsMap()) { result = this.executeForMap(SQLSession, args); } else if (this.method.returnsCursor()) { result = this.executeForCursor(SQLSession, args); } else { param = this.method.convertArgsToSQLCommandParam(args); result = SQLSession.selectOne(this.command.getName(), param); if (this.method.returnsOptional() && (result == null || !this.method.getReturnType().equals(result.getClass()))) { result = Optional.ofNullable(result); } } break; //flush操作 case FLUSH: result = SQLSession.flushStatements(); break; default: throw new BindingException(\"Unknown execution method for: \" + this.command.getName()); } if (result == null && this.method.getReturnType().isPrimitive() && !this.method.returnsVoid()) { throw new BindingException(\"Mapper method '\" + this.command.getName() + \" attempted to return null from a method with a primitive return type (\" + this.method.getReturnType() + \").\"); } else { return result; } } 我的selectPersonById方法是SELECT,就看看SELECT执行的逻辑吧： case SELECT: //如果方法没有返回值 if (this.method.returnsVoid() && this.method.hasResultHandler()) { this.executeWithResultHandler(SQLSession, args); result = null; } //如果方法返回集合 else if (this.method.returnsMany()) { result = this.executeForMany(SQLSession, args); } //如果方法返回map else if (this.method.returnsMap()) { result = this.executeForMap(SQLSession, args); } //返回游标类 else if (this.method.returnsCursor()) { result = this.executeForCursor(SQLSession, args); } else { //正常普通返回值 //抓换 param = this.method.convertArgsToSQLCommandParam(args); result = SQLSession.selectOne(this.command.getName(), param); if (this.method.returnsOptional() && (result == null || !this.method.getReturnType().equals(result.getClass()))) { result = Optional.ofNullable(result); } } 锁定最后一个 selectOne 方法: public List selectList(String statement, Object parameter, RowBounds rowBounds) { List var5; try { //获取mapperStatement MappedStatement ms = this.configuration.getMappedStatement(statement); //执行器执行 var5 = this.executor.query(ms, this.wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception var9) { throw ExceptionFactory.wrapException(\"Error querying database. Cause: \" + var9, var9); } finally { ErrorContext.instance().reset(); } return var5; } 可以看到SQLSession的select还是代理到Executor的query方上了: public List query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { //获取SQL BoundSQL boundSQL = ms.getBoundSQL(parameter); //创建二级缓存的key,这个key非常长 CacheKey key = this.createCacheKey(ms, parameter, rowBounds, boundSQL); //查询 return this.query(ms, parameter, rowBounds, resultHandler, key, boundSQL); } 继续深入重载的query方法: public List query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSQL boundSQL) throws SQLException { ErrorContext.instance().resource(ms.getResource()).activity(\"executing a query\").object(ms.getId()); if (this.closed) { throw new ExecutorException(\"Executor was closed.\"); } else { //上一个缓存已经查询完成,并且标注了FlushCache=true的属性,那么清空本地缓存 if (this.queryStack == 0 && ms.isFlushCacheRequired()) { this.clearLocalCache(); } List list; try { ++this.queryStack; //如果没有指定resultHandler,那么线程本地缓存查询 list = resultHandler == null ? (List)this.localCache.getObject(key) : null; if (list != null) { //如果有缓存,就覆盖当前参数值,但只针对CALLABLE this.handleLocallyCachedOutputParameters(ms, key, parameter, boundSQL); } else { //从数据库查询 list = this.queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSQL); } } finally { --this.queryStack; } if (this.queryStack == 0) { Iterator var8 = this.deferredLoads.iterator(); while(var8.hasNext()) { BaseExecutor.DeferredLoad deferredLoad = (BaseExecutor.DeferredLoad)var8.next(); deferredLoad.load(); } this.deferredLoads.clear(); //查询完清除缓存 if (this.configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) { this.clearLocalCache(); } } return list; } } 继续深入 queryFromDatabase 方法,看Executor到底是怎么查询的: private List queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSQL boundSQL) throws SQLException { this.localCache.putObject(key, ExecutionPlaceholder.EXECUTION_PLACEHOLDER); List list; try { //又是一个接口方法,有需要深入 list = this.doQuery(ms, parameter, rowBounds, resultHandler, boundSQL); } finally { this.localCache.removeObject(key); } this.localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) { this.localOutputParameterCache.putObject(key, parameter); } return list; } 继续看doQuery方法,因为我没有指定Executor的类型,所以这个doQuery肯定是在SimpleExecutor中了: public List doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSQL boundSQL) throws SQLException { Statement stmt = null; List var9; try { Configuration configuration = ms.getConfiguration(); //来了,又是一个核心对象 StatementHandler handler = configuration.newStatementHandler(this.wrapper, ms, parameter, rowBounds, resultHandler, boundSQL); stmt = this.prepareStatement(handler, ms.getStatementLog()); var9 = handler.query(stmt, resultHandler); } finally { this.closeStatement(stmt); } return var9; } 在doQuery方法中根据Configuration创建了StatementHandler,它是SQL的处理器,看看Configuration是怎么创建它的: public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSQL boundSQL) { //RoutingStatementHandler StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSQL); //再次对插件进行了封装 StatementHandler statementHandler = (StatementHandler)this.interceptorChain.pluginAll(statementHandler); return statementHandler; } 上面最重要的创建RoutingStatementHandler那句代码,RoutingStatementHandler是个什么东西,看样子它是路由的StatementHandler,岂不是创建各种类型的StatementHandler: public RoutingStatementHandler(Executor executor, MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSQL boundSQL) { switch(ms.getStatementType()) { //普通的StatementHandler case STATEMENT: this.delegate = new SimpleStatementHandler(executor, ms, parameter, rowBounds, resultHandler, boundSQL); break; //预编译StatementHandler,也就是预编译的SQL case PREPARED: this.delegate = new PreparedStatementHandler(executor, ms, parameter, rowBounds, resultHandler, boundSQL); break; //CALLABLE类型的StatementHandler case CALLABLE: this.delegate = new CallableStatementHandler(executor, ms, parameter, rowBounds, resultHandler, boundSQL); break; default: throw new ExecutorException(\"Unknown statement type: \" + ms.getStatementType()); } } 从RoutingStatementHandler的源码可知,它负责创建不同类型的SQL的StatementHandler. 那么创建完这个StatementHandler后,有啥用呢？ 回到doQuery方法: public List doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSQL boundSQL) throws SQLException { Statement stmt = null; List var9; try { Configuration configuration = ms.getConfiguration(); //创建StatementHandler StatementHandler handler = configuration.newStatementHandler(this.wrapper, ms, parameter, rowBounds, resultHandler, boundSQL); //预编译StatemenT stmt = this.prepareStatement(handler, ms.getStatementLog()); var9 = handler.query(stmt, resultHandler); } finally { this.closeStatement(stmt); } return var9; } 可以看到下面的一个 prepareStatement 方法直接预编译出来了一个Statement,Statement相信各位同学不陌生吧,java原生的SQL操作啊,由StatementHandler预编译成Statement这个方法肯定是做些事情的,到现在还没有设置参数呢,而且参数一直都随着那几颗方法： private Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException { Connection connection = this.getConnection(statementLog); Statement stmt = handler.prepare(connection, this.transaction.getTimeout()); //设置参数 handler.parameterize(stmt); return stmt; } 看看parameterize是如何设置参数的吧: public void parameterize(Statement statement) throws SQLException { //ParameterHandler出来了,它时StatementHandler的一个属性,负责SQL的入参 this.parameterHandler.setParameters((PreparedStatement)statement); } 继续看看ParameterHandler是如何完成入参的吧： public void setParameters(PreparedStatement ps) { ErrorContext.instance().activity(\"setting parameters\").object(this.mappedStatement.getParameterMap().getId()); //从BoundSQL中获取ParameterMapping,也就是参数 List parameterMappings = this.boundSQL.getParameterMappings(); if (parameterMappings != null) { for(int i = 0; i 上面设置完参数后,回到doQuery方法,使用StatementHandler的query方法执行: public List query(Statement statement, ResultHandler resultHandler) throws SQLException { PreparedStatement ps = (PreparedStatement)statement; //执行 ps.execute(); //处理结果集 return this.resultSetHandler.handleResultSets(ps); } 最后看看ResultSetHandler是如何处理结果集的吧: public List handleResultSets(Statement stmt) throws SQLException { ErrorContext.instance().activity(\"handling results\").object(this.mappedStatement.getId()); List multipleResults = new ArrayList(); int resultSetCount = 0; //获取第一个结果集 ResultSetWrapper rsw = this.getFirstResultSet(stmt); List resultMaps = this.mappedStatement.getResultMaps(); int resultMapCount = resultMaps.size(); this.validateResultMapsCount(rsw, resultMapCount); //一个resultMap对应一个结果集,不断遍历 while(rsw != null && resultMapCount > resultSetCount) { ResultMap resultMap = (ResultMap)resultMaps.get(resultSetCount); //处理结果集 this.handleResultSet(rsw, resultMap, multipleResults, (ResultMapping)null); //获取下一个结果集 rsw = this.getNextResultSet(stmt); this.cleanUpAfterHandlingResultSet(); ++resultSetCount; } String[] resultSets = this.mappedStatement.getResultSets(); if (resultSets != null) { while(rsw != null && resultSetCount 算是完成了对mybatis执行过程的一个简单的源码分析吧,由于我功力浅薄, 即使是分析出来了这么一个大致的运行流程,其中的大部分细节我仍然是不懂的,所以我会继续学习. mybatis源码总结 Mybatis最核心的对象莫过于Configuration了, Configuration在解析完配置文件和mapper文件后就一直流转于整个mybatis执行的生命周期内。 首先由Configuration创建出Executor,从而创建DefaultSQLSession, 又由Configuration内的MapperRegistry获取MapperProxy对象,执行SQL的时候, 也由Configuration创建StatementHandler,几乎可以说Configuration是无处不在。 然后说下Mybatis核心的组件:Executor。 负责调度StatementHandler。 StatementHandler负责调度ParameterHandler对Statement 进行参数处理,执行Statement调用ResultSetHandler对SQL执行的结果做出封装。 ParameterHandler负责Statement的参数处理, ResultSetHandler负责Statement执行后的结果集处理. 在分析mybatis源码的过程中,我觉得mybatis整个框架的设计和面向对象的思想是发挥的淋漓尽致的。 其实有很多人说mybatis不够智能化,但是我想说的是, Mybatis帮我们做掉这么多繁琐的事情,还能让我们灵活的掌握SQL，在设计上实属np。 听说Hibernate不需要写SQL,我没学过,不好妄下定论。 但是我觉得SQL本身就不属于Java语言这个范畴,如果连SQL都不需要写,是什么ORM框架,又怎么谈SQL优化呢? "},"gitbook_doc/spring-learning/SpringFramework常见知识点.html":{"url":"gitbook_doc/spring-learning/SpringFramework常见知识点.html","title":"SpringFramework常见知识点","keywords":"","body":" Spring常见知识点 什么是Spring Framework? Spring的优缺点 Spring的优点 Spring的缺点 Spring 主要提供了哪些模块? Spring主要使用了哪些设计模式? Spring IOC容器的配置方式有哪些？ BeanFactory和ApplicationContext的区别是什么? 什么是IOC容器和DI依赖注入? Spring依赖注入的方式有几种? 一个bean的定义包含了什么?(BeanDefinition) bean的作用域有哪些? Spring 的扩展点主要有哪些? Spring如何解决循环依赖? 事务的传播行为是什么?有哪些? 什么是AOP? AOP的组成元素和概念有哪些? AOP实现方式有哪些? AspectJ AOP 和 Spring AOP的区别? cglib动态代理和jdk动态代理的区别? Spring常见知识点 什么是Spring Framework? Spring是一个轻量级的，开源的Java应用程序开发框架。它提供的IOC和AOP等核心功能，能够使开发者很方便的开发出松耦合的应用。 Spring的优缺点 Spring的优点 方便解耦，简化开发：对象统一交由容器管理，实现了资源的可配置和易管理。 并且不再需要显示的编写管理对象的代码，降低了应用的代码量。 AOP支持：Spring 提供 AOP模块，能够很方便的编写出AOP程序. 声明式事务：只需要通过配置或注解就可以完成对事务的支持，而不需要手动的编写事务代码 第三方框架无缝集成：Spring可以很方便的将第三方框架继承到系统中，很灵活。 ... Spring的缺点 复杂：Spring发展到现在，确认有些复杂了，但是对于它解决的问题来说，复杂已经不算什么了。 效率：Spring内部依赖反射，而反射会带来一定的效率损耗。 Spring 主要提供了哪些模块? core模块提供IOC和DI等核心功能 aop模块提供面向切面编程的实现 web模块提供对web应用的支持 dao模块提供数据库方面的支持 test模块提供测试方面的支持 Spring主要使用了哪些设计模式? Spring使用的设计模式有很多，此处只列举几个常见的 工厂模式 BeanFactory 就是简单工厂的实现，用来创建和获取Bean 单例模式 Spring容器的Bean默认是单例的 代理模式 aop使用的就是代理模式 模板方法模式 jdbcTemplate等就使用模板方法模式 观察者模式 当有事件触发时，该事件的监听者(观察者)就会做出相应的动作。 ... Spring IOC容器的配置方式有哪些？ xml (不再推荐使用) 注解(推荐使用) Java API(和注解一起使用) BeanFactory和ApplicationContext的区别是什么? BeanFactory是最底层，最顶级的IOC容器接口，它提供了对bean的基本操作，属于低级容器。 而ApplicationContext是BeanFactory的应用扩展接口， 提供了比BeanFactory更多高级的功能和扩展接口，属于高级容器。 什么是IOC容器和DI依赖注入? IOC(Inversion Of Control): 控制反转. DI(Dependencies Inject): 依赖注入. Spring IOC容器是Spring框架的核心功能， 它负责管理用户定义好的bean以及bean的生命周期，包括(创建，初始化,使用和销毁)， 而依赖注入是处理bean与bean之间的依赖关系。 控制反转是指原本由用户来管理对象，现在交由容器管理， 不再需要我们手动去处理对象之间的依赖关系了。 Spring依赖注入的方式有几种? setter方法注入: 通过构造器或工厂方法(静态工厂方法或实例bean工厂方法)构造bean所需要的依赖后， 使用setter方法设置bean的依赖。 构造器注入: 构造器的每个参数都可以代表对其他bean的依赖。 一个bean的定义包含了什么?(BeanDefinition) BeanDefinition 是对Bean的定义，它定义了Bean的元数据， 如Bean的Scope，Class，beanName，bean的实例化方式等等。 bean的作用域有哪些? singleton(单例bean): singleton作用域表示在容器中，一个bean只存在一个实例。 每次获取这个bean，都是获取它唯一的实例。 prototype(原型bean): prototype作用域表示如果有一个bean是prototype scope, 那么每次获取该作用域的bean时，容器都会新创建该bean的实例。 request(请求域): 作用于Web应用。request作用域表示如果一个bean是request scope， 那每次HTTP请求，容器都会创建一个该bean的实例。 session(会话域): 作用于Web应用。session作用域表示如果一个bean是session scope， 那么容器为每个session创建一个该bean的实例，当session销毁时，该session内的bean也就销毁了。 application(web应用作用域): 作用于web应用。 application作用域表示如果一个bean是 application scope的， 那么容器会为整个Web应用上下文创建一个该bean的实例，这个实例属于ServletContext级别的。 不同于singleton，singleton是Spring的每个ApplicationContext唯一， 而application是每个ServletContext唯一，对于Web应用来说， ServletContext也只有一个，所以application可以理解为web应用唯一。 websocket(websocket作用域): 应用于web应用。 websocket作用域表示如果一个bean是websocket scope的， 那么该bean作用域整个WebSocket作用域内，也是唯一的。 Spring 的扩展点主要有哪些? 这里列举的并不是很全，因为Spring的扩展点实在是太多了， 但究其根本，还是在bean实例化/初始化前后的扩展。 如果容器中有BeanFactoryPostProcessor,那么执行它的postProcessBeanFactory方法。 该接口是对ConfigurableListableBeanFactory的一个扩展。 实例化bean 注入bean的属性 如果容器中有Aware的实现，那么执行各种Aware扩展实现方法,如BeanNameAware, BeanFactoryAware,ApplicationContextAware等扩展的setXXX方法 如果容器中有BeanPostProcessor，那么执行BeanPostProcessor的postProcessBeforeInitialization方法 执行bean指定的init方法,如果bean还实现了InitializingBean接口, 那么继续执行InitializingBean的afterPropertiesSet方法 如果容器中有BeanPostProcessor，那么执行BeanPostProcessor的postProcessAfterInitialization方法 bean可以被使用了 容器销毁后，执行bean指定的destroy方法，如果bean还实现了DisposableBean接口， 那么继续执行DisposableBean的destroy方法 Spring如何解决循环依赖? 循环依赖是指:A依赖B，并且B依赖A的情况。或者 A依赖B，B依赖C，C依赖A的情况。 构造器注入的循环依赖无法解决，直接抛出BeanCurrentlyInCreationException异常。 容器在创建Bean的时候，会将Bean添加到正在创建的Bean池中，如果在创建Bean的时候， 发现自己已经在创建的Bean池中，就说明Bean陷入循环依赖了， 直接抛出BeanCurrentlyInCreationException异常。 为什么构造器注入不能像Setter方法注入一样解决循环依赖问题? 因为Setter方法注入的前提是首先需要实例化这个对象，而构造器注入的参数正是bean， 怎么实例化，所以无法解决这个问题。 Setter方法注入的循环依赖可以通过缓存解决。 三级缓存： 初始化完成的Bean池。 实例化完成，但是没有填充属性的Bean池。 刚刚实例化完成的Bean的工厂缓存，用于提前曝光Bean。 Setter方法注入时，如果Bean A发现自己依赖于Bean B， 那么将自己实例化后并添加到第三级缓存(Bean 工厂)。 然后再初始化B,检查到B又依赖于A，于是到三级缓存里查询A,那么查询肯定是成功的, 于是将A设置为B的属性。当A初始化时， 发现B已经初始化完成,就可以直接将B设置为A的属性了。 非单例bean不能缓存，无法解决循环依赖: IOC容器是不会缓存非单例bean的，所以无法解决循环依赖问题。 事务的传播行为是什么?有哪些? 事务的传播行为是Spring提供的对事务增强的一种特性，不属于数据库事务特性。 事务的传播行为描述的是当多个事务同时存在时，这些事务该如何被处理。 Spring定义了7种事务的传播行为:REQUIRED,SUPPORTS,MANDATORY,REQUIRES_NEW,NOT_SUPPORTED,NEVER,NESTED REQUIRED: 当一个方法A(REQUIRED)被另一个方法B调用时，如果B已经开启了事务，那么A就加入到B的事务中去， A和B要么同时成功，要么同时失败。如果B没有开启事务，那么A将自己开启新的事务,独立运行。 REQUIRES_NEW: 当一个方法A(REQUIRE_NEW)被非事务方法调用时，A会自己开启事务。 如果A被另一个方法B调用时，无论B是否开启事务，A都会开启自己的事务，且会挂起B的事务， 然后执行A，A提交后才会恢复B，这样外层的B即使失败也不会影响A，但是如果A失败了，B也会回滚。 NESTED: 如果方法A(NESTED)被另一个方法B调用，如果B已经开启了事务， 那么A将作为B的子事务运行，B失败，A也失败，但是A失败却不影响B。A是作为B的嵌套事务运行的， 所以并不会影响B。如果B没有事务，那么A将自己新开启一个事务运行。 PS:NESTED和REQUIRES_NEW很相似，但是可以理解为他们是相反的，NESTED的方法作为嵌套事务运行在 外层事务内部，外层事务失败则内层事务也失败，但内层事务失败却不影响外层事务；REQUIRES_NEW则是 自己开启自己的事务，外层事务失败也不影响内层事务，但内层事务失败会导致外层事务回滚。 SUPPORTS: 当一个方法A(SUPPORTS)被另一个方法B调用时，如果B开启了事务， 那么A就加入到B的事务中去。如果B没有开启事务,那就以非事务方式运行执行。 MANDATORY: 当一个方法A(MANDATORY)被另一个方法B调用时，如果B没有开启事务，那么A将抛出异常， 如果B开启了事务，则A加入到B中共用事务。 NOT_SUPPORTED: 当一个方法A(NOT_SUPPORTED)被另一个方法B调用时，如果B开启了事务，那么B的事务将挂起， 直到A执行完，B再以事务的方式运行。 NEVER: 当一个方法A(NEVER)被另一个方法B调用时，如果B开启了事务，那么A将抛出异常。 什么是AOP? AOP(Aspect Oriented Programming)面向切面编程，个人认为AOP是一种程序设计思想。 在AOP编程中，将系统的核心逻辑和辅助逻辑分离开来，并将通用的辅助逻辑封装成一个模块， 提高了代码的重用性和程序的可维护性，降低了系统模块之间的耦合度。 AOP的组成元素和概念有哪些? 连接点(join point): 连接点指程序执行的某个位置，能够执行辅助逻辑(通知/增强)。 如方法执行前，方法抛出异常时，方法执行完，方法返回后等等，这些点都被称为连接点。 通知/增强(advice): 通知/增强 可以理解为辅助逻辑，就是在连接点要做的事情。 切点(pointcut): 连接点可以看做是一个方法的执行辅助逻辑的不同位置的集合， 切入点指的就是这个方法。切入点会匹配 通知/增强 需要作用的类或方法。 切面(aspect): 切面是切入点的集合，可以看作是拥有多个切入点的类。 织入(weave): 织入是一个概念。它描述的是将切入点的 通知/增强 应用到连接点的过程。 AOP实现方式有哪些? 常见的AOP实现的方式有代理和织入。 代理分为静态代理和动态代理。 由于静态代理没有动态代理灵活，所以现在几乎都使用动态代理来实现AOP。 以动态代理实现AOP的框架主要有cglib和jdk原生的这2种。 织入可以理解为以操作字节码的方式对class源文件进行修改，从而实现通知/增强。 以织入实现AOP的框架主要有AspectJ。 AspectJ AOP 和 Spring AOP的区别? AspectJ AOP: AspectJ是一整套AOP的工具，它提供了切面语法(切入点表达式)以及织入等强大的功能。 Aspect提供文件和注解2种方式来进行AOP编程， 并且它允许在编译时，编译后和加载时织入， 但是需要使用它特定的ajc编译器才能实现织入这一功能。 Spring AOP: Spring AOP吸收了AspectJ的优点，采用了AspectJ的切入点语法以及AspectJ式的注解， 但却并未使用AspectJ的一整套工具, 而是集cglib和jdk于一体(动态代理)的方式来实现AOP功能，真的很强。 cglib动态代理和jdk动态代理的区别? jdk动态代理: jdk只提供基于接口式的动态代理来对目标进行增强。 cglib动态代理: cglib则是使用字节码技术，动态生成目标的子类，以继承的方式来对目标方法进行重写， 所以如果方法是final的，那么cglib将无法对方法进行增强。 在SpringAOP 中，如果目标类实现了接口，那么默认使用jdk动态代理来实现AOP， 如果目标类没有实现接口，那么将使用cglib来实现AOP。 "},"gitbook_doc/spring-learning/SpringMVC常见知识点.html":{"url":"gitbook_doc/spring-learning/SpringMVC常见知识点.html","title":"SpringMVC常见知识点","keywords":"","body":" Spring MVC常见知识点及源码解析 MVC 是什么 / 有什么优点? 什么是 Spring MVC? Spring MVC的优缺点? 什么是DispatcherServlet? Spring MVC有哪些组件?(见:DispatcherServlet源码) 简述SpringMVC原理/执行流程 Spring MVC 拦截器是什么 / 有什么作用 / 与 Filter有什么区别? @Component @Controller @Service @Repository 区别? Spring MVC常见知识点及源码解析 MVC 是什么 / 有什么优点? MVC是一种设计模式，遵循 模型(Model),视图(View) 和 控制器(Controller)的架构设计。 MVC的优点很明显: 应用层次分明，职责分明，使得系统的耦合性降低，并有利于系统的维护。 什么是 Spring MVC? Spring MVC是一个基于Spring框架的轻量级的MVC Web应用框架。 Spring MVC的优缺点? 优点： 基于Spring，拥有Spring的所有优点 Spring MVC中的组件:角色分明，耦合性低，非常有利于应用的维护。 支持多种视图技术:不仅支持JSP，还支持各种模板视图。 功能特性强大:轻松的文件上传，数据校验与格式转换，异常处理，RESTful风格的API等等。 支持前后端分离。 缺点: Spring MVC是基于Spring的，这既是它的优点也是它的缺点，它必须与Spring一起使用， 个人认为这应该是它最大的一个限制了。 什么是DispatcherServlet? DispatcherServlet是Spring MVC的核心, 可以说SpringMVC就是这个DispatcherServlet。 它是一个Servlet，负责拦截所有的请求，并以调用各种组件来对请求进行分发并处理。 Spring MVC有哪些组件?(见:DispatcherServlet源码) MultipartResolver: 核心组件之一，处理文件上传请求。MultipartResolver负责判断普通请求是否为文件上传请求， 并将普通请求(HttpServletRequest)解析为文件上传请求(MultipartHttpServletRequest)。 LocalResolver: 区域解析器。它主要被用于国际化的资源方面的解析。 ThemeResolver: 主题资源解析器。SpringMVC允许用户提供不同主题，主题就是一系列资源的集合使用这些主题可以提高用户体验。 ViewResolver: 核心组件之一，视图解析器。在Handler执行完请求后，ViewResolver将ModelAndView解析成物理视图， 并对物理视图进行model渲染。 HandlerMapping: 核心组件之一，请求处理器。根据用户的请求来匹配对应的Handler。 HandlerAdapter: 核心组件之一，Handler适配器。使用Handler处理请求，并返回处理后的视图(ModelAndView)。 HandlerExceptionResolver: 核心组件之一，异常处理解析器。在Handler执行请求的过程中可能出现异常， HandlerExceptionResolver就负责处理Handler执行请求过程中的异常。 RequestToViewNameTranslator: 核心组件之一,请求到视图的转换器。根据Request设置最终的视图名,当Handler执行完请求后，它会将Request解析成视图名。 FlashMapManager: 核心组件之一，在请求进行重定向时，FlashMapManager用于保存请求中的参数。 简述SpringMVC原理/执行流程 用户发出的请求被DispatcherServlet拦截。 DispatcherServlet使用HandlerMapping根据请求匹配到相应的Handler。Handler实际上是一个(HandlerMethod)。 DispatcherServlet根据Handler适配合适的HandlerAdapter。 HandlerAdapter使用Handler执行请求，并返回ModelAndView。 使用RequestToViewNameTranslator,HandlerExceptionResolver和ViewResolver等解析器， 解析并渲染ModelAndView，并处理相关异常信息。 渲染后的结果反馈给用户。 Spring MVC 拦截器是什么 / 有什么作用 / 与 Filter有什么区别? HandlerInterceptor: Spring MVC拦截器是Spring MVC提供的对用户请求的目标资源做出拦截扩展的处理器。 它允许在目标方法执行前后以及View渲染后做出处理。 Servlet Filter: Filter是Servlet提供的过滤器，它会在目标方法执行前后做出拦截处理。 要说Servlet Filter和HandlerInterceptor有啥区别， 个人认为除了它们提供的拦截时机不同，目的都是相同的，没啥区别。 @Component @Controller @Service @Repository 区别? @Component 声明一个类为IOC容器的组件，会被IOC容器管理。 而@Controller,@Service和@Repository则拥有更细分的语义。 @Controller通常用于Web应用，被@Controller注解的类，应该作为一个处理请求的控制器。 @Service则是声明一个类为Service类，处理业务逻辑。 被@Repository注解的类，应该被用于处理与数据库交互和持久化相关的功能。 "},"gitbook_doc/spring-learning/SpringMVC源码分析.html":{"url":"gitbook_doc/spring-learning/SpringMVC源码分析.html","title":"SpringMVC源码分析","keywords":"","body":"Spring的源码分析 在分析SpringMVC源码之前我想先回顾一下JavaWeb的知识.JavaWeb的核心是Servlet,一个Servlet对应一个URL, 每次一个Http请求访问,那么对应URL的Servlet就会调用service方法处理。 其实这里我是对SpringMVC的一个复习,所以我先说说就我目前SpringMVC的理解吧。 大家都知道SpringMVC是一个MVC框架,但它还是脱离不了Tomcat,Undertow,Jetty这样的Servlet容器, 因为SpringMVC的核心还是是Servlet。 在初学SpringMVC的时候,各位同学可能都在web.xml里配置过DispatcherServlet,可能当时都没有想过为什么要去配置这个类, 甚至把它拦截的url配置成/**,我当时确实也没有想过,不过后来在学习的时候,已经明白了为什么这样去做, 并且已经明白了SpringMVC的设计思想。 前方高能 SpringMVC通过一个DispatcherServlet拦截所有请求,也就是url为 /** 。 通过拦截所有请求,在内部通过路由匹配的方式把请求转给对应Controller的某个RequestMapping处理。 这就是SpringMVC的基本工作流程,我们传统的JavaWeb是一个Servlet对应一个URL, 而DispatcherServlet是一个Servlet拦截全部URL,并做分发处理. 这也是SpringMVC的设计的精妙之处。 当然上面只是简单的一个流程,在这个过程中肯定有很多细节值得我们细细揣摩。 我是以SpringBoot搭建的调试环境,再加上已经知晓DispatcherServlet是核心, 几乎可以直接定位到这个类了,但是在这之前可以看看DispatcherServlet的父类:FrameworkServlet。 上面说过,Servlet是以service方法处理请求的,所以直接定位到FrameworkServlet的service方法: protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { HttpMethod httpMethod = HttpMethod.resolve(request.getMethod()); if (httpMethod != HttpMethod.PATCH && httpMethod != null) { super.service(request, response); } else { //就是你了,骚年 this.processRequest(request, response); } } 一个明显的方法 processRequest就进入了眼帘,看看这个processRequest做了什么吧: protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { ... try { //就是这个方法了 this.doService(request, response); } catch (IOException | ServletException var16) { failureCause = var16; throw var16; } catch (Throwable var17) { failureCause = var17; throw new NestedServletException(\"Request processing failed\", var17); } finally { this.resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) { requestAttributes.requestCompleted(); } this.logResult(request, response, (Throwable)failureCause, asyncManager); this.publishRequestHandledEvent(request, response, startTime, (Throwable)failureCause); } } 可以看到一大坨代码都是try这个doService方法,而这个doService方法肯定是核心了, 但是这个doService方法在FrameworkServlet类中是个abstract方法,所以直接回到 DispatcherServlet找到doService方法： protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception { //...此处省略一大坨看不懂的代码 try { //从这里开始分析 this.doDispatch(request, response); } finally { if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted() && attributesSnapshot != null) { this.restoreAttributesAfterInclude(request, attributesSnapshot); } } } 已经找到了目标:doDispatch,听名字这个方法就是做事情的方法了, 所谓的做事情当然就是处理request了,如果各位同学学到现在, 肯定有一个意识:在SpringMVC框架中,所有的核心方法几乎都是do开头, 并且都有2个必要的参数:HttpServletRequest和HttpServletResponse. 但是DispatcherServlet是我接触的所有框架依赖不那么扯的, 就是这个doDispatch方法,它里面几乎包含了我们即将要学习的所有知识了, 所以接下来的核心就是doDispatch方法,各位同学在做源码阅读的时候,建议一行也不要放过(实在看不懂就别为难自己了^-^)。 不过在看DispatcherServlet源码之前,我们最后看下这个DispatcherServlet究竟何德何能, 能够这么厉害,可以完成我们几乎所有的需求方法: public class DispatcherServlet extends FrameworkServlet { .... @Nullable private MultipartResolver multipartResolver; @Nullable private LocaleResolver localeResolver; @Nullable private ThemeResolver themeResolver; @Nullable private List handlerMappings; @Nullable private List handlerAdapters; @Nullable private List handlerExceptionResolvers; @Nullable private RequestToViewNameTranslator viewNameTranslator; @Nullable private FlashMapManager flashMapManager; @Nullable private List viewResolvers; .... } 这就是DispatcherServlet的九大组件,正是这九大组件, DispatcherServlet才能在支持请求和相应的同时,对许多功能和细节做出完善。 再看看doDispatch方法的源码,其实都是上面组件之间的配合完成任务: protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; //核心对象:HanderExecutorChain HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; //管理异步请求 WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { try { ModelAndView mv = null; Object dispatchException = null; try { //检查请求是否存在文件上传 processedRequest = this.checkMultipart(request); multipartRequestParsed = processedRequest != request; //getHandler方法是根据请求获取对应的Controller和方法 mappedHandler = this.getHandler(processedRequest); if (mappedHandler == null) { //如果没有获取到匹配的handler,就要么抛出异常要么设置响应码404 this.noHandlerFound(processedRequest, response); return; } //通过handler获取HandlerAdapter,由HandlerAdapter完成方法的执行 HandlerAdapter ha = this.getHandlerAdapter(mappedHandler.getHandler()); String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if ((new ServletWebRequest(request, response)).checkNotModified(lastModified) && isGet) { return; } } //执行拦截器的preHandle方法,它内部如果执行不成功就会先执行afterCompletion,然后返回false,然后程序就退出 //这也是为什么拦截器的前置方法为什么返回false,程序就不会执行我们的逻辑了,等下会分析源码 if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } //执行方法 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } //根据Request使用RequestToViewNameTranslator设置默认的视图 this.applyDefaultViewName(processedRequest, mv); //倒序执行拦截器的postHandle方法 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception var20) { dispatchException = var20; } catch (Throwable var21) { dispatchException = new NestedServletException(\"Handler dispatch failed\", var21); } //解析并渲染ModelAndView this.processDispatchResult(processedRequest, response, mappedHandler, mv, (Exception)dispatchException); } catch (Exception var22) { //发生异常会倒序执行afterCompletion方法 this.triggerAfterCompletion(processedRequest, response, mappedHandler, var22); } catch (Throwable var23) { //发生异常会倒序执行afterCompletion方法 this.triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", var23)); } } finally { if (asyncManager.isConcurrentHandlingStarted()) { if (mappedHandler != null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else if (multipartRequestParsed) { this.cleanupMultipart(processedRequest); } } } 首先看看MultiPartResolver是如何检查文件上传请求的吧(checkMultipart方法): protected HttpServletRequest checkMultipart(HttpServletRequest request) throws MultipartException { //关键在isMultpart方法 if (this.multipartResolver != null && this.multipartResolver. isMultipart(request) ) { if (WebUtils.getNativeRequest(request, MultipartHttpServletRequest.class) != null) { if (request.getDispatcherType().equals(DispatcherType.REQUEST)) { this.logger.trace(\"Request already resolved to MultipartHttpServletRequest, e.g. by MultipartFilter\"); } } else if (this.hasMultipartException(request)) { this.logger.debug(\"Multipart resolution previously failed for current request - skipping re-resolution for undisturbed error rendering\"); } else { try { //执行到这里就说明HttpServletRequest是一个文件上传请求,那么就把HttpServletRequest包装成 //MultpartHttpServletRequest返回 return this.multipartResolver.resolveMultipart(request); } catch (MultipartException var3) { if (request.getAttribute(\"javax.servlet.error.exception\") == null) { throw var3; } } this.logger.debug(\"Multipart resolution failed for error dispatch\", var3); } } return request; } 先看看isMultiPart是怎么判断请求是否为文件上传请求的吧(isMultipart方法)： public boolean isMultipart(HttpServletRequest request) { //判断Http请求包头的Content-Type return StringUtils.startsWithIgnoreCase(request.getContentType(), \"multipart/\"); } 着重看看MultipartResolver是如何把普通请求包装成MultipartHttpServletRequest的吧(resolveMultipart方法): public MultipartHttpServletRequest resolveMultipart(HttpServletRequest request) throws MultipartException { //原来我们处理文件上传时拿到的请求是这个请求 return new StandardMultipartHttpServletRequest(request, this.resolveLazily); } 从上面代码分析出,文件上传时,拿到的请求被包装成了StandardMultipartHttpServletRequest, 但是我不再深入,因为MultipartResolver是DispatcherServlet的一个 组件而已,再深入,就跑题了...也不是说跑题,毕竟都是SpringMVC的组成, 只是相信各位同学看到这里已经有能力去看看这个源码了, 可以明确的告诉各位这个地方不存在什么封装之类的曲折,单纯的就是 StandardMultipartHttpServletRequest内部对请求做了解析,并使用集合把解析的结果存储起来了而已 . 回到doDispatch方法,在执行完checkMultipart方法后,就通过getHandler方法, 获取了mappedHandler(HandlerExecutionChain)对象,这个HandlerExecutionChain是什么, 为什么getHandler返回它?先看看HandlerExecutionChain类的源码吧: public class HandlerExecutionChain { private static final Log logger = LogFactory.getLog(HandlerExecutionChain.class); //handler,至于为什么设计成Object类型,我想可能是handler是执行method的关键,所以隐藏了细节,不对外暴露真实类型. //也有可能是在适配Controller的method的过程中需要多种类型的转换,总之不管哪种理由设计成Object类型,只有好好研究了. private final Object handler; @Nullable //一堆 HandlerInterceptor private HandlerInterceptor[] interceptors; @Nullable private List interceptorList; private int interceptorIndex; public HandlerExecutionChain(Object handler) { this(handler, (HandlerInterceptor[])null); } ... } 这个HandlerExecutionChain包含了handler和它的所有HandleInterceptor. 再看看DispatcherServlet是如何根据HttpServletRequest获取HandlerExecutionChain的吧(getHandler方法): protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { if (this.handlerMappings != null) { //之前说过HandlerMapping是DispatcherServlet的9大组件之一,所以这里它遍历HandlerMapping来获取 Iterator var2 = this.handlerMappings.iterator(); while(var2.hasNext()) { HandlerMapping mapping = (HandlerMapping)var2.next(); //调用HandlerMapping的getHandler方法来获取HandlerExecutionChain HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) { return handler; } } } return null; } 继续看看HandlerMapping的getHandler怎么实现的: public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { //首先就获取Handler Object handler = this.getHandlerInternal(request); if (handler == null) { handler = this.getDefaultHandler(); } if (handler == null) { return null; } else { if (handler instanceof String) { String handlerName = (String)handler; handler = this.obtainApplicationContext().getBean(handlerName); } //这一步就构造了HandlerExecutionChain HandlerExecutionChain executionChain = this.getHandlerExecutionChain(handler, request); if (this.logger.isTraceEnabled()) { this.logger.trace(\"Mapped to \" + handler); } else if (this.logger.isDebugEnabled() && !request.getDispatcherType().equals(DispatcherType.ASYNC)) { this.logger.debug(\"Mapped to \" + executionChain.getHandler()); } if (this.hasCorsConfigurationSource(handler)) { CorsConfiguration config = this.corsConfigurationSource != null ? this.corsConfigurationSource.getCorsConfiguration(request) : null; CorsConfiguration handlerConfig = this.getCorsConfiguration(handler, request); config = config != null ? config.combine(handlerConfig) : handlerConfig; executionChain = this.getCorsHandlerExecutionChain(request, executionChain, config); } return executionChain; } } 在说上面一段代码之前,先回顾下, 之前介绍了HandlerExecutionChain由Handler和HandlerInterceptor组成, 那么我们就需要在getHandler里找到这2个关键的地方. 而第一行的getHandlerInternal方法就获取到了Handler, 后面又通过getHandlerExecutionChain直接构造出来了HandlerExecutionChain, 所以可以肯定:getHandlerInternal是根据URL映射找到了Handler getHandlerExecutionChain通过HandlerInterceptor和Handler构造了HandlerExecutionChain对象, 至于我为什么这么肯定, 自然是因为我已经阅读过了源码啦...这里留个空子,希望有缘看到这里的同学能够自己看看实现的细节. 回到doDispatch方法,在获取到handler(HandlerExecutionChain)后, 紧接着判断获取的handler是否为空,如果为空,说明url不对, 没有匹配的handler,就会调用noHandlerFound方法处理. 接下来 DispatcherServlet调用了getHandlerAdapter方法获取了又一个9大组件, 开始的时候介绍过:HandlerMapping是通过Request获取匹配的handler(HandlerExecutionChain)对象, 而HandlerAdapter才是指挥handler干活的, 所以这一步相当重要,看看这个getHandlerAdapter是如何获 HandlerAdapter的吧： protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException { if (this.handlerAdapters != null) { Iterator var2 = this.handlerAdapters.iterator(); while(var2.hasNext()) { HandlerAdapter adapter = (HandlerAdapter)var2.next(); //判断HandlerAdapter是否支持handler if (adapter.supports(handler)) { return adapter; } } } throw new ServletException(\"No adapter for handler [\" + handler + \"]: The DispatcherServlet configuration needs to include a HandlerAdapter that supports this handler\"); } 可以看到getHandlerAdapter方法遍历DispatcherServlet的HandlerAdapter集合, 并且调用 HandlerAdapter 的 supports方法来选择合适的 HandlerAdapter,看看 supports 方法具体实现: public final boolean supports(Object handler) { //这里的主要是第一个判断: handler instanceof HandlerMethod return handler instanceof HandlerMethod && this.supportsInternal((HandlerMethod)handler); } //鉴于方便,我直接把supportsInternal方法贴过来 protected boolean supportsInternal(HandlerMethod handlerMethod) { return true; } 可以看到supportsInternal方法对于HandlerMethod类型的参数直接返回true,当然这只是一个HandlerAdapter的实现,但也足够说明问题了,最重要的是第一个判断: handler instanceof HandlerMethod 我们之前说过handler在HandlerExecutionChain内部是一个Object类型, 到了这里为什么就变成了HandlerMethod类型了。 其实早在getHandler那一步就变成了Handler, 到这一步只是适配确认一下.那来看看HandlerMethod是什么吧, 它为什么可以执行我们的RequestMapping方法呢? public class HandlerMethod { ... //bean既可以作为Controller,也可以作为Controller的name private final Object bean; //Controller所在的容器 private final BeanFactory beanFactory; //Controller的 Class private final Class beanType; //关键之处:requestmapping 对应的方法 private final Method method; //桥接方法,我google了下,他是起兼容作用的,应该是与method有互补作用 //这是那位大神的分析:[bregedmethod]( https://www.cnblogs.com/guangshan/p/4661305.html ) private final Method bridgedMethod; //方法的参数 private final MethodParameter[] parameters; //Http状态码 private HttpStatus responseStatus; //不得不说Spring真是流比,像我等之流,只能仰望,返回状态码还得给个原因 private String responseStatusReason; //保留的一份HandlerMethod,这个是解析当前HttpMethod实例的那个HttpMethod private HandlerMethod resolvedFromHandlerMethod; .... } 其实到这里基本就知道SpringMVC到底是怎么通过反射执行我们的方法了, 还是不断的封装和反射,对于这些框架来说,万物皆可封装, 你不服就封装的你服,不管你服不服,我是服了. 再回到doDispatcher方法吧,这个时候已经获取到了需要的handler了, 那么就该执行拦截器的preHandle方法了吧,看接下来的applyPreHandle方法: boolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception { HandlerInterceptor[] interceptors = this.getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for(int i = 0; i 再回到doDispatcher方法,执行完拦截器的preHandle方法后 ,就直接调用HandlerAdapter的handle方法了： public final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //调用了handleInternal方法 return this.handleInternal(request, response, (HandlerMethod)handler); } //handleInteral方法 protected ModelAndView handleInternal(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception { //检查是否支持请求的类型和是否需要session this.checkRequest(request); ModelAndView mav; if (this.synchronizeOnSession) { HttpSession session = request.getSession(false); if (session != null) { Object mutex = WebUtils.getSessionMutex(session); synchronized(mutex) { //终于执行目标方法了 ~-~ mav = this.invokeHandlerMethod(request, response, handlerMethod); } } else { mav = this.invokeHandlerMethod(request, response, handlerMethod); } } else { mav = this.invokeHandlerMethod(request, response, handlerMethod); } if (!response.containsHeader(\"Cache-Control\")) { if (this.getSessionAttributesHandler(handlerMethod).hasSessionAttributes()) { this.applyCacheSeconds(response, this.cacheSecondsForSessionAttributeHandlers); } else { this.prepareResponse(response); } } return mav; } 直接看 invokeHandlerMethod ,到底是如何执行method的: protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception { ServletWebRequest webRequest = new ServletWebRequest(request, response); ModelAndView var15; try { WebDataBinderFactory binderFactory = this.getDataBinderFactory(handlerMethod); ModelFactory modelFactory = this.getModelFactory(handlerMethod, binderFactory); ServletInvocableHandlerMethod invocableMethod = this.createInvocableHandlerMethod(handlerMethod); if (this.argumentResolvers != null) { invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); } if (this.returnValueHandlers != null) { invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); } invocableMethod.setDataBinderFactory(binderFactory); invocableMethod.setParameterNameDiscoverer(this.parameterNameDiscoverer); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); mavContainer.addAllAttributes(RequestContextUtils.getInputFlashMap(request)); modelFactory.initModel(webRequest, mavContainer, invocableMethod); mavContainer.setIgnoreDefaultModelOnRedirect(this.ignoreDefaultModelOnRedirect); AsyncWebRequest asyncWebRequest = WebAsyncUtils.createAsyncWebRequest(request, response); asyncWebRequest.setTimeout(this.asyncRequestTimeout); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.setTaskExecutor(this.taskExecutor); asyncManager.setAsyncWebRequest(asyncWebRequest); asyncManager.registerCallableInterceptors(this.callableInterceptors); asyncManager.registerDeferredResultInterceptors(this.deferredResultInterceptors); Object result; if (asyncManager.hasConcurrentResult()) { result = asyncManager.getConcurrentResult(); mavContainer = (ModelAndViewContainer)asyncManager.getConcurrentResultContext()[0]; asyncManager.clearConcurrentResult(); LogFormatUtils.traceDebug(this.logger, (traceOn) -> { String formatted = LogFormatUtils.formatValue(result, !traceOn); return \"Resume with async result [\" + formatted + \"]\"; }); invocableMethod = invocableMethod.wrapConcurrentResult(result); } invocableMethod.invokeAndHandle(webRequest, mavContainer, new Object[0]); if (asyncManager.isConcurrentHandlingStarted()) { result = null; return (ModelAndView)result; } var15 = this.getModelAndView(mavContainer, modelFactory, webRequest); } finally { webRequest.requestCompleted(); } return var15; } 上面这个代码...我哭了....^^^----^^^------^^^,总之我就看到了关于数据绑定, SpringMVC的异步核心管理器,ModelAndViewContainer(看名字就知道,容纳了当前handler的很多数据,恩...) 其实吧,我没哭,只是确实以我的功力, 还是有很多知识没有理解...好吧,我差点哭了.....(说好的轻量级框架呢?) 继续看doDispatcher,逻辑方法执行完了, RequestToViewNameTranslator 就会调用 applyDefaultViewName 方法设置默认的视图. 然后就会执行拦截器的 postHandle方法了: void applyPostHandle(HttpServletRequest request, HttpServletResponse response, @Nullable ModelAndView mv) throws Exception { HandlerInterceptor[] interceptors = this.getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for(int i = interceptors.length - 1; i >= 0; --i) { HandlerInterceptor interceptor = interceptors[i]; //执行postHandle方法 interceptor.postHandle(request, response, this.handler, mv); } } } 最后,会通过processDispatchResult方法处理最终的结果: private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, @Nullable HandlerExecutionChain mappedHandler, @Nullable ModelAndView mv, @Nullable Exception exception) throws Exception { boolean errorView = false; if (exception != null) { if (exception instanceof ModelAndViewDefiningException) { this.logger.debug(\"ModelAndViewDefiningException encountered\", exception); mv = ((ModelAndViewDefiningException)exception).getModelAndView(); } else { Object handler = mappedHandler != null ? mappedHandler.getHandler() : null; //通过HandlerExceptionResolver处理异常 mv = this.processHandlerException(request, response, handler, exception); errorView = mv != null; } } if (mv != null && !mv.wasCleared()) { //通过View的render方法渲染视图 this.render(mv, request, response); if (errorView) { WebUtils.clearErrorRequestAttributes(request); } } else if (this.logger.isTraceEnabled()) { this.logger.trace(\"No view rendering, null ModelAndView returned.\"); } if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) { if (mappedHandler != null) { mappedHandler.triggerAfterCompletion(request, response, (Exception)null); } } } 这样一份源码也算是勉勉强强过了一遍,其实我发现从这个项目的第一份源码分析起到现在, 其中的一些知识不懂不是我笨,其实顺藤蘑摸瓜倒也好寻到一些线索。 只是像这样的框架我发现我根本没有去了解它的全貌. 我说一个现象:可能有很多同学在分析一份源码的时候, 不知道从哪就蹦出来一个比较陌生的对象,你知道它大概是干什么的, 与哪些你认识的类有关,但是你就是不知道它哪来了,久而久之,就会对这些框架形成敬畏感。 其实我确实对这些框架有敬畏感,毕竟从代码量和各种设计来说,就不得不有敬畏感, 到了他们这个体量,也很难再通过HelloWorld去了解他们的全貌了。 简单总结下SpringMVC工作的流程: SpringMVC通过DispatcherServlet拦截所有的请求, 并通过HandlerMapping与指定的请求找出匹配的handler, handler实际是HandlerMethod对象。 再通过与handler适配的HandlerAdapter执行目标方法, 执行完目标方法后会返回ModelAndView对象, 最后通过ViewResolver解析ModelAndView的View视图。 "},"gitbook_doc/spring-learning/SpringBoot常见知识点.html":{"url":"gitbook_doc/spring-learning/SpringBoot常见知识点.html","title":"SpringBoot常见知识点","keywords":"","body":" SpringBoot常见知识点 什么是SpringBoot? SpringBoot的优点 SpringBoot缺点 SpringBoot的核心注解是哪个? Java API配置的好处 SpringBoot自动配置原理 SpringBoot配置文件加载顺序 SpringBoot 怎么切换生产环境和开发环境配置 SpringBoot是如何推断应用类型和main的 SpringBoot常见知识点 什么是SpringBoot? SpringBoot是Spring开源组织Pivotal为Spring应用提供的一站式解决方案， 简化了Spring应用开发的流程，并提供了非常多的相关生态组件，非常强大。 SpringBoot的优点 容易上手，编码简单: 只要学过Spring/SpringMVC，那么就能很快的编写出一个Spring应用。 内嵌Web容器: SpringBoot无需配置外部Servlet容器就可以轻松的编写出Web应用。 并且SpringBoot支持多种容器，如Tomcat，Undertow，jetty，netty等。 开箱即用，习惯大于配置: 即使你不做任何配置，也能够跑起来一个HelloWorld。 天然集成SpringCloud微服务。 SpringBoot缺点 易学难精: 说SpringBoot开启了Java EE的新时代一点也不为过，但是它为我们带来这么多好处的同时， 我们想要真正了解他的全貌，已经是很难了。虽然上手容易，但是SpringBoot的特性之多， 恐怕需要很长时间了才能琢磨清楚。 SpringBoot的核心注解是哪个? 核心注解自然是启动SpringBoot应用的那个注解啦:@SpringBootApplication。 它由 @SpringBootConfiguration(@Configuration) @EnableAutoConfiguration @ComponentScan 3个核心注解组成。 @SpringBootConfiguration等同于@Configuration,它声明一个类为配置类。 @EnableAutoConfiguration是SpringBoot自动配置的核心注解，没有它， SpringBoot的自动配置就不会生效 @ComponentScan 容器组件扫描的注解，负责扫描所有的容器组件，也是最核心的注解之一。 Java API配置的好处 Spring应用有三种配置组件的方式:注解，Java API(Java Config)，XML. 其中注解和Java API的组合，完全可以干掉冗余的XML配置。 初学Spring的时候，见得最多的不是某行代码，而是XML配置啊。 有的同学恨不得要把Spring的约束文件给背下来了~_~。。 而@Configuration和@Bean的到来，无疑是开启了Spring应用配置方式的另一个时代。 Java配置的最大好处就是配置灵活，编写简单，其次是修改方便。 但是个人认为还是有缺点的，因为使用Java配置就意味着你需要创建很多的配置类， 如果把所有配置都写在一个类里面，那会变得很难维护，所以较好的选择是不同的配置， 创建不同的类，这样易于维护。 XML配置的优点就是很通俗易懂，不然XML语言不可能成为通用的数据传输语言。 但是想想一个应用里的配置何其多，这就是它最大的缺点:繁琐。 SpringBoot自动配置原理 @EnableAutoConfiguration使得容器会读取类路径下 META-INF/spring.factories 文件， 并导入文件中声明的自动配置类。 spring.factories里定义的都是扩展组件的自动配置类, 这些配置类往往都会依赖于XXXProperties.java的属性类， 我们在application.properties/yml文件里配置的属性， 就是配置的这些XXXProperties.java类的属性。 SpringBoot自动配置体现的是SPI(Service Provider Interface)的思想， 包括JDK中都大量使用到了这种思想。关于SPI可见: 知乎-SPI SpringBoot配置文件加载顺序 application.properties application.yml ... SpringBoot 怎么切换生产环境和开发环境配置 因为springboot优先读取application.properties, 所以可以在application.properties中配置spring.profile.active = dev/pro/... 来进行配置文件的切换。 SpringBoot是如何推断应用类型和main的 在SpringBoot中，有一个WebApplicationType的枚举类定义了3种Web类型: NONE SERVLET(Servlet 类型) REACTIVE(响应式类型)， SpringBoot在启动时会根据对应类型的核心类是否存在，据此判断应用的类型。 而判断main方法是根据异常堆栈来判断的。 "}}